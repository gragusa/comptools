[
  {
    "objectID": "comptools_ass2.html",
    "href": "comptools_ass2.html",
    "title": "Optimization",
    "section": "",
    "text": "Optimization is a field of mathematics that focuses on the problem of finding the solution to a minimization problem. More precisely, given a function \\(f:\\mathbb{R}^{k}\\to \\mathbb{R}\\), we seek \\(x^{*}\\in\\mathcal{C}\\subset \\mathbb{R}^{k}\\) such that \\[\nf(x^{*}) \\leqslant f(x) \\text{ for all }x\\in\\mathcal{C}.\n\\] The set \\(\\mathcal{C}\\) constraints the solution to leave in a subset of \\(\\mathbb{R}^{k}\\)\nWhen \\(\\mathcal{C}\\) coincides with \\(\\mathbb{R}^k\\) the problem is said to be unconstrained.\nIn unconstrained optimization, the objective function \\(f(x)\\) needs to be minimized (or maximized) without any restrictions on the variable \\(x\\). The problem is described as \\[\n\\min_{x} f(x), \\quad x\\in\\mathbb{R}^k.\n\\]\nMany problems involve constraints that the solution must satisfy, that is, \\(\\mathcal{C}\\) does not coincide with \\(\\mathbb{R}^k\\). For instance, we want to minimize the function over a space where \\(x_j &lt; c_j\\), \\(c_j\\in\\mathbb{R}\\), \\(j=1,\\ldots,k\\) or we may be interested in values of \\(x\\) that minimizes \\(f\\) when certain restrictions are satisfied. Generally, the constrained optimization is \\[\n   \\begin{aligned}\n   &\\min_{x} \\ f(x) \\\\\n   &\\text{subject to} \\\\\n   & \\ g_i(x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n                     & \\ h_j(x) = 0, \\quad j = 1, \\ldots, p.\n   \\end{aligned}\n\\] Here, \\(g_i(x)\\) and \\(h_j(x)\\) are functions that represent inequality and equality constraints.\nIn what follows, we will focus on the class of unconstrained problems where \\(f\\) is smooth, that is, a function that everywhere continuously differentiable."
  },
  {
    "objectID": "comptools_ass2.html#optimization-problems",
    "href": "comptools_ass2.html#optimization-problems",
    "title": "Optimization",
    "section": "",
    "text": "Optimization is a field of mathematics that focuses on the problem of finding the solution to a minimization problem. More precisely, given a function \\(f:\\mathbb{R}^{k}\\to \\mathbb{R}\\), we seek \\(x^{*}\\in\\mathcal{C}\\subset \\mathbb{R}^{k}\\) such that \\[\nf(x^{*}) \\leqslant f(x) \\text{ for all }x\\in\\mathcal{C}.\n\\] The set \\(\\mathcal{C}\\) constraints the solution to leave in a subset of \\(\\mathbb{R}^{k}\\)\nWhen \\(\\mathcal{C}\\) coincides with \\(\\mathbb{R}^k\\) the problem is said to be unconstrained.\nIn unconstrained optimization, the objective function \\(f(x)\\) needs to be minimized (or maximized) without any restrictions on the variable \\(x\\). The problem is described as \\[\n\\min_{x} f(x), \\quad x\\in\\mathbb{R}^k.\n\\]\nMany problems involve constraints that the solution must satisfy, that is, \\(\\mathcal{C}\\) does not coincide with \\(\\mathbb{R}^k\\). For instance, we want to minimize the function over a space where \\(x_j &lt; c_j\\), \\(c_j\\in\\mathbb{R}\\), \\(j=1,\\ldots,k\\) or we may be interested in values of \\(x\\) that minimizes \\(f\\) when certain restrictions are satisfied. Generally, the constrained optimization is \\[\n   \\begin{aligned}\n   &\\min_{x} \\ f(x) \\\\\n   &\\text{subject to} \\\\\n   & \\ g_i(x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n                     & \\ h_j(x) = 0, \\quad j = 1, \\ldots, p.\n   \\end{aligned}\n\\] Here, \\(g_i(x)\\) and \\(h_j(x)\\) are functions that represent inequality and equality constraints.\nIn what follows, we will focus on the class of unconstrained problems where \\(f\\) is smooth, that is, a function that everywhere continuously differentiable."
  },
  {
    "objectID": "comptools_ass2.html#unconstrained-problems",
    "href": "comptools_ass2.html#unconstrained-problems",
    "title": "Optimization",
    "section": "Unconstrained problems",
    "text": "Unconstrained problems\n\\[\n\\min_{x} f(x), \\quad x\\in\\mathbb{R}^k.\n\\tag{1}\\]\nA point \\(x^*\\) is a global minimizer of \\(f\\) if \\(f(x^*)\\leqslant f(x)\\) for all \\(x\\) ranging over of of \\(\\mathbb{R}^k\\).\nThe global minimizer can be difficult to find. The algorithms to solve Equation 1 exploit local knowledge of \\(f\\), we do not have a clear picture of the overall shape of \\(f\\) and, as such, we cannot ever be sure that the solution we find is indeed a global solution to the minimization problem. Most algorithms are able to find only a local minimizer, which is a point that achieves the samllest value of \\(f\\) in a neighborhood of \\(x\\).\nA point \\(x^*\\) is a local minimizer if there is a neighborhood1 \\(\\mathcal{N}\\) of \\(x^*\\) such that \\(f(x^*)\\leqslant f(x)\\) for all \\(x\\in\\mathcal{N}\\).\nA point \\(x^*\\) is a strict local minimizer if there a neighborhood of \\(\\mathcal{N}\\) such that \\(f(x^*) &lt; f(x)\\) for all \\(x\\in \\mathcal{N}\\).\nFor instance, for the function \\(f(x) = 2024\\), every point is a weak local minimizer, while the function \\(f(x) = (x-a)^2\\) has a strict local minimizer at \\(x=a^{-1}\\).\nNotation: Given a function \\(f:\\mathbb{R}^k\\to\\mathbb{R}\\), the gradient of \\(f\\) evaluated at \\(x^o\\) is: \\[\n\\nabla f(x^o) := \\begin{pmatrix}\\left.\\frac{\\partial f(x)}{\\partial x_{1}}\\right|_{x=x^{o}}\\\\\n\\left.\\frac{\\partial f(x)}{\\partial x_{2}}\\right|_{x=x^{o}}\\\\\n\\vdots\\\\\n\\left.\\frac{\\partial f(x)}{\\partial x_{k}}\\right|_{x=x^{o}}\n\\end{pmatrix}.\n\\] Similarly, the \\(k\\times k\\) hessian matrix of \\(f\\) evaluated at \\(x^o\\) is \\[\n\\nabla^{2}f(x^{o})=\\begin{pmatrix}\\left.\\frac{\\partial^{2}f(x)}{\\partial x_{1}\\partial x_{1}}\\right|_{x=x^{o}} & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{1}\\partial x_{2}}\\right|_{x=x^{o}} & \\cdots & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{1}\\partial x_{k}}\\right|_{x=x^{o}}\\\\\n\\left.\\frac{\\partial^{2}f(x)}{\\partial x_{2}\\partial x_{1}}\\right|_{x=x^{o}} & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{2}\\partial x_{2}}\\right|_{x=x^{o}} & \\cdots & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{2}\\partial x_{k}}\\right|_{x=x^{o}}\\\\\n\\vdots & \\vdots &  & \\vdots\\\\\n\\left.\\frac{\\partial^{2}f(x)}{\\partial x_{k}\\partial x_{1}}\\right|_{x=x^{o}} & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{k}\\partial x_{2}}\\right|_{x=x^{o}} & \\cdots & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{k}\\partial x_{k}}\\right|_{x=x^{o}}\n\\end{pmatrix}.\n\\]"
  },
  {
    "objectID": "comptools_ass2.html#fundamental-mathematical-tools",
    "href": "comptools_ass2.html#fundamental-mathematical-tools",
    "title": "Optimization",
    "section": "Fundamental mathematical tools",
    "text": "Fundamental mathematical tools\nThe necessary conditions for optimality are derived assuming that \\(x^*\\) is a local minimizer and the proving facts about \\(\\nabla f(x^*)\\) and \\(\\nabla^2 f(x^*)\\).\n\nTheorem 1 If \\(x^*\\) is a local optimizer and \\(f\\) is continuously differentiable in an open neighborhood of \\(x^*\\), then \\(\\nabla f(x^*)=0\\) (first-order necessary condition); if \\(\\nabla^2 f\\) exists and its continuous in an open neighborhood of \\(x^*\\), then \\(\\nabla^2 f(x^*)\\) is positive definite (second-order necessary conditions).2\n\nSufficient conditions for optimality are conditions on the derivatives of \\(f\\) at the point \\(x^*\\) that guarantee that \\(x^*\\) is a local minimizer.\n\nTheorem 2 Suppose that \\(\\nabla^2 f(x^*)\\) is continuous in an open neighborhood of \\(x^*\\) and that \\(\\nabla f(x^*)=0\\) and \\(\\nabla^2 f(x^*)\\) is positive definite. Then \\(x^*\\) is a strict local minimizer of \\(f\\).\n\nThe second-order sufficient conditions guarantee something stronger than the necessary conditions; namely, that the minimizer is a strict local minimizer. Note too that the second-order sufficient conditions are not necessary: a point \\(x^∗\\) may be a strict local minimizer, and yet may fail to satisfy the sufficient conditions. A simple example is given by the function \\(f(x) = x^3\\), for which the point \\(x^∗=0\\) is a strict local minimizer at which the Hessian matrix vanishes (and is therefore not positive definite).\nWhen the objective function is convex, local and global minimizers are simple to characterize.\n\nTheorem 3 When \\(f\\) is convex, any local minimizer of \\(x^*\\) is a global minimizer of \\(f\\). If in addition, \\(f\\) is differentiable, any stationary point \\(x^*\\) is a global minimizer.\n\n\nExample 1 The sum of square residuals \\[\nSSR(\\beta) = \\sum_{i=1}^n (Y_i - X'_i\\beta)^2\n\\] is strictly convex provided that \\(\\sum_{i=1}^n X_iX'_i\\) is invertible (that is, it has full column rank)."
  },
  {
    "objectID": "comptools_ass2.html#overview-of-algorithms",
    "href": "comptools_ass2.html#overview-of-algorithms",
    "title": "Optimization",
    "section": "Overview of algorithms",
    "text": "Overview of algorithms\nA common approach to optimization is to incrementally improve a point \\(x\\) by taking a step that minimizes the objective value based on a local model. The local model may be obtained, for example, from a first- or second-order Taylor approximation. Optimization algorithms that follow this general approach are referred to as descent direction methods. They start with a design point \\(x^{(0)}\\) and then generate a sequence of points, sometimes called iterates, to converge to a local minimum.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function and its derivative\ndef f(x):\n    return x**2 - np.log(x)\n\ndef df(x):\n    return 2*x - 1/x\n\ndef connectpoints(x,y,p1,p2):\n    x1, x2 = x[p1], x[p2]\n    y1, y2 = y[p1], y[p2]\n    plt.plot([x1,x2],[y1,y2],'k-')\n\n\n# Initial point\nx0 = 2\nalpha = 0.3\n\n# Gradient descent update\nx1 = x0 - alpha * df(x0)\nx2 = x1 - alpha * df(x1)\n# Points for the function plot\nx = np.linspace(-2.5, 2.5, 400)\ny = f(x)\n\n# Tangent line at x0 (y = m*x + b)\n\n# Creating the plot\n\nplt.figure(figsize=(8, 5))\nplt.plot(x, y, label='f(x) = x^2')\nplt.scatter([x0, x1], [f(x0), f(x1)], color='red')  # Points\nm = df(x0)\nb = f(x0) - m*x0\ntangent_line = m*x + b\nplt.plot(x, tangent_line, 'b--', label=f'Tangent at x0={x0}')\nplt.arrow(x0, 0.4, x1-x0, 0.0, head_width=0.1, length_includes_head=True, color = 'r')\n\nm = df(x1)\nb = f(x1) - m*x1\ntangent_line = m*x + b\nplt.plot(x, tangent_line, 'b--', label=f'Tangent at x0={x1}', )\nplt.ylim([-0.2,6])\nplt.xlim([-0.4,3])\nplt.plot(x, tangent_line, 'b--', label=f'Tangent at x0={x1}', )\nm = df(x0)\nb = f(x0) - m*x0\ntangent_line = m*x + b\n\nplt.scatter(x0, f(x0), color='green')  # Initial point\nplt.scatter(x1, f(x1), color='green')  # Next point after step\nplt.scatter(x2, f(x2), color='green')  # Next point after step\n\nplt.arrow(x1, 0., x2-x1, 0., head_width=0.1, length_includes_head=True, color = 'r')\n\nplt.title('Gradient Descent on f(x)')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.xticks([])  # Remove x-axis ticks\nplt.xticks([x0, x1, x2], [r\"$x^{(0)}$\", r\"$x^{(1)}$\", r\"$x^{(2)}$\"])\n#plt.legend()\nplt.grid(True)\nplt.plot([x0, x0],[f(x0), 0],'g--')\nplt.plot([x1, x1],[f(x1), 0],'g--')\nplt.plot([x2, x2],[f(x1), 0],'g--')\nplt.show()\n\n/var/folders/72/d3_1t_ps0z1fw0_l58cwv_s80000gn/T/ipykernel_1891/189701708.py:6: RuntimeWarning: invalid value encountered in log\n  return x**2 - np.log(x)\n\n\n\n\n\n\n\n\nFigure 1: Gradient descent\n\n\n\n\n\nThe intuition behind this approach is relatively simple. Consider the function plotted in Figure 1. We start the algorithm at \\(x^{(0)}\\). The derivative at this point (the tangent line in blue) is positive, that is, \\(f'(x^{(0)})&gt;0\\). Thus, to descend toward smaller values of the function have to set a point \\(x^{(1)}\\) smaller. One possibility is to use the following iteration \\[\nx^{(1)} = x^{(0)} - \\alpha \\nabla f(x^{(0)}),\n\\] where \\(\\alpha\\in(0,1)\\) is the step factor. As it can be seen from Figure 1, at this point the value of \\(f\\) is now lower. Applying a new iterate we obtain \\(x^{(2)} = x^{(1)} - \\alpha f'(x^{(1)})\\) which is now very close to the minimum value of the function.\nWe will keep iterating until the termination condition is satisfied. The termination condition will be satisfied when the current iterate is likely to be a local minimum.[^termination]\n3: The most common termination conditions are 1. Maximum iterations. We may want to terminate when the number of iterations \\(k\\) exceeds some threshold \\(k_max\\). Alternatively, we might want to terminate once a maximum amount of elapsed time is exceeded. 2. Absolute improvement. This termination condition looks at the change in the function value over subsequent steps. If the change is smaller than a given threshold, it will terminate: \\[\nf(x^{(k)}) - f(x^{(r+1)}) &lt; \\epsilon_a\n\\] 1. Relative improvement. This termination condition looks at the change in function value but uses the relative change of the function. The iterations will terminate if \\[\nf(x^{(k)}) - f(x^{(r+1)}) &lt; \\epsilon_r|f(x^{(k)})|\n\\] 1. Gradient magnitude. We can terminate if the derivative is smaller than a certain tolerance \\[\n\\Vert \\delta f(x^{(k)}) \\Vert &lt; \\epsilon_g\n\\]\nThe same logic can be applied to the case in which \\(f:\\mathbb{R}^k \\to \\mathbb{R}\\) the only difference is that now \\(\\nabla f(x^{(0)})\\) is a \\(k\\times 1\\) vector instead of being a scalar.\nThe gradient descent idea can be generalized by considering the following iterate \\[\nx^{(r)} = x^{(r-1)} - \\alpha d^{(r)},\n\\] where \\(d^{(r)}\\) is a descent direction. The idea of this generalization is that instead of using the gradient as direction, we can use different directions that might speed up the algorithm.\nWhen \\(d^{(r)} = \\nabla f(x^{(r)})\\), the algorithm is called the gradient descent (and direction is called the direction of deepest descent).\nDirections that can be used belong to two classes: 1. first-order: the direction only uses information about the gradient of \\(f\\). The many variations of the gradient descent (Ada, Momentum, etc.) and the conjugate gradient method belong to this class. 2. second order: the direction incorporate information about the second derivatives of \\(f\\). The key idea here is that \\[\nf(x^{(r+1)}) = f(x^{(r)}) + \\nabla f'(x^{(r)})(x^{(r+1)}-x^{(r)}) + (x^{(r+1)}-x^{(r)}) \\dot{H}_k (x^{(r+1)}-x^{(r)}),\n\\] where \\(\\dot{H}_k = \\nabla^2 f(\\dot{x}^{(r)})\\) is the Hessian evaluated at some point between \\(x^{(r+1)}\\) and \\(x^{(r)}\\). Then, approximating the gradient of \\(f\\) and setting it equal to zero yields \\[\nx^{(r+1)} = x^{(r)}-[\\nabla^2 f(x^{(r)})]^{-1} \\nabla f(x^{(r)}) +\n\\] which suggests using the inverse of the hessian to form the direction. Since evaluating the Hessian at each iterate is too costly computationally, different algorithms approximate the Hessian in different ways."
  },
  {
    "objectID": "comptools_ass2.html#a-simple-implementation-of-gradient-descent",
    "href": "comptools_ass2.html#a-simple-implementation-of-gradient-descent",
    "title": "Optimization",
    "section": "A simple implementation of gradient descent",
    "text": "A simple implementation of gradient descent\nThe following code implements a gradient descent with steepest direction in Python.\n\nfrom numpy import linalg as la\n\ndef steepest_descent(f, gradient, initial_guess, learning_rate, num_iterations = 100, epsilon_g = 1e-07):\n    x = initial_guess\n    for i in range(num_iterations):\n        grad = gradient(x)\n        x = x - learning_rate * grad\n        normg = la.norm(grad)\n        print(f\"Iteration {i+1}: x = {x}, f(x) = {f(x)}, ||g(x)||={normg}\")\n        ## Termination condition\n        if  normg &lt; epsilon_g:\n            break\n    return x\n\nThis is the Julia version.\nusing LinearAlgebra ## needed for norm\nfunction steepest_descent(f, gradient, initial_guess, learning_rate; num_iterations = 100; epsilon_g = 1e-7)\n    x = initial_guess\n    for i in 1:num_iterations\n        grad = gradient(x)\n        x = x - learning_rate * grad\n        normg = norm(g)\n        println(\"Iteration $i: x = $x, f(x) = $(objective_function(x)), ||g(x)||=$(normg)\")\n        if normg &lt; epsilon_g\n            break\n        end\n    end\n    return x\nend\n\nSuppose we want to solve the following problem \\[\n\\min_{x} f(x)\n\\] where, for some \\(d&gt;1\\), \\[\nf(x) = \\sum_{i=1}^d \\left((x_i-3)^2 \\right)\n\\] The gradient of this function is \\[\n\\nabla f(x)=\\begin{pmatrix}2(x_{1}-3)\\\\\n2(x_{2}-3)\\\\\n\\vdots\\\\\n2(x_{d}-3)\n\\end{pmatrix}.\n\\]\nThe minimum of this function is \\(x_i = 3\\).\n\ndef f(x):\n    return np.sum((x-3.0)**2)\n\ndef gradient(x):\n    return 2*(x-3.0)\n\nsteepest_descent(f, gradient, np.array([0., 0.]), 0.2)\n\nIteration 1: x = [1.2 1.2], f(x) = 6.479999999999999, ||g(x)||=8.48528137423857\nIteration 2: x = [1.92 1.92], f(x) = 2.3327999999999993, ||g(x)||=5.091168824543142\nIteration 3: x = [2.352 2.352], f(x) = 0.8398079999999992, ||g(x)||=3.0547012947258847\nIteration 4: x = [2.6112 2.6112], f(x) = 0.3023308799999997, ||g(x)||=1.8328207768355302\nIteration 5: x = [2.76672 2.76672], f(x) = 0.10883911679999973, ||g(x)||=1.099692466101318\nIteration 6: x = [2.860032 2.860032], f(x) = 0.039182082047999806, ||g(x)||=0.6598154796607905\nIteration 7: x = [2.9160192 2.9160192], f(x) = 0.014105549537279988, ||g(x)||=0.39588928779647375\nIteration 8: x = [2.94961152 2.94961152], f(x) = 0.005077997833420814, ||g(x)||=0.23753357267788475\nIteration 9: x = [2.96976691 2.96976691], f(x) = 0.0018280792200315037, ||g(x)||=0.1425201436067311\nIteration 10: x = [2.98186015 2.98186015], f(x) = 0.0006581085192113414, ||g(x)||=0.08551208616403891\nIteration 11: x = [2.98911609 2.98911609], f(x) = 0.00023691906691608675, ||g(x)||=0.051307251698423345\nIteration 12: x = [2.99346965 2.99346965], f(x) = 8.529086408979587e-05, ||g(x)||=0.03078435101905426\nIteration 13: x = [2.99608179 2.99608179], f(x) = 3.070471107232651e-05, ||g(x)||=0.01847061061143306\nIteration 14: x = [2.99764908 2.99764908], f(x) = 1.1053695986035874e-05, ||g(x)||=0.011082366366859834\nIteration 15: x = [2.99858945 2.99858945], f(x) = 3.9793305549719125e-06, ||g(x)||=0.0066494198201153985\nIteration 16: x = [2.99915367 2.99915367], f(x) = 1.4325589997895879e-06, ||g(x)||=0.003989651892068737\nIteration 17: x = [2.9994922 2.9994922], f(x) = 5.15721239924432e-07, ||g(x)||=0.002393791135240991\nIteration 18: x = [2.99969532 2.99969532], f(x) = 1.8565964637257903e-07, ||g(x)||=0.0014362746811448456\nIteration 19: x = [2.99981719 2.99981719], f(x) = 6.683747269425835e-08, ||g(x)||=0.000861764808686405\nIteration 20: x = [2.99989032 2.99989032], f(x) = 2.4061490169894037e-08, ||g(x)||=0.0005170588852123454\nIteration 21: x = [2.99993419 2.99993419], f(x) = 8.662136461115092e-09, ||g(x)||=0.00031023533112715604\nIteration 22: x = [2.99996051 2.99996051], f(x) = 3.118369125973376e-09, ||g(x)||=0.0001861411986757912\nIteration 23: x = [2.99997631 2.99997631], f(x) = 1.1226128853672496e-09, ||g(x)||=0.00011168471920497228\nIteration 24: x = [2.99998578 2.99998578], f(x) = 4.0414063872210937e-10, ||g(x)||=6.70108315234858e-05\nIteration 25: x = [2.99999147 2.99999147], f(x) = 1.454906299338991e-10, ||g(x)||=4.020649891358905e-05\nIteration 26: x = [2.99999488 2.99999488], f(x) = 5.237662677983984e-11, ||g(x)||=2.4123899347651e-05\nIteration 27: x = [2.99999693 2.99999693], f(x) = 1.8855585639651492e-11, ||g(x)||=1.447433960909303e-05\nIteration 28: x = [2.99999816 2.99999816], f(x) = 6.788010829620027e-12, ||g(x)||=8.684603765204602e-06\nIteration 29: x = [2.99999889 2.99999889], f(x) = 2.4436838986632097e-12, ||g(x)||=5.210762258871547e-06\nIteration 30: x = [2.99999934 2.99999934], f(x) = 8.797262039900029e-13, ||g(x)||=3.1264573553229284e-06\nIteration 31: x = [2.9999996 2.9999996], f(x) = 3.167014331536526e-13, ||g(x)||=1.8758744136961865e-06\nIteration 32: x = [2.99999976 2.99999976], f(x) = 1.1401251593531493e-13, ||g(x)||=1.1255246477152823e-06\nIteration 33: x = [2.99999986 2.99999986], f(x) = 4.10445057876081e-14, ||g(x)||=6.753147886291694e-07\nIteration 34: x = [2.99999991 2.99999991], f(x) = 1.477602202246525e-14, ||g(x)||=4.0518887342871644e-07\nIteration 35: x = [2.99999995 2.99999995], f(x) = 5.31936792808749e-15, ||g(x)||=2.431133235548003e-07\nIteration 36: x = [2.99999997 2.99999997], f(x) = 1.914972432124978e-15, ||g(x)||=1.4586799413288017e-07\nIteration 37: x = [2.99999998 2.99999998], f(x) = 6.893900623730809e-16, ||g(x)||=8.752079597729852e-08\n\n\narray([2.99999998, 2.99999998])\n\n\nfunction f(x)\n    sum((x.-3.0).^2)\nend\n\nfunction gradient(x)\n    2.*(x.-3.0)\nend\n\nsteepest_descent(f, gradient, [.0, .0], 0.2)"
  },
  {
    "objectID": "comptools_ass2.html#calculating-the-derivative",
    "href": "comptools_ass2.html#calculating-the-derivative",
    "title": "Optimization",
    "section": "Calculating the derivative",
    "text": "Calculating the derivative\nWhen the gradient is difficult to calculate analytically, we can use algorithmically calculate the derivative of the function \\(f\\).\n\nFinite difference: We use \\[\n\\lim_{h} \\frac{f(x+h) - f(x)}{h}\n\\]\n\nusing FiniteDifferences\n\n# Create a central finite difference method with the default settings\nfdm = central_fdm(5, 1)\n\n# Calculate the gradient at a point\nx0 = [1.0, 2.0]\ngradient = grad(fdm, f, x0)\n\nprintln(\"Numerical Gradient:\", gradient)\n\nimport numpy as np\nfrom scipy.optimize import approx_fprime\n\n\nepsilon = np.sqrt(np.finfo(float).eps)  \n\n# Point at which to calculate the gradient\nx0 = np.array([1.0, 2.0])\n\n# Calculate the gradient at the point x0\ngradient = approx_fprime(x0, f, epsilon)\n\nprint(\"Gradient at x0:\", gradient)\n\nGradient at x0: [-4. -2.]\n\n\n\nAutomatic differentiation\n\nAutomatic Differentiation (AD) is a computational technique used to evaluate the derivative of a function specified by a computer program. AD exploits the fact that any computer program, no matter how complex, executes a sequence of elementary arithmetic operations and functions (like additions, multiplications, and trigonometric functions). By applying the chain rule to these operations, AD efficiently computes derivatives of arbitrary order, which are accurate up to machine precision. This contrasts with numerical differentiation, which can introduce significant rounding errors.\nOne popular Python library that implements automatic differentiation is autograd. It extends the capabilities of NumPy by allowing you to automatically compute derivatives of functions composed of many standard mathematical operations. Here is a simple example of using autograd to compute the derivative of a function:\nNow, let’s compute the derivative of the function defined above.\n\nimport autograd.numpy as np   # Import wrapped NumPy\nfrom autograd import grad    # Import the gradient function\n\n# Create a function that returns the derivative of f\ndf = grad(f)\n\n# Evaluate the derivative at x = pi\nprint(\"The derivative of f(x) at x = [0.2, 0.1] is:\", df(np.array([0.2, 0.1])))\n\nThe derivative of f(x) at x = [0.2, 0.1] is: [-5.6 -5.8]\n\n\nWhen comparing the precision and applicability of finite difference methods and automatic differentiation (AD), especially for functions with large input dimensions, there are several key points to consider.\nFinite difference methods approximate derivatives by evaluating differences in function values at nearby points. The accuracy of these methods is highly dependent on the step size chosen: too large, and the approximation becomes poor; too small, and floating-point errors can dominate the result. This trade-off can be particularly challenging in high-dimensional spaces as errors can accumulate more significantly (each partial derivative may introduce errors that affect the overall gradient computation).\nAD computes derivatives using the exact chain rule and is not based on numerical approximations of difference quotients. Therefore, it can provide derivatives that are accurate to machine precision. AD efficiently handles computations in high dimensions because it systematically applies elementary operations and chain rules, bypassing the curse of dimensionality that often affects finite difference methods. AD is less sensitive to the numerical issues that affect finite differences, such as choosing a step size or dealing with subtractive cancellation.\nThe visualization provided by @fig-error demonstrates the comparative performance of the finite difference method and automatic differentiation (AD) when used to calculate gradients. This graph illustrates a key observation: as the dimensionality of the input increases, the error associated with the finite difference method tends to rise almost linearly. This escalation in error is accompanied by an increase in computation time, underscoring the method’s sensitivity to higher dimensions.\nIn contrast, the automatic differentiation method showcases remarkable stability across dimensions. The error remains negligible, essentially zero, highlighting AD’s inherent precision. Furthermore, AD’s computation time remains consistent regardless of input dimensionality. This performance characteristic of AD is due to its methodological approach, which systematically applies the chain rule to derive exact derivatives, bypassing the numerical instability and scaling issues often encountered with finite differences.\nThese insights are particularly relevant in fields that rely heavily on precise and efficient computation of derivatives, such as in numerical optimization, machine learning model training, and dynamic systems simulation. The stability and scalability of AD make it an invaluable tool in these areas, especially when dealing with high-dimensional data sets or complex models.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\ndef multivariate_function(x):\n    \"\"\" A sample multivariate function, sum of squares plus sin of each component. \"\"\"\n    return np.sum(x**2 + np.sin(x))\n\ndef analytical_derivative(x):\n    \"\"\" Analytical derivative of the multivariate function. \"\"\"\n    return 2*x + np.cos(x)\n\ndef finite_difference_derivative(f, x, h=1e-5):\n    \"\"\" Compute the gradient of `f` at `x` using the central finite difference method. \"\"\"\n    grad = np.zeros_like(x)\n    for i in range(len(x)):\n        x_plus = np.copy(x)\n        x_minus = np.copy(x)\n        x_plus[i] += h\n        x_minus[i] -= h\n        grad[i] = (f(x_plus) - f(x_minus)) / (2*h)\n    return grad\n\n# Range of dimensions\ndimensions = range(1, 101)\nerrors = []\ntimes = []\n\n# Loop over dimensions\nfor dim in dimensions:\n    x = np.random.randn(dim)\n    \n    # Analytical derivative\n    true_grad = analytical_derivative(x)\n    \n    # Start timing\n    start_time = time.time()\n    \n    # Finite difference derivative\n    fd_grad = finite_difference_derivative(multivariate_function, x)\n    \n    # End timing\n    elapsed_time = time.time() - start_time\n    times.append(elapsed_time)\n    \n    # Error\n    error = np.linalg.norm(fd_grad - true_grad)\n    errors.append(error)\n\n# Plotting error\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(dimensions, errors, marker='o')\nplt.xlabel('Number of Dimensions')\nplt.ylabel('Error')\nplt.title('Approximation Error by Dimension')\nplt.grid(True)\n\n# Plotting computational time\nplt.subplot(1, 2, 2)\nplt.plot(dimensions, times, marker='o')\nplt.xlabel('Number of Dimensions')\nplt.ylabel('Time (seconds)')\nplt.title('Computational Time by Dimension')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\nUsing solver\nIt is almost always advantageous to utilize established minimization routines and libraries rather than crafting custom code from scratch. This approach not only saves time but also leverages the extensive testing and optimizations embedded within these libraries, which are designed to handle a broad range of mathematical challenges efficiently and accurately.\nIn Python, the scipy.optimize module from the SciPy library is a robust toolkit that provides several algorithms for function minimization, including methods for unconstrained and constrained optimization. Some of the notable algorithms include BFGS, Nelder-Mead, and conjugate gradient, among others. These algorithms are well-suited for numerical optimization in scientific computing.\nIn Julia, Optim.jl offers a similar breadth of optimization routines, with support for a variety of optimization problems, from simple univariate function minimization to complex multivariate cases. Optim.jl includes algorithms like L-BFGS, Gradient Descent, and Newton’s Method, each tailored for specific types of optimization scenarios.\nBoth Python’s SciPy and Julia’s Optim.jl represent just a slice of the available tools. There are many other solvers and libraries dedicated to optimization. The most widely used is Ipopt which is particularly useful for problems that require embedding in lower-level systems for performance.\nThe choice of a solver can depend on several factors:\n\nProblem Type: Some solvers are better suited for large-scale problems, others for problems with complex constraints, or for nondifferentiable or noisy functions.\nAccuracy and Precision: Different algorithms and implementations can provide varying levels of precision and robustness, important in applications like aerospace or finance.\nPerformance and Speed: Depending on the implementation, some solvers might be optimized for speed using advanced techniques like parallel processing or tailored data structures.\nEase of Use and Flexibility: Some libraries offer more user-friendly interfaces and better documentation, which can be crucial for less experienced users or complex projects where customization is key.\n\n\n\nJulia’s Optim.jl\nOptim.jl supports various optimization algorithms. For our simple example, we can use the BFGS method, which is suitable for smooth functions and is part of a family of quasi-Newton methods.\nYou can now call the optimize function from Optim.jl, specifying the function, an initial guess, and the optimization method. Here is how you do it:\n# Initial guess\ninitial_guess = [0.0, 0.0]\n\n# Perform the optimization\nresult = optimize(f, initial_guess, BFGS())\nThe result object contains all the information about the optimization process, including the optimal values found, the value of the function at the optimum, and the convergence status:\n# Access the results\nprintln(\"Optimal parameters: \", Optim.minimizer(result))\nprintln(\"Minimum value: \", Optim.minimum(result))\nprintln(\"Convergence information: \", result)\nIf your function is more complex or if you want to speed up the convergence for large-scale problems, you can also provide the gradient (and even the Hessian) to the optimizer. Optim.jl can utilize these for more efficient calculations.\nFor constrained optimization, Optim.jl has support for simple box constraints which can be set using the Fminbox method to wrap around other methods like BFGS.\n# Define the bounds\nlower_bounds = [0.5, 1.5]\nupper_bounds = [1.5, 2.5]\n\n# Initial guess within the bounds\ninitial_guess = [1.0, 2.0]\n\n# Set up the optimizer with Fminbox\noptimizer = Fminbox(BFGS())\n# Run the optimization with bounds\nresult = optimize(f, lower_bounds, upper_bounds, initial_guess, optimizer, Optim.Options(g_tol = 1e-6))"
  },
  {
    "objectID": "comptools_ass2.html#pythons-minimize",
    "href": "comptools_ass2.html#pythons-minimize",
    "title": "Optimization",
    "section": "Python’s minimize",
    "text": "Python’s minimize\nThe scipy.optimize.minimize function in Python is a versatile solver for minimization problems of both unconstrained and constrained functions. It provides a wide range of algorithms for different kinds of optimization problems.\nSciPy’s minimize function supports various methods like ‘BFGS’, ‘Nelder-Mead’, ‘TNC’, etc. The choice of method can depend on the nature of your problem (e.g., whether it has constraints, whether the function is differentiable, etc.). As in the Julia’s discussion, we’ll use ‘BFGS’.\n\nimport numpy as np\nfrom scipy.optimize import minimize\ninitial_guess = np.array([0, 0])\nresult = minimize(f, initial_guess, method='BFGS')\n\nprint(\"Optimal parameters:\", result.x)\nprint(\"Minimum value:\", result.fun)\nprint(\"Success:\", result.success)\nprint(\"Message:\", result.message)\n\nOptimal parameters: [2.99999999 2.99999999]\nMinimum value: 2.4602836913395623e-16\nSuccess: True\nMessage: Optimization terminated successfully.\n\n\nTo add some bounds to the variables we can use the Bounds module.\n\nfrom scipy.optimize import Bounds\n\n# Define bounds (0, +Inf) for all parameters\nbounds = Bounds(np.array([0., 0.]), [np.inf, np.inf])\n\n# Run the optimization with bounds\nresult_with_bounds = minimize(f, initial_guess, method='L-BFGS-B', bounds=bounds)\nprint(\"Optimal parameters:\", result_with_bounds.x)\nprint(\"Minimum value:\", result_with_bounds.fun)\nprint(\"Success:\", result_with_bounds.success)\nprint(\"Message:\", result_with_bounds.message)\n\nOptimal parameters: [3.00000044 3.00000044]\nMinimum value: 3.9332116120454443e-13\nSuccess: True\nMessage: CONVERGENCE: NORM OF PROJECTED GRADIENT &lt;= PGTOL"
  },
  {
    "objectID": "comptools_ass2.html#assignment",
    "href": "comptools_ass2.html#assignment",
    "title": "Optimization",
    "section": "Assignment",
    "text": "Assignment\n\\[\nY_t = c + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\varepsilon_t, \\quad \\varepsilon_t ~ WN(0, \\sigma^2).\n\\]\nParameter Estimation - Estimate the parameters of the AR(2) model using both conditional and unconditional likelihood approaches on the monthly log differences of INDPRO from the FRED-MD (FRED data is here).\nTasks:\n\nCoding the Likelihood Function\n\nImplement the likelihood function for an AR(2) model in Python. You are required to code both the conditional and unconditional likelihood functions.\nConditional Likelihood: This approach uses the first 2 observations as given and starts the likelihood calculation from the 3rd observation.\nUnconditional Likelihood: This approach integrates over \\(p(Y_1, Y_2)\\), the unconditional distributions of the first two observations.\n\nUse the following model specification for the AR(7) process: \\[\ny_t = \\phi_0 + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\epsilon_t\n\\] where \\(\\epsilon_t\\) is i.i.d. normal with mean zero and variance \\(\\sigma^2\\).\nMaximizing the Likelihood\n\nWrite Python or Julia code to maximize the likelihood functions (both conditional and unconditional) with respect to the parameters \\(\\phi_0, \\phi_1, \\phi_2,\\) and \\(\\sigma^2\\). You may consider using optimization routines available in libraries such as scipy.optimize or Optim.jl.\n\nForecasting\n\nWith the estimated parameters from both approaches, forecast the future values of the log differences of INDPRO for the next 8 months (\\(h=1,2,\\dots,8\\)).\n(Optional) Provide a brief comparison of the forecast accuracy from both sets of parameters based on out-of-sample forecasting. Discuss any notable differences and potential reasons for these differences.\n\n\nDeliverables: - A Python (or Julia) script containing the implementations and results for the tasks outlined above. The script should run (you might give instructions on what is needed to make it run). Ideally, a Jupyter notebook. - A report discussing the methodology and the results.\nAssessment Criteria: - Correctness of the likelihood function implementations. - Quality of the code, including readability and proper documentation.\nResources: - FRED-MD dataset: Access the dataset directly from the FRED website or through any API that provides access to it. - SciPy library documentation for optimization functions. - Optim.jl\nThis assignment will test your ability to implement statistical models, manipulate time-series data, perform parameter estimation, and use statistical methods to forecast future values.\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\ndef ar_likelihood(params, data, p):\n    \"\"\"\n    Calculate the negative (unconditional) log likelihood for an AR(p) model.\n\n    params: list of parameters, where the first p are AR coefficients and the last is the noise variance.\n    data: observed data.\n    p: order of the AR model.\n    \"\"\"\n    # Extract AR coefficients and noise variance\n    c = params[0]\n    phi = params[1:p+1]\n    sigma2 = params[-1]\n        \n    # Calculate residuals\n    T = len(data)\n    residuals = data[p:] - c - np.dot(np.column_stack([data[p-j-1:T-j-1] for j in range(p)]), phi)\n    \n    # Calculate negative log likelihood\n    log_likelihood = (-T/2 * np.log(2 * np.pi * sigma2) - np.sum(residuals**2) / (2 * sigma2))\n    \n    return -log_likelihood\n\ndef estimate_ar_parameters(data, p):\n    \"\"\"\n    Estimate AR model parameters using maximum likelihood estimation.\n\n    data: observed data.\n    p: order of the AR model.\n    \"\"\"\n    # Initial parameter guess (random AR coefficients, variance of 1)\n    params_initial = np.zeros(p+2)\n    params_initial[-1] = 1.0\n\n    ## Bounds\n    bounds = [(None, None)]\n    # Then p AR coefficients, each bounded between -1 and 1\n    bounds += [(-1, 1) for _ in range(p)]\n    # The variance parameter, bounded to be positive\n    bounds += [(1e-6, None)]\n\n    # Minimize the negative log likelihood\n    result = minimize(ar_likelihood, params_initial, args=(data, p), bounds=bounds)\n    \n    if result.success:\n        estimated_params = result.x\n        return estimated_params\n    else:\n        raise Exception(\"Optimization failed:\", result.message)\n\n# Example usage\ndata = np.random.randn(100)  # Simulated data; replace with actual data\np = 2  # AR(2) model\nparams = estimate_ar_parameters(data, p)\nprint(\"Estimated parameters:\", params)\n\nEstimated parameters: [ 0.11376649 -0.10794687  0.07171477  0.80931585]\n\n\n\nimport numpy as np\n\ndef fit_ar_ols_xx(data, p):\n    \"\"\"\n    data: observed data.\n    p: order of the AR model.\n    note: no constant\n    \"\"\"\n    # Prepare the lagged data matrix\n    T = len(data)\n    Y = data[p:]  # Dependent variable (from p to end)\n    X = np.column_stack([data[p-i-1:T-i-1] for i in range(p)])\n    X = np.column_stack((np.ones(X.shape[0]), X))\n    \n    # Calculate OLS estimates using the formula: beta = (X'X)^-1 X'Y\n    XTX = np.dot(X.T, X)  # X'X\n    XTY = np.dot(X.T, Y)  # X'Y\n    beta_hat = np.linalg.solve(XTX, XTY)  # Solve (X'X)beta = X'Y\n    \n    return beta_hat\n\n\nbeta_hat = fit_ar_ols_xx(data, p)\nprint(\"Estimated AR coefficients:\", beta_hat)\n\nEstimated AR coefficients: [ 0.11376621 -0.10794739  0.0717144 ]\n\n\n\nimport numpy as np\nimport statsmodels.api as sm\n\ndef fit_ar_ols_sm(data, p):\n    \"\"\"\n    Estimate parameters of an AR(p) model using OLS.\n\n    data: observed data.\n    p: order of the AR model.\n    \"\"\"\n    # Prepare the lagged data matrix\n    T = len(data)\n    Y = data[p:]  # Dependent variable (from p to end)\n    X = np.column_stack([data[p-i-1:T-i-1] for i in range(p)])\n    X = np.column_stack((np.ones(X.shape[0]), X))\n    # Fit the OLS model\n    model = sm.OLS(Y, X)\n    results = model.fit()\n    return results\n\n# Example usage\n  # Simulated data; replace with actual data\np = 2  # AR(2) model\nresults = fit_ar_ols_sm(data, p)\n\n# Print the results\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.019\nModel:                            OLS   Adj. R-squared:                 -0.002\nMethod:                 Least Squares   F-statistic:                    0.9079\nDate:                Fri, 07 Mar 2025   Prob (F-statistic):              0.407\nTime:                        11:30:19   Log-Likelihood:                -129.68\nNo. Observations:                  98   AIC:                             265.4\nDf Residuals:                      95   BIC:                             273.1\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.1138      0.095      1.196      0.235      -0.075       0.303\nx1            -0.1079      0.102     -1.056      0.293      -0.311       0.095\nx2             0.0717      0.103      0.694      0.489      -0.133       0.277\n==============================================================================\nOmnibus:                        1.946   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.378   Jarque-Bera (JB):                1.923\nSkew:                          -0.329   Prob(JB):                        0.382\nKurtosis:                       2.806   Cond. No.                         1.28\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "comptools_ass2.html#julia-code",
    "href": "comptools_ass2.html#julia-code",
    "title": "Optimization",
    "section": "Julia code",
    "text": "Julia code\nusing Distributions\nusing Optim\nusing LinearAlgebra\n\nfunction ar_likelihood(params, data, p)\n    \"\"\"\n    Calculate the negative (unconditional) log likelihood for an AR(p) model.\n    \n    params: list of parameters, where the first p+1 are AR coefficients and the last is the noise variance.\n    data: observed data.\n    p: order of the AR model.\n    \"\"\"\n    # Extract AR coefficients and noise variance\n    c = params[1]\n    phi = params[2:p+1]\n    sigma2 = params[p+2]\n        \n    # Calculate residuals\n    T = length(data)\n    Y = data[p+1:end]\n    X = hcat([data[p-j:T-j-1] for j in 0:p-1]...)\n    residuals = Y .- c .- X * phi\n    \n    # Calculate negative log likelihood\n    log_likelihood = (-T/2 * log(2 * π * sigma2) - sum(residuals.^2) / (2 * sigma2))\n    \n    return -log_likelihood\nend\n\nfunction estimate_ar_parameters(data, p)\n    \"\"\"\n    Estimate AR model parameters using maximum likelihood estimation.\n    \n    data: observed data.\n    p: order of the AR model.\n    \"\"\"\n    # Initial parameter guess (zeros AR coefficients, variance of 1)\n    params_initial = zeros(p+2)\n    params_initial[p+2] = 1.0  # Initial variance\n    \n    # Setup lower and upper bounds\n    lower_bounds = vcat(-Inf, fill(-1.0, p), 1e-6)\n    upper_bounds = vcat(+Inf, fill(1.0, p), Inf)\n    \n    # Minimize the negative log likelihood\n    result = optimize(params -&gt; ar_likelihood(params, data, p), \n                     lower_bounds, upper_bounds, params_initial,\n                     Fminbox(LBFGS()))\n    \n    if Optim.converged(result)\n        estimated_params = Optim.minimizer(result)\n        return estimated_params\n    else\n        error(\"Optimization failed\")\n    end\nend\n\n# Example usage\ndata = randn(100)  # Simulated data; replace with actual data\np = 2  # AR(2) model\nparams = estimate_ar_parameters(data, p)\nprintln(\"Estimated parameters: \", params)\n\nfunction fit_ar_ols_xx(data, p)\n    \"\"\"\n    data: observed data.\n    p: order of the AR model.\n    note: no constant\n    \"\"\"\n    # Prepare the lagged data matrix\n    T = length(data)\n    Y = data[p+1:end]  # Dependent variable (from p+1 to end)\n    X = hcat([data[p-j:T-j-1] for j in 0:p-1]...)\n    X = [ones(T-p) X]  # Add a constant to the model\n    # Calculate OLS estimates using the formula: beta = (X'X)^-1 X'Y\n    XTX = X' * X  # X'X\n    XTY = X' * Y  # X'Y\n    beta_hat = XTX \\ XTY  # Solve (X'X)beta = X'Y\n    \n    return beta_hat\nend\n\nbeta_hat, X = fit_ar_ols_xx(data, p)\nprintln(\"Estimated AR coefficients: \", beta_hat)\n\nfunction fit_ar_ols_sm(data, p)\n    \"\"\"\n    Estimate parameters of an AR(p) model using OLS.\n    \n    data: observed data.\n    p: order of the AR model.\n    \"\"\"\n    # Prepare the lagged data matrix\n    T = length(data)\n    Y = data[p+1:end]  # Dependent variable (from p+1 to end)\n    X = hcat([data[p-j:T-j-1] for j in 0:p-1]...)\n    # Add a constant to the model if you want an intercept (optional)\n    X = hcat(ones(size(X, 1)), X)\n    \n    # Fit the OLS model using direct matrix calculation\n    beta = (X'X) \\ (X'Y)\n    \n    # Calculate residuals\n    residuals = Y - X * beta\n    \n    # Calculate standard errors\n    n, k = size(X)\n    sigma2 = sum(residuals.^2) / (n - k)\n    var_beta = sigma2 * inv(X'X)\n    std_errors = sqrt.(diag(var_beta))\n    \n    # t-statistics\n    t_stats = beta ./ std_errors\n    \n    # p-values (assuming normal distribution)\n    p_values = 2 * (1 .- cdf.(Normal(), abs.(t_stats)))\n    \n    # Return a named tuple with results\n    return (\n        coefficients = beta,\n        std_errors = std_errors,\n        t_statistics = t_stats,\n        p_values = p_values,\n        residuals = residuals,\n        sigma2 = sigma2\n    )\nend\n\n# Example usage\nresults = fit_ar_ols_sm(data, p)\n\n# Print summary\nprintln(\"\\nOLS Results Summary:\")\nprintln(\"====================\")\nprintln(\"Coefficients:\")\nfor i in 1:p\n    println(\"AR($i): $(results.coefficients[i+1]) (std err: $(results.std_errors[i+1]), t: $(results.t_statistics[i+1]), p: $(results.p_values[i+1]))\")\nend\nprintln(\"Residual standard error: $(sqrt(results.sigma2))\")"
  },
  {
    "objectID": "comptools_ass2.html#footnotes",
    "href": "comptools_ass2.html#footnotes",
    "title": "Optimization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA neighborhood of a point \\(x\\in\\mathbb{R}\\) is any open subset of \\(\\mathbb{R}^k\\) containing \\(x\\).↩︎\nA square matrix \\(H\\) is positive definite if for every \\(z\\in\\mathbb{R}^k\\) with \\(\\norm{z}\\neq 0\\), \\(z'Hz&gt;0\\).↩︎\ntermanation↩︎"
  },
  {
    "objectID": "comptools_ass1.html",
    "href": "comptools_ass1.html",
    "title": "Computational Tools for Macroeconometrics",
    "section": "",
    "text": "This assignment introduces students to practical and theoretical aspects of macroeconometrics, focusing on forecasting using the FRED-MD dataset. Students will learn to handle macroeconomic data, perform necessary transformations, apply univariate models to predict key economic indicators and to evaluate these forecasts."
  },
  {
    "objectID": "comptools_ass1.html#introduction",
    "href": "comptools_ass1.html#introduction",
    "title": "Computational Tools for Macroeconometrics",
    "section": "",
    "text": "This assignment introduces students to practical and theoretical aspects of macroeconometrics, focusing on forecasting using the FRED-MD dataset. Students will learn to handle macroeconomic data, perform necessary transformations, apply univariate models to predict key economic indicators and to evaluate these forecasts."
  },
  {
    "objectID": "comptools_ass1.html#the-fred-md-dataset",
    "href": "comptools_ass1.html#the-fred-md-dataset",
    "title": "Computational Tools for Macroeconometrics",
    "section": "The FRED-MD dataset",
    "text": "The FRED-MD dataset\nThe FRED-MD dataset is a comprehensive monthly database for macroeconomic research compiled by the Federal Reserve Bank of St. Louis. It features a wide array of economic indicators. The list of economic indicators can be obtained from the paper accompanying the data pdf.\nThe data can be downloaded here. The page contains all the different vintages of the data.\nLet us start to download the current.csv file:\n\nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('https://www.stlouisfed.org/-/media/project/frbstl/stlouisfed/research/fred-md/monthly/current.csv?sc_lang=en&hash=80445D12401C59CF716410F3F7863B64')\n\n# Clean the DataFrame by removing the row with transformation codes\ndf_cleaned = df.drop(index=0)\ndf_cleaned.reset_index(drop=True, inplace=True)\ndf_cleaned['sasdate'] = pd.to_datetime(df_cleaned['sasdate'], format='%m/%d/%Y')\ndf_cleaned\n\n\n\n\n\n\n\n\nsasdate\nRPI\nW875RX1\nDPCERA3M086SBEA\nCMRMTSPLx\nRETAILx\nINDPRO\nIPFPNSS\nIPFINAL\nIPCONGD\n...\nDNDGRG3M086SBEA\nDSERRG3M086SBEA\nCES0600000008\nCES2000000008\nCES3000000008\nUMCSENTx\nDTCOLNVHFNM\nDTCTHFNM\nINVEST\nVIXCLSx\n\n\n\n\n0\n1959-01-01\n2583.560\n2426.0\n15.188\n2.766768e+05\n18235.77392\n21.9616\n23.3868\n22.2620\n31.6664\n...\n18.294\n10.152\n2.13\n2.45\n2.04\nNaN\n6476.00\n12298.00\n84.2043\nNaN\n\n\n1\n1959-02-01\n2593.596\n2434.8\n15.346\n2.787140e+05\n18369.56308\n22.3917\n23.7024\n22.4549\n31.8987\n...\n18.302\n10.167\n2.14\n2.46\n2.05\nNaN\n6476.00\n12298.00\n83.5280\nNaN\n\n\n2\n1959-03-01\n2610.396\n2452.7\n15.491\n2.777753e+05\n18523.05762\n22.7142\n23.8459\n22.5651\n31.8987\n...\n18.289\n10.185\n2.15\n2.45\n2.07\nNaN\n6508.00\n12349.00\n81.6405\nNaN\n\n\n3\n1959-04-01\n2627.446\n2470.0\n15.435\n2.833627e+05\n18534.46600\n23.1981\n24.1903\n22.8957\n32.4019\n...\n18.300\n10.221\n2.16\n2.47\n2.08\nNaN\n6620.00\n12484.00\n81.8099\nNaN\n\n\n4\n1959-05-01\n2642.720\n2486.4\n15.622\n2.853072e+05\n18679.66354\n23.5476\n24.3911\n23.1161\n32.5567\n...\n18.280\n10.238\n2.17\n2.48\n2.08\n95.3\n6753.00\n12646.00\n80.7315\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n788\n2024-09-01\n19993.464\n16283.1\n121.690\n1.541305e+06\n716388.00000\n102.5873\n100.4044\n100.1100\n102.0602\n...\n119.220\n128.682\n31.45\n36.18\n28.01\n70.1\n553347.06\n934283.59\n5368.5671\n17.6597\n\n\n789\n2024-10-01\n20067.376\n16340.0\n121.904\n1.538666e+06\n720393.00000\n102.1219\n99.6821\n99.0178\n101.4336\n...\n119.218\n129.176\n31.53\n36.27\n28.07\n70.5\n554951.25\n938525.34\n5407.2449\n19.9478\n\n\n790\n2024-11-01\n20111.246\n16385.1\n122.435\n1.544822e+06\n725079.00000\n101.9736\n99.5645\n99.0025\n101.0038\n...\n119.230\n129.390\n31.59\n36.26\n28.22\n71.8\n556075.09\n941204.79\n5382.5669\n15.9822\n\n\n791\n2024-12-01\n20136.069\n16407.9\n123.103\n1.555153e+06\n730300.00000\n102.9833\n100.2940\n99.6550\n101.3436\n...\n119.746\n129.875\n31.73\n36.46\n28.33\n74.0\n558854.68\n946489.00\n5370.9871\n15.6997\n\n\n792\n2025-01-01\n20248.025\n16464.7\n122.519\nNaN\n723853.00000\n103.5110\n101.1431\n100.7228\n102.1889\n...\n120.453\n130.196\n31.90\n36.54\n28.55\n71.7\nNaN\nNaN\n5372.8381\n16.8122\n\n\n\n\n793 rows × 127 columns\n\n\n\n\n# Extract transformation codes\ntransformation_codes = df.iloc[0, 1:].to_frame().reset_index()\ntransformation_codes.columns = ['Series', 'Transformation_Code']\n\nThe transformation codes map variables to the transformations we must apply to each variable to render them (approximately) stationary. The data frame transformation_codes has the variable’s name (Series) and its transformation (Transformation_Code). There are six possible transformations (\\(x_t\\) denotes the variable to which the transformation is to be applied):\n\ntransformation_code=1: no trasformation\ntransformation_code=2: \\(\\Delta x_t\\)\ntransformation_code=3: \\(\\Delta^2 x_t\\)\ntransformation_code=4: \\(log(x_t)\\)\ntransformation_code=5: \\(\\Delta log(x_t)\\)\ntransformation_code=6: \\(\\Delta^2 log(x_t)\\)\ntransformation_code=7: \\(\\Delta (x_t/x_{t-1} - 1)\\)\n\nWe can apply these transformations using the following code:\n\nimport numpy as np\n\n# Function to apply transformations based on the transformation code\ndef apply_transformation(series, code):\n    if code == 1:\n        # No transformation\n        return series\n    elif code == 2:\n        # First difference\n        return series.diff()\n    elif code == 3:\n        # Second difference\n        return series.diff().diff()\n    elif code == 4:\n        # Log\n        return np.log(series)\n    elif code == 5:\n        # First difference of log\n        return np.log(series).diff()\n    elif code == 6:\n        # Second difference of log\n        return np.log(series).diff().diff()\n    elif code == 7:\n        # Delta (x_t/x_{t-1} - 1)\n        return series.pct_change()\n    else:\n        raise ValueError(\"Invalid transformation code\")\n\n# Applying the transformations to each column in df_cleaned based on transformation_codes\nfor series_name, code in transformation_codes.values:\n    df_cleaned[series_name] = apply_transformation(df_cleaned[series_name].astype(float), float(code))\n\n\n1df_cleaned = df_cleaned[2:]\n2df_cleaned.reset_index(drop=True, inplace=True)\ndf_cleaned.head()\n\n\n1\n\nSince some transformations induce missing values, we drop the first two observations of the dataset\n\n2\n\nWe reset the index so that the first observation of the dataset has index 0\n\n\n\n\n\n\n\n\n\n\n\nsasdate\nRPI\nW875RX1\nDPCERA3M086SBEA\nCMRMTSPLx\nRETAILx\nINDPRO\nIPFPNSS\nIPFINAL\nIPCONGD\n...\nDNDGRG3M086SBEA\nDSERRG3M086SBEA\nCES0600000008\nCES2000000008\nCES3000000008\nUMCSENTx\nDTCOLNVHFNM\nDTCTHFNM\nINVEST\nVIXCLSx\n\n\n\n\n0\n1959-03-01\n0.006457\n0.007325\n0.009404\n-0.003374\n0.008321\n0.014300\n0.006036\n0.004896\n0.000000\n...\n-0.001148\n0.000292\n-0.000022\n-0.008147\n0.004819\nNaN\n0.004929\n0.004138\n-0.014792\nNaN\n\n\n1\n1959-04-01\n0.006510\n0.007029\n-0.003622\n0.019915\n0.000616\n0.021080\n0.014339\n0.014545\n0.015652\n...\n0.001312\n0.001760\n-0.000022\n0.012203\n-0.004890\nNaN\n0.012134\n0.006734\n0.024929\nNaN\n\n\n2\n1959-05-01\n0.005796\n0.006618\n0.012043\n0.006839\n0.007803\n0.014954\n0.008267\n0.009580\n0.004766\n...\n-0.001695\n-0.001867\n-0.000021\n-0.004090\n-0.004819\nNaN\n0.002828\n0.002020\n-0.015342\nNaN\n\n\n3\n1959-06-01\n0.003068\n0.003012\n0.003642\n-0.000097\n0.009064\n0.001137\n0.007035\n0.007125\n-0.004766\n...\n0.003334\n0.001946\n-0.004619\n0.003992\n0.004796\nNaN\n0.009726\n0.009007\n-0.012252\nNaN\n\n\n4\n1959-07-01\n-0.000580\n-0.000762\n-0.003386\n0.012155\n-0.000330\n-0.024237\n0.001168\n0.008251\n0.013056\n...\n-0.001204\n-0.000013\n0.000000\n-0.004040\n-0.004796\nNaN\n-0.004631\n-0.001000\n0.029341\nNaN\n\n\n\n\n5 rows × 127 columns\n\n\n\n\n1import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\n2series_to_plot = ['INDPRO', 'CPIAUCSL', 'TB3MS']\nseries_names = ['Industrial Production',\n                'Inflation (CPI)',\n                '3-month Treasury Bill rate']\n\n\n# Create a figure and a grid of subplots\n3fig, axs = plt.subplots(len(series_to_plot), 1, figsize=(8, 15))\n\n# Iterate over the selected series and plot each one\nfor ax, series_name, plot_title in zip(axs, series_to_plot, series_names):\n4    if series_name in df_cleaned.columns:\n5        dates = pd.to_datetime(df_cleaned['sasdate'], format='%m/%d/%Y')\n6        ax.plot(dates, df_cleaned[series_name], label=plot_title)\n7        ax.xaxis.set_major_locator(mdates.YearLocator(base=5))\n        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n8        ax.set_title(plot_title)\n9        ax.set_xlabel('Year')\n        ax.set_ylabel('Transformed Value')\n10        ax.legend(loc='upper left')\n11        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n    else:\n        ax.set_visible(False)  # Hide plots for which the data is not available\n\n12plt.tight_layout()\n13plt.show()\n\n\n1\n\nWe use library matplotlib to plot\n\n2\n\nWe consider three series (INDPRO, CPIAUCSL, TB3MS) and assign them human-readable names (“Industrial Production”, “Inflation (CPI)”, “3-month Treasury Bill rate.”).\n\n3\n\nWe create a figure with three (len(series_to_plot)) subplots arranged vertically. The figure size is 8x15 inches.\n\n4\n\nWe check if the series exists in each series df_cleaned DataFrame columns.\n\n5\n\nWe convert the sasdate column to datetime format (not necessary, since sasdate was converter earlier)\n\n6\n\nWe plot each series against the sasdate on the corresponding subplot, labeling the plot with its human-readable name.\n\n7\n\nWe format the x-axis to display ticks and label the x-axis with dates taken every five years.\n\n8\n\nEach subplot is titled with the name of the economic indicator.\n\n9\n\nWe label the x-axis “Year,” and the y-axis “Transformed Value,” to indicate that the data was transformed before plotting.\n\n10\n\nA legend is added to the upper left of each subplot for clarity.\n\n11\n\nWe rotate the x-axis labels by 45 degrees to prevent overlap and improve legibility.\n\n12\n\nplt.tight_layout() automatically adjusts subplot parameters to give specified padding and avoid overlap.\n\n13\n\nplt.show() displays the figure with its subplots."
  },
  {
    "objectID": "comptools_ass1.html#forecasting-in-time-series",
    "href": "comptools_ass1.html#forecasting-in-time-series",
    "title": "Computational Tools for Macroeconometrics",
    "section": "Forecasting in Time Series",
    "text": "Forecasting in Time Series\nForecasting in time series analysis involves using historical data to predict future values. The objective is to model the conditional expectation of a time series based on past observations.\n\nDirect Forecasts\nDirect forecasting involves modeling the target variable directly at the desired forecast horizon. Unlike iterative approaches, which forecast one step ahead and then use those forecasts as inputs for subsequent steps, direct forecasting directly models the relationship between past observations and future value.\n\n\nARX Models\nAutoregressive Moving with predictors (ARX) models are a class of univariate time series models that extend ARMA models by incorporating exogenous (independent) variables. These models are formulated as follows:\n\\[\n\\begin{aligned}\nY_{t+h} &=  \\alpha + \\phi_0 Y_t + \\phi_1 Y_{t-1} + \\dots + \\phi_p Y_{t-p} + \\theta_{0,1} X_{t,1} + \\theta_{1,1} X_{t-1,1} + \\dots + \\theta_{p,1} X_{t-p,1} + \\dots + \\theta_{0,k} X_{t,k} + \\dots + \\theta_{p,k} X_{t-p,k} + u_{t+h}\\\\\n        &=  \\alpha + \\sum_{i=0}^p \\phi_i Y_{t-i} + \\sum_{j=1}^k\\sum_{s=0}^p \\theta_{s,j} X_{t-s,j} + \\epsilon_{t+h}\n\\end{aligned}\n\\tag{1}\\]\n\n\\(Y_{t+h}\\): The target variable at time \\(t+h\\).\n\\(X_{t,j}\\): Predictors (variable \\(j=1,\\ldots,k\\) at time \\(t\\)).\n\\(p\\) number of lags of the target and the predictors.1\n\\(\\phi_i\\), \\(i=0,\\dots,p\\), and \\(\\theta_{j,s}\\), \\(j=1,\\dots,k\\), \\(s=1,\\ldots,r\\): Parameters of the model.\n\\(\\epsilon_{t+h}\\): error term.\n\nFor instance, to predict Industrial Prediction using as predictor inflation and the 3-month t-bill, the target variable is INDPRO, and the predictors are CPIAUSL and TB3MS. Notice that the target and the predictors are the transformed variables. Thus, if we use INDPRO as the target, we are predicting the log-difference of industrial production, which is a good approximation for its month-to-month percentage change.\nBy convention, the data ranges from \\(t=1,\\ldots,T\\), where \\(T\\) is the last period, we have data (for the df_cleaned dataset, \\(T\\) corresponds to January 2024).\n\n\nForecasting with ARX\nSuppose that we know the parameters of the model for the moment. To obtain a forecast for \\(Y_{T+h}\\), the \\(h\\)-step ahead forecast, we calculate \\[\n\\begin{aligned}\n\\hat{Y}_{T+h} &=  \\alpha + \\phi_0 Y_T + \\phi_1 Y_{T-1} + \\dots + \\phi_p Y_{T-p} \\\\\n                  &\\,\\,\\quad \\quad + \\theta_{0,1} X_{T,1} + \\theta_{1,1} X_{T-1,1} + \\dots + \\theta_{p,1} X_{T-p,1} \\\\\n                  &\\,\\,\\quad \\quad + \\dots + \\theta_{0,k} X_{T,k} + \\dots + \\theta_{p,k} X_{T-p,k}\\\\\n        &=  \\alpha + \\sum_{i=0}^p \\phi_i Y_{T-i} + \\sum_{j=1}^k\\sum_{s=0}^p \\theta_{s,j} X_{T-s,j}\n\\end{aligned}\n\\]\nWhile this is conceptually easy, implementing the steps needed to calculate the forecast is insidious, and care must be taken to ensure we are calculating the correct forecast.\nTo start, it is convenient to rewrite the model in Equation 1 as a linear model \\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{u},\n\\] where \\(\\boldsymbol{\\beta}\\) is the vector (of size \\(1+(1+p)(1+k)\\)) \\[\n\\boldsymbol{\\beta}=\\begin{pmatrix}\\alpha\\\\\n\\phi_{0}\\\\\n\\vdots\\\\\n\\phi_{p}\\\\\n\\theta_{0,1}\\\\\n\\vdots\\\\\n\\theta_{p,1}\\\\\n\\vdots\\\\\n\\theta_{1,k}\\\\\n\\vdots\\\\\n\\theta_{p,k}\n\\end{pmatrix},\n\\] \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) are respectively given by \\[\n\\mathbf{y} = \\begin{pmatrix}\ny_{p+h+1}  \\\\\ny_{p+h+2}\\\\\n\\vdots \\\\\ny_{T}\n\\end{pmatrix}\n\\] and \\[\n\\mathbf{X} = \\begin{pmatrix}1 & Y_{p+1} & Y_{p} & \\cdots & Y_{1} & X_{p+1,1} & X_{p,1} & \\cdots & X_{1,1} & X_{p+1,k} & X_{p,k} & \\cdots & X_{1,k}\\\\\n\\vdots & \\vdots & \\vdots &  & \\vdots & \\vdots & \\vdots &  & \\vdots & \\vdots & \\vdots &  & \\vdots\\\\\n1 & Y_{T-h-1} & Y_{T-h-2} & \\cdots & Y_{T-h-p-1} & X_{T-h-1,1} & X_{T-h-2,1} & \\cdots & X_{T-h-p-1,1} & X_{T-h-1,k} & X_{T-h-2,k} & \\cdots & X_{T-h-p-1,k}\\\\\n1 & Y_{T-h} & Y_{T-h-1} & \\cdots & Y_{T-h-p} & X_{T-h,1} & X_{T-h-1,1} & \\cdots & X_{T-h-p,1} & X_{T-h,k} & X_{T-h-1,k} &  & X_{T-h-p,k}\n\\end{pmatrix}.\n\\] The size of \\(\\mathbf{X}\\) is \\((T-p-h)\\times 1+(1+k)(1+p)\\) and that of \\(\\mathbf{y}\\) is \\(T-h-p\\).\nThe matrix \\(\\mathbf{X}\\) can be obtained in the following way:\n\nYraw = df_cleaned['INDPRO']\nXraw = df_cleaned[['CPIAUCSL', 'TB3MS']]\n\nnum_lags  = 4  ## this is p\nnum_leads = 1  ## this is h\nX = pd.DataFrame()\n## Add the lagged values of Y\ncol = 'INDPRO'\nfor lag in range(0,num_lags+1):\n        # Shift each column in the DataFrame and name it with a lag suffix\n        X[f'{col}_lag{lag}'] = Yraw.shift(lag)\n\nfor col in Xraw.columns:\n    for lag in range(0,num_lags+1):\n        # Shift each column in the DataFrame and name it with a lag suffix\n        X[f'{col}_lag{lag}'] = Xraw[col].shift(lag)\n## Add a column on ones (for the intercept)\nX.insert(0, 'Ones', np.ones(len(X)))\n\n\n## X is now a DataFrame\nX.head()\n\n\n\n\n\n\n\n\nOnes\nINDPRO_lag0\nINDPRO_lag1\nINDPRO_lag2\nINDPRO_lag3\nINDPRO_lag4\nCPIAUCSL_lag0\nCPIAUCSL_lag1\nCPIAUCSL_lag2\nCPIAUCSL_lag3\nCPIAUCSL_lag4\nTB3MS_lag0\nTB3MS_lag1\nTB3MS_lag2\nTB3MS_lag3\nTB3MS_lag4\n\n\n\n\n0\n1.0\n0.014300\nNaN\nNaN\nNaN\nNaN\n-0.000690\nNaN\nNaN\nNaN\nNaN\n0.10\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1.0\n0.021080\n0.014300\nNaN\nNaN\nNaN\n0.001380\n-0.000690\nNaN\nNaN\nNaN\n0.15\n0.10\nNaN\nNaN\nNaN\n\n\n2\n1.0\n0.014954\n0.021080\n0.014300\nNaN\nNaN\n0.001723\n0.001380\n-0.000690\nNaN\nNaN\n-0.11\n0.15\n0.10\nNaN\nNaN\n\n\n3\n1.0\n0.001137\n0.014954\n0.021080\n0.01430\nNaN\n0.000339\n0.001723\n0.001380\n-0.00069\nNaN\n0.37\n-0.11\n0.15\n0.10\nNaN\n\n\n4\n1.0\n-0.024237\n0.001137\n0.014954\n0.02108\n0.0143\n-0.001034\n0.000339\n0.001723\n0.00138\n-0.00069\n-0.01\n0.37\n-0.11\n0.15\n0.1\n\n\n\n\n\n\n\nNote that the first \\(p=\\)4 rows of X have missing values.\nThe vector \\(\\mathbf{y}\\) can be similarly created as\n\ny = Yraw.shift(-num_leads)\ny\n\n0      0.021080\n1      0.014954\n2      0.001137\n3     -0.024237\n4     -0.034464\n         ...   \n786   -0.004547\n787   -0.001453\n788    0.009853\n789    0.005111\n790         NaN\nName: INDPRO, Length: 791, dtype: float64\n\n\nThe variable y has missing values in the last h positions (it is not possible to lead the target beyond \\(T\\)).\nNotice also that we must keep the last row of X for constructing the forecast.\nNow we create two numpy arrays with the missing values stripped:\n\n## Save last row of X (converted to numpy)\nX_T = X.iloc[-1:].values\n## Subset getting only rows of X and y from p+1 to h-1\n## and convert to numpy array\ny = y.iloc[num_lags:-num_leads].values\nX = X.iloc[num_lags:-num_leads].values\n\n\nX_T\n\narray([[ 1.00000000e+00,  5.11104809e-03,  9.85288291e-03,\n        -1.45324151e-03, -4.54694564e-03, -4.20511799e-03,\n         1.01839974e-03,  8.39506878e-04,  5.38574156e-04,\n        -2.74117085e-05,  4.89182561e-04, -6.00000000e-02,\n        -1.50000000e-01, -9.00000000e-02, -2.10000000e-01,\n        -3.30000000e-01]])\n\n\nNow, we have to estimate the parameters and obtain the forecast.\n\n\nEstimation\nThe parameters of the model can be estimated by OLS (the OLS estimates the coefficient of the linear projection of \\(Y_{t+h}\\) on its lags and the lags of \\(X_t\\)).\nThe OLS estimator of \\(\\boldsymbol{\\beta}\\) is \\[\n\\hat{\\boldsymbol{\\beta}} = (X'X)^{-1}X'Y.\n\\]\nWhile this is the formula used to describe the OLS estimator, from a computational poijnt of view is much better to define the estimator as the solution of the set of linear equations: \\[\n(X'X)\\boldsymbol{\\beta} = X'Y\n\\]\nThe function solve can be used to solve this linear system of equation.\n\nfrom numpy.linalg import solve\n# Solving for the OLS estimator beta: (X'X)^{-1} X'Y\nbeta_ols = solve(X.T @ X, X.T @ y)\n\n## Produce the One step ahead forecast\n## % change month-to-month INDPRO\nforecast = X_T@beta_ols*100\nforecast\n\narray([0.24917247])\n\n\nThe variable forecast contains now the one-step ahead (\\(h=1\\) forecast) of INDPRO. Since INDPRO has been transformed in logarithmic differences, we are forecasting the percentage change (and multiplying by 100 gives the forecast in percentage points).\nTo obtain the \\(h\\)-step ahead forecast, we must repeat all the above steps using a different h.\n\n\nForecasting Exercise\nHow good is the forecast that the model is producing? One thing we could do to assess the forecast’s quality is to wait for the new data on industrial production and see how big the forecasting error is. However, this evaluation would not be appropriate because we need to evaluate the forecast as if it were repeatedly used to forecast future values of the target variables. To properly assess the model and its ability to forecast INDPRO, we must keep producing forecasts and calculating the errors as new data arrive. This procedure would take time as we must wait for many months to have a series of errors that is large enough.\nA different approach is to do what is called a Real-time evaluation. A Real-time evaluation procedure consists of putting ourselves in the shoes of a forecaster who has been using the forecasting model for a long time.\nIn practice, that is what are the steps to follow to do a Real-time evaluation of the model:\n\nSet \\(T\\) such that the last observation of df coincides with December 1999;\nEstimate the model using the data up to \\(T\\)\nProduce \\(\\hat{Y}_{T+1}, \\hat{Y}_{T+2}, \\dots, \\hat{Y}_{T+H}\\)\nSince we have the actual data for January, February, …, we can calculate the forecasting errors of our model \\[\n\\hat{e}_{T+h} = \\hat{Y}_{T+h} - Y_{T+h}, \\,\\, h = 1,\\ldots, H.\n\\]\nSet \\(T = T+1\\) and do all the steps above.\n\nThe process results are a series of forecasting errors we can evaluate using several metrics. The most commonly used is the MSFE, which is defined as \\[\nMSFE_h = \\frac{1}{J}\\sum_{j=1}^J  \\hat{e}_{T+j+h}^2,\n\\] where \\(J\\) is the number of errors we collected through our real-time evaluation.\nThis assignment asks you to perform a real-time evaluation assessment of our simple forecasting model and calculate the MSFE for steps \\(h=1,4,8\\).\nAs a bonus, we can evaluate different models and see how they perform differently. For instance, you might consider different numbers of lags and/or different variables in the model.\n\nHint\nA sensible way to structure the code for real-time evaluation is to use several functions. For instance, you can define a function that calculates the forecast given the DataFrame.\n\ndef calculate_forecast(df_cleaned, p = 4, H = [1,4,8], end_date = '12/1/1999',target = 'INDPRO', xvars = ['CPIAUCSL', 'TB3MS']):\n\n    ## Subset df_cleaned to use only data up to end_date\n    rt_df = df_cleaned[df_cleaned['sasdate'] &lt;= pd.Timestamp(end_date)]\n    ## Get the actual values of target at different steps ahead\n    Y_actual = []\n    for h in H:\n        os = pd.Timestamp(end_date) + pd.DateOffset(months=h)\n        Y_actual.append(df_cleaned[df_cleaned['sasdate'] == os][target]*100)\n        ## Now Y contains the true values at T+H (multiplying * 100)\n\n    Yraw = rt_df[target]\n    Xraw = rt_df[xvars]\n\n    X = pd.DataFrame()\n    ## Add the lagged values of Y\n    for lag in range(0,p):\n        # Shift each column in the DataFrame and name it with a lag suffix\n        X[f'{target}_lag{lag}'] = Yraw.shift(lag)\n\n    for col in Xraw.columns:\n        for lag in range(0,p):\n            X[f'{col}_lag{lag}'] = Xraw[col].shift(lag)\n    \n    ## Add a column on ones (for the intercept)\n    X.insert(0, 'Ones', np.ones(len(X)))\n    \n    ## Save last row of X (converted to numpy)\n    X_T = X.iloc[-1:].values\n\n    ## While the X will be the same, Y needs to be leaded differently\n    Yhat = []\n    for h in H:\n        y_h = Yraw.shift(-h)\n        ## Subset getting only rows of X and y from p+1 to h-1\n        y = y_h.iloc[p:-h].values\n        X_ = X.iloc[p:-h].values\n        # Solving for the OLS estimator beta: (X'X)^{-1} X'Y\n        beta_ols = solve(X_.T @ X_, X_.T @ y)\n        ## Produce the One step ahead forecast\n        ## % change month-to-month INDPRO\n        Yhat.append(X_T@beta_ols*100)\n\n    ## Now calculate the forecasting error and return\n\n    return np.array(Y_actual) - np.array(Yhat)\n\nWith this function, you can calculate real-time errors by looping over the end_date to ensure you end the loop at the right time.\n\nt0 = pd.Timestamp('12/1/1999')\ne = []\nT = []\nfor j in range(0, 10):\n    t0 = t0 + pd.DateOffset(months=1)\n    print(f'Using data up to {t0}')\n    ehat = calculate_forecast(df_cleaned, p = 4, H = [1,4,8], end_date = t0)\n    e.append(ehat.flatten())\n    T.append(t0)\n\n## Create a pandas DataFrame from the list\nedf = pd.DataFrame(e)\n## Calculate the RMSFE, that is, the square root of the MSFE\nnp.sqrt(edf.apply(np.square).mean())\n\nUsing data up to 2000-01-01 00:00:00\nUsing data up to 2000-02-01 00:00:00\nUsing data up to 2000-03-01 00:00:00\nUsing data up to 2000-04-01 00:00:00\nUsing data up to 2000-05-01 00:00:00\nUsing data up to 2000-06-01 00:00:00\nUsing data up to 2000-07-01 00:00:00\nUsing data up to 2000-08-01 00:00:00\nUsing data up to 2000-09-01 00:00:00\nUsing data up to 2000-10-01 00:00:00\n\n\n0    0.345387\n1    0.516334\n2    0.627048\ndtype: float64\n\n\nYou may change the function calculate_forecast to output also the actual data end the forecast, so you can, for instance, construct a plot."
  },
  {
    "objectID": "comptools_ass1.html#working-with-github",
    "href": "comptools_ass1.html#working-with-github",
    "title": "Computational Tools for Macroeconometrics",
    "section": "Working with github",
    "text": "Working with github\n\n\nUsing Visual Studio Code\nUnless you are familiar with the command line and you are using Linux or MacOS, the best way to interact with github is through Visual Studio Code. Instructions on how to install Visual Studio Code on Windows are here. For MacOS the instructions are here.\nVisual Studio Code has an extension system. The extensions extend VSCode adding features that simplify writing and interacting with code.\nThe extensions you should install are\n\nJulia Extension Instructions\nPython Extension Instruction\nR Extension Instructions\n\nThere are many other extensions that you might find useful. For those, google is your friend.\n\n\nCloning the repository\nCloning a repository from GitHub into Visual Studio Code (VSCode) allows you to work on projects directly from your local machine. Here’s a detailed step-by-step guide on how to clone the repository https://github.com/uniroma/comptools-assignments into VSCode:\n\nOpen Visual Studio Code\n\n\nStart by opening Visual Studio Code on your computer.\n\n\nAccess the Command Palette\n\n\nWith VSCode open, access the Command Palette by pressing Ctrl+Shift+P on Windows/Linux or Cmd+Shift+P on macOS. This is where you can quickly access various commands in VSCode.\n\n\nClone Repository\n\n\nIn the Command Palette, type “Git: Clone” and select the option Git: Clone from the list that appears. This action will prompt VSCode to clone a repository.\n\n\nEnter the Repository URL\n\n\nA text box asking for the repository URL will appear at the top of the VSCode window. Enter https://github.com/uniroma/comptools-assignments and press Enter. (This is the URL of the assignment 1 repository).\n\n\nChoose a Directory\n\n\nNext, VSCode will ask you to select a directory where you want to clone the repository. Navigate through your file system and choose a directory that will be the local storage place for the repository. The directory should exist. Create it if it doesn’t. Once selected, the cloning process will start.\n\n\nOpen the Cloned Repository\n\n\nAfter the repository has been successfully cloned, a notification will pop up in the bottom right corner of VSCode with the option to Open Repository. Click on it. If you missed the notification, you can navigate to the directory where you cloned the repository and open it manually from within VSCode by going to File &gt; Open Folder.\n\n\nStart Working\n\n\nNow that the repository is cloned and opened in VSCode, you can start working on the project. You can edit files, commit changes, and manage branches directly from VSCode.\n\n\n\n\n\n\n\nTip\n\n\n\n\nEnsure you have Git installed on your computer to use the Git features in VSCode. If you do not have Git installed, you can download it from the official Git website. Instructions to install Git.\nIf you are working with GitHub repositories frequently, consider authenticating with GitHub in VSCode to streamline your workflow. This can be done through the Command Palette by finding the GitHub: Sign in command.\n\n\n\n\n\nMake changes and commit them to the repository\n\nMake Your Changes\n\nOpen the repository you have cloned in VSCode.\nNavigate to the file(s) you wish to change within the VSCode Explorer pane.\nMake the necessary modifications or additions to the file(s). These changes can be anything from fixing a bug to adding new features.\n\nReview Your Changes\n\n\nAfter making changes, you can see which files have been modified by looking at the Source Control panel. You can access this panel by clicking on the Source Control icon (it looks like a branch or a fork) on the sidebar or by pressing Ctrl+Shift+G (Windows/Linux) or Cmd+Shift+G (macOS) and searching for Show control panel.\nModified files are listed within the Source Control panel. Click on a file to view the changes (differences) between your working version and the last commit. Lines added are highlighted in green, and lines removed are highlighted in red.\n\n\nStage Your Changes\n\n\nBefore committing, you need to stage your changes. Staging is like preparing and reviewing exactly what changes you will commit to without making the commit final.\nYou can stage changes by right-clicking on a modified file in the Source Control panel and selecting Stage Changes. Alternatively, you can stage all changes at once by clicking the + icon next to the “Changes” header.\n\n\nCommit Your Changes\n\n\nAfter staging your changes, commit them to the repository. To do this, type a commit message in the message box at the top of the Source Control panel. This message should briefly describe the changes you’ve made.\nPress Ctrl+Enter (Windows/Linux) or Cmd+Enter (macOS) to commit the staged changes (search for Git: Commit). Alternatively, you can click the checkmark icon (Commit) at the top of the Source Control panel.\nBefore committing, you should enter a commit message that briefly describes the changes that you have made. Commit messages are essential for making the project’s history understandable for yourself and the other collaborators.\n\n\nPush Your Changes\n\n\nIf you’re working with a remote repository (like one hosted on GitHub), you must push your commits to update the remote repository with your local changes.\nYou can push changes by clicking on the three dots (...) menu in the Source Control panel, navigating to Push and selecting it. If you’re using Git in VSCode for the first time, you might be prompted to enter your GitHub credentials or authenticate in another way.\n\n\n\n\n\n\n\nTip\n\n\n\nIt’s a good practice to pull changes from the remote repository before starting your work session (to ensure you’re working with the latest version) and before pushing your changes (to ensure no conflicts). You can pull changes by clicking on the three dots (...) menu in the Source Control panel and selecting Pull.\n\n\nThe following video explores in more detail how to use git in VSCode."
  },
  {
    "objectID": "comptools_ass1.html#footnotes",
    "href": "comptools_ass1.html#footnotes",
    "title": "Computational Tools for Macroeconometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTheoretically, the number of lags for the target variables and the predictors could be different. Here, we consider the simpler case in which both are equal.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Tools for Macroeconometrics (AAF2351)",
    "section": "",
    "text": "Prof. Giuseppe Ragusa\n   Sapienza, University of Rome\n   Department of Economics and Law\n   Viale del Castro Laurenziano, 9\n   giuseppe.ragusa at uniroma1 dot it\n   Reserve an office hour:\n\n\n\n\n\n   Friday\n   17 february, 2025 - 31 may 2025\n   12:00-14:00\n   Ecodir"
  },
  {
    "objectID": "index.html#classroom",
    "href": "index.html#classroom",
    "title": "Computational Tools for Macroeconometrics (AAF2351)",
    "section": "Classroom",
    "text": "Classroom\nPlease enroll to Google Clasroom."
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Computational Tools for Macroeconometrics (AAF2351)",
    "section": "Description",
    "text": "Description\nComputational Tools for Macroeconometrics (AAF2351) covers the computational aspects of time series. It begins with an overview of the computational challenges inherent in macroeconomic data analysis and the pivotal role of software tools, with a particular focus on Python, Julia and R. The syllabus covers a broad range of topics starting gong from forecasting to nonlinear optimization and simulations.\nThe course is structured as an interactive lab, emphasizing a hands-on learning approach through practical assignments. In this lab environment, students are expected to learn by doing, applying the theoretical knowledge acquired in lectures to real-world data and scenarios."
  }
]