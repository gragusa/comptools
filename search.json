[
  {
    "objectID": "comptools_ass3.html",
    "href": "comptools_ass3.html",
    "title": "Computational Tools for Macroeconometrics",
    "section": "",
    "text": "This assignment covers two topics: Monte Carlo simulations and Bootstrap, with a focus on time series models. As usual, I will discuss the basic ideas behind the concepts and then present the assignment."
  },
  {
    "objectID": "comptools_ass3.html#monte-carlo-experiments",
    "href": "comptools_ass3.html#monte-carlo-experiments",
    "title": "Computational Tools for Macroeconometrics",
    "section": "Monte Carlo experiments",
    "text": "Monte Carlo experiments\nMonte Carlo experiments is a handy tool to assess the quality of the asymptotic approximation of econometric estimators.\nConsider the following linear model \\[\ny_t = \\beta^o_0 + \\beta^o_1 x_t + u_t, t = 1,\\dots, T,\n\\] where \\(y_t\\), \\(x_t\\), and \\(u_t\\) are random variables and \\(\\beta^o_0\\) and \\(\\beta^o_1\\) are parameters to be estimated. In matrix form, the model can be written as \\[\nY_t = \\mathbf{X}_t \\beta^o + U_t,\n\\] where \\[\n\\underbracket{Y}_{(T\\times 1)}=\\begin{pmatrix}Y_{1}\\\\\n\\vdots\\\\\nY_{T}\n\\end{pmatrix},\\,\\underbracket{\\mathbf{X}}_{T\\times 2}=\\begin{pmatrix}1 & x_{1}\\\\\n\\vdots & \\vdots\\\\\n1 & x_{T}\n\\end{pmatrix},\\,\\underbracket{U}_{(T\\times 1)}=\\begin{pmatrix}u_{1}\\\\\n\\vdots\\\\\nu_{T}\n\\end{pmatrix},\\,\\underbracket{\\beta^{o}}_{(2\\times 1)}=\\begin{pmatrix}\\beta_{0}^{o}\\\\\n\\beta_{1}^{o}\n\\end{pmatrix}\n\\]\nWe will assume that 1. \\((u_t, x_t)\\) are i.i.d. over \\(t\\); 2. \\(E(u_t|x_t) = 0\\), \\(t=1,\\dots,T\\); 3. \\(E(|x_t|^4)&lt;\\infty\\) and \\(E(|u_t|^4)&lt;\\infty\\); 4. \\(E(u^t|x_t) = \\sigma^2&gt;0\\).\nUnder these assumptions, the OLS estimator of \\(\\beta^o\\), \\[\n\\hat{\\beta}^{ols} = \\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\mathbf{X}'Y\n\\] is unbiased, consistent, and asymptotically normal: \\[\nE(\\hat{\\beta}^{ols}) = \\beta^o, \\,\\,\\, \\hat{\\beta}^{ols} \\xrightarrow{p} \\beta^o,\\,\\,\\, \\left(\\hat{\\beta}^{ols}-\\beta^o\\right) \\xrightarrow{d} N\\left[0, \\sigma^2E\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\right].\n\\]\nThese results are derived theoretically. For unbiasedness, the law of iterated expectations and the iid of \\((u_t,x_t)\\) gives \\[\nE\\left(E\\left(\\hat{\\beta}^{ols}|\\mathbf{X}\\right)\\right)=\\beta^{o}+E\\left(\\left(\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\mathbf{X}'E(U|\\mathbf{X})\\right)=\\beta^{o}.\n\\] Under the iid assumption and the restrictions on the fourth moments of \\(u_t\\) and \\(x_t\\), we have \\[\n\\frac{1}{T}\\mathbf{X}'U=\\frac{1}{n}\\sum_{t=1}^{T}\\begin{pmatrix}u_{t}\\\\\nx_{t}u_{t},\n\\end{pmatrix}\\xrightarrow{p}\\begin{pmatrix}0\\\\\n0\n\\end{pmatrix},\\text{ and }\\frac{1}{T}\\mathbf{X}'\\mathbf{X}=\\frac{1}{T}\\sum_{t=1}^{T}\\begin{pmatrix}1 & x_{t}\\\\\nx_{t}, & x_{t}^{2}\n\\end{pmatrix}\\xrightarrow{p}\\begin{pmatrix}1 & E(x_{t})\\\\\nE(x_{t}), & E(x_{t}^{2})\n\\end{pmatrix}.\n\\] These convergences in probability deliver consistency of the OLS estimator \\[\n\\hat{\\beta}^{ols}=\\beta^{o}+\\left(\\frac{1}{T}\\mathbf{X}'\\mathbf{X}\\right)^{-1}\\frac{1}{T}\\mathbf{X}'U\\xrightarrow{p}\\beta^{o}+\\begin{pmatrix}1 & E(x_{t})\\\\\nE(x_{t}), & E(x_{t}^{2})\n\\end{pmatrix}^{-1}\\begin{pmatrix}0\\\\\n0\n\\end{pmatrix}=\\beta^{o}.\n\\] Finally, iid, the moments’ restrictions, and the homoskedasticity assumptions, give \\[\n\\sqrt{T}\\left(\\hat{\\beta}^{ols}-\\beta^{o}\\right)\\xrightarrow{d}N\\left[0,\\sigma^{2}\\begin{pmatrix}1 & E(x_{t})\\\\\nE(x_{t}), & E(x_{t}^{2})\n\\end{pmatrix}^{-1}\\right].\n\\]\nConsistency and asymptotic results, and they hold as \\(T\\to\\infty\\). Given a sample of size \\(T\\), say \\(T=50\\), how close is the distribution of \\(\\hat{\\beta}^o\\) the normal one postulated by the usual asymptotic theory? How close is \\(\\hat{\\beta}^o\\) to \\(\\beta^o\\)? Unfortunately, theory only tells us that the larger the sample size, the better the normal approximation. Similarly, it tells us that \\(\\hat{\\beta}^o-\\beta^o\\) is smaller the larger the sample size.\nMonte Carlo simulations can help us determine how good these approximations are for a given size of \\(T\\). The idea is simple: we simulate a sample of \\((u_t, x_t)\\), \\(t=1,\\ldots,T\\) from which we generate \\(y_t\\) (using a arbitrary value for \\(\\beta^o_0\\) and \\(\\beta^o_1\\).) We estimate the parameters using the simulated data. We repeat this operation many times, saving the estimated parameters. The saved parameters give the empirical distribution of the OLS estimator and help verify how closely the theory matches the empirical distribution.\nThe following code does a Monte Carlo for the linear model above:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Set parameters\nT = 20\nbeta_0_true = 1\nbeta_1_true = 2\nsigma = 1\nnum_simulations = 10000\n\n# Arrays to store the estimates from each simulation\nbeta_0_estimates = np.zeros(num_simulations)\nbeta_1_estimates = np.zeros(num_simulations)\n\n# Run simulations\nfor i in range(num_simulations):\n x = np.random.normal(0, 1, T)\n u = np.random.normal(0, sigma, T)\n y = beta_0_true + beta_1_true * x + u\n\n    # OLS estimation\n X = np.vstack([np.ones(T), x]).T\n beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n beta_0_estimates[i] = beta_hat[0]\n beta_1_estimates[i] = beta_hat[1]\n\n# Plotting the results\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\n\n# Distribution of beta_0\nax[0].hist(beta_0_estimates, bins=100, alpha=0.5, density=True)\nxmin, xmax = ax[0].get_xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, beta_0_true, 1/np.sqrt(T))\nax[0].plot(x, p, 'k', linewidth=2)\nax[0].set_title(f'Empirical vs. Normal Approximation for $\\\\hat{{\\\\beta}}_0$')\n\n# Distribution of beta_1\nax[1].hist(beta_1_estimates, bins=100, alpha=0.50, density=True)\nxmin, xmax = ax[1].get_xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, beta_1_true, 1/np.sqrt(T))\nax[1].plot(x, p, 'k', linewidth=2)\nax[1].set_title(f'Empirical vs. Normal Approximation')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe approximation is excellent, even if \\(T=20\\). This might sound surprising since the normal distribution of \\(\\hat{\\beta}^{ols}\\) is normal when \\(T\\) is large. We get such a close agreement between the empirical distribution of \\(\\hat{\\beta}^{ols}\\) and the theoretical one because we are simulating \\(u_t\\) the data from a normal distribution independently from \\(x_t\\). When \\(u_t|x_t\\sim N(0,\\sigma^2)\\), the distribution of the OLS estimator is precisely normal, even when \\(T=3\\). Of course, if \\(T\\) is small, the estimator’s variance will be larger, but a normal distribution will approximate the OLS estimators’ distribution.\nInstead of squinting at the histograms and the density implied by the CLT, we can modify the code to calculate the confidence interval at each simulation and see how many times the “true” values of the parameters fall into all the intervals generated. If the approximation is good, a 95% confidence interval should contain the “true” parameters 95% of the time.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Set parameters\nT = 50\nbeta_0_true = 1\nbeta_1_true = 2\nsigma = 1\nnum_simulations = 10000\n\n# Arrays to store the estimates from each simulation\nbeta_0_estimates = np.zeros(num_simulations)\nbeta_1_estimates = np.zeros(num_simulations)\nbeta_0_in = np.zeros(num_simulations)\nbeta_1_in = np.zeros(num_simulations)\n\n# Run simulations\nfor i in range(num_simulations):\n x = np.random.normal(0,1,T)\n u = np.random.normal(0,sigma,T)\n y = beta_0_true + beta_1_true * x + u\n    # OLS estimation\n X = np.vstack([np.ones(T), x]).T\n XXinv = np.linalg.inv(X.T @ X)\n beta_hat = XXinv @ X.T @ y\n beta_0_estimates[i] = beta_hat[0]\n beta_1_estimates[i] = beta_hat[1]\n u_hat = y - beta_hat[0] - beta_hat[1] * x\n sigma2_hat = np.dot(u_hat, u_hat)/(T-2)\n variance_hat = sigma2_hat*XXinv\n se_0 = np.sqrt(variance_hat[0,0])\n se_1 = np.sqrt(variance_hat[1,1])\n    ## Check weather beta_0 in CI 95%\n beta_0_in[i] = beta_hat[0] - 1.965*se_0 &lt; beta_0_true &lt; beta_hat[0] + 1.965*se_0\n beta_1_in[i] = beta_hat[1] - 1.965*se_1 &lt; beta_1_true &lt; beta_hat[1] + 1.965*se_1\n\n# Output the results\nprint(f\"Empirical 95% CI for beta_0: {np.mean(beta_0_in)}\")\nprint(f\"Empirical 95% CI for beta_1: {np.mean(beta_1_in)}\")\n\nEmpirical 95% CI for beta_0: 0.9457\nEmpirical 95% CI for beta_1: 0.9428\n\n\nThe empirical coverage of the confidence intervals is only in the neighborhood of 95%. The reason is that we are estimating the variance of the coefficient, and in this case, the distribution of \\(\\hat{\\beta}_1/se(\\hat{\\beta}_1)\\) has a \\(t\\)-student distribution with \\(T-1\\) degrees of freedom. If you run the code above for \\(T=50\\), the confidence interval coverage will be closer to 95%.\nLet us see what happens if we simulate \\(x_t\\) and \\(u_t\\) from a chi-squared distribution with \\(5\\) of freedom centered in a way to have mean zero and unit variance.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Set parameters\nT = 50\nbeta_0_true = 1\nbeta_1_true = 2\nsigma = 1\nnum_simulations = 10000\n\n# Arrays to store the estimates from each simulation\nbeta_0_estimates = np.zeros(num_simulations)\nbeta_1_estimates = np.zeros(num_simulations)\nbeta_0_in = np.zeros(num_simulations)\nbeta_1_in = np.zeros(num_simulations)\n\n# Run simulations\nfor i in range(num_simulations):\n x = (np.random.chisquare(4,T) - 4)/np.sqrt(2*4)\n u = (np.random.chisquare(4,T) - 4)/np.sqrt(2*4)\n y = beta_0_true + beta_1_true * x + u\n    # OLS estimation\n X = np.vstack([np.ones(T), x]).T\n XXinv = np.linalg.inv(X.T @ X)\n beta_hat = XXinv @ X.T @ y\n beta_0_estimates[i] = beta_hat[0]\n beta_1_estimates[i] = beta_hat[1]\n u_hat = y - beta_hat[0] - beta_hat[1] * x\n sigma2_hat = np.dot(u_hat, u_hat)/(T-2)\n variance_hat = sigma2_hat*XXinv\n se_0 = np.sqrt(variance_hat[0,0])\n se_1 = np.sqrt(variance_hat[1,1])\n    ## Check weather beta_0 in CI 95%\n beta_0_in[i] = beta_hat[0] - 1.965*se_0 &lt; beta_0_true &lt; beta_hat[0] + 1.965*se_0\n beta_1_in[i] = beta_hat[1] - 1.965*se_1 &lt; beta_1_true &lt; beta_hat[1] + 1.965*se_1\n\n# Output the results\nprint(f\"Empirical 95% CI for beta_0: {np.mean(beta_0_in)}\")\nprint(f\"Empirical 95% CI for beta_1: {np.mean(beta_1_in)}\")\n\nEmpirical 95% CI for beta_0: 0.9397\nEmpirical 95% CI for beta_1: 0.9485\n\n\nEven a \\(T=50\\), there is some difference between the empirical and theoretical coverage.\nWe can use the simulations to see what happens when the conditions on the moments of \\(u_t\\) and \\(x_t\\) are unsatisfied.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Set parameters\nT = 150\nbeta_0_true = 1\nbeta_1_true = 2\nsigma = 1\nnum_simulations = 10000\n\n# Arrays to store the estimates from each simulation\nbeta_0_estimates = np.zeros(num_simulations)\nbeta_1_estimates = np.zeros(num_simulations)\nbeta_0_in = np.zeros(num_simulations)\nbeta_1_in = np.zeros(num_simulations)\n\n# Run simulations\nfor i in range(num_simulations):\n x = np.random.standard_cauchy(T)\n u = np.random.standard_cauchy(T)\n y = beta_0_true + beta_1_true * x + u\n    # OLS estimation\n X = np.vstack([np.ones(T), x]).T\n XXinv = np.linalg.inv(X.T @ X)\n beta_hat = XXinv @ X.T @ y\n beta_0_estimates[i] = beta_hat[0]\n beta_1_estimates[i] = beta_hat[1]\n u_hat = y - beta_hat[0] - beta_hat[1] * x\n sigma2_hat = np.dot(u_hat, u_hat)/(T-2)\n variance_hat = sigma2_hat*XXinv\n se_0 = np.sqrt(variance_hat[0,0])\n se_1 = np.sqrt(variance_hat[1,1])\n    ## Check weather beta_0 in CI 95%\n beta_0_in[i] = beta_hat[0] - 1.965*se_0 &lt; beta_0_true &lt; beta_hat[0] + 1.965*se_0\n beta_1_in[i] = beta_hat[1] - 1.965*se_1 &lt; beta_1_true &lt; beta_hat[1] + 1.965*se_1\n\n# Output the results\nprint(f\"Empirical 95% CI for beta_0: {np.mean(beta_0_in)}\")\nprint(f\"Empirical 95% CI for beta_1: {np.mean(beta_1_in)}\")\n\nEmpirical 95% CI for beta_0: 0.9764\nEmpirical 95% CI for beta_1: 0.9541"
  },
  {
    "objectID": "comptools_ass3.html#bootstrap",
    "href": "comptools_ass3.html#bootstrap",
    "title": "Computational Tools for Macroeconometrics",
    "section": "Bootstrap",
    "text": "Bootstrap\nThe bootstrap is a statistical tool for estimating the distribution of a statistic based on random sampling with replacement. It allows for assessing a statistic’s variability and is beneficial in settings where the theoretical distribution of the statistic is unknown or difficult to derive.\nIn the context of a linear regression model, we can use the bootstrap to perform inference on the regression coefficients. Despite its theoretical intricacies, the bootstrap is conceptually simple and describing its application can be done quite easily.\nInput: Dataset \\((X, y)\\) with \\(n\\) observations Fit original model: Estimate coefficients \\(\\beta\\) from original dataset Store these coefficients coefficients Bootstrap procedure: For \\(b = 1\\) to $ (number of bootstrap samples): Create “bootstrap” sample \\((X^*, y^*)\\) by randomly sampling \\(n\\) observations with replacement from original dataset Fit linear model to bootstrap sample Estimate and store bootstrap coefficients \\(\\beta^*\\) Calculate statistics: Compute variance of bootstrap coefficient estimates Optionally calculate confidence intervals using percentiles of bootstrap distribution Output: Original coefficients, bootstrap variance estimates, confidence intervals\n\nimport numpy as np\n\n# Set random seed for reproducibility\nnp.random.seed(0)\n\n# Generate some sample data\nT = 100  # Number of observations\nx = np.random.normal(0, 1, T)\nu = np.random.normal(0, 1, T)\nbeta_0_true = 1\nbeta_1_true = 2\n\n# Simulate response variable y\ny = beta_0_true + beta_1_true * x + u\n\n# Function to fit linear model\ndef fit_linear_model(x, y):\n  X = np.vstack([np.ones(len(x)), x]).T\n  beta_hat = np.linalg.inv(X.T @ X) @ (X.T @ y)\n  return beta_hat\n\n# Initial fit\ninitial_beta = fit_linear_model(x, y)\n\n# Number of bootstrap samples\nB = 1000\nbootstrap_estimates = np.zeros((B, 2))\n\n# Perform bootstrap resampling\nfor i in range(B):\n  indices = np.random.choice(range(T), size=T, replace=True)\n  x_resampled = x[indices]\n  y_resampled = y[indices]\n  bootstrap_estimates[i] = fit_linear_model(x_resampled, y_resampled)\n\n# Compute standard errors\nstandard_errors = bootstrap_estimates.std(axis=0)\n\nprint(\"Bootstrap Standard Errors:\")\nprint(\"SE(beta_0):\", standard_errors[0])\nprint(\"SE(beta_1):\", standard_errors[1])\n\nprint(\"LM Standard Errors:\")\nimport statsmodels.api as sm\nX = sm.add_constant(x)\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n# Standard errors from statsmodels\nstatsmodels_se = results.bse\nprint(\"Standard Errors from statsmodels OLS:\")\nprint(\"SE(beta_0):\", statsmodels_se[0])\nprint(\"SE(beta_1):\", statsmodels_se[1])\n\nBootstrap Standard Errors:\nSE(beta_0): 0.10061319113712466\nSE(beta_1): 0.09469134082416591\nLM Standard Errors:\nStandard Errors from statsmodels OLS:\nSE(beta_0): 0.10404543947505016\nSE(beta_1): 0.10305046686003079"
  },
  {
    "objectID": "comptools_ass3.html#linear-model-with-dependent-data",
    "href": "comptools_ass3.html#linear-model-with-dependent-data",
    "title": "Computational Tools for Macroeconometrics",
    "section": "Linear model with dependent data",
    "text": "Linear model with dependent data\nSo far, we have assumed that \\((u_t, x_t)\\) are iid. With macroeconomic data, it is often the case that \\((u_t, x_t)\\) are correlated.\nConsider the following model: \\[\ny_t = \\beta_0 + \\beta_1 x_t + u_t,\n\\] with \\[\nx_t = \\phi_x x_{t-1} + \\eta_i,\\,\\, |\\phi_x|&lt;1\n\\] and \\[\nu_t = \\phi_u u_{t-1} + \\varepsilon_t, \\,\\, |\\phi_u|&lt;1.\n\\]\nIf \\(\\eta_i\\) and \\(\\varepsilon_t\\) are independent, we will have that \\(E(u_t|x_t)=0\\) and so we can consistently estimate \\(\\beta_1\\) by OLS. The asymptotic distribution of the OLS estimator will be normal, but the variance of this distribution is difficult to estimate. If we use the standard errors that do not consider the correlation over time of the variables, the inference will be off. We perform a simple Monte Carlo to see the impact of serial correlation in \\(x_t\\) and \\(u_t\\).\n\ndef simulate_ar1(n, phi, sigma):\n  \"\"\"\n  Simulate an AR(1) process.\n\n  Parameters:\n  n (int): Number of observations.\n  phi (float): Coefficient of AR(1) process.\n  sigma (float): Standard deviation of the innovation term.\n\n  Returns:\n  np.array: Simulated AR(1) error terms.\n  \"\"\"\n  errors = np.zeros(n)\n  eta = np.random.normal(0, sigma, n)  # white noise\n  for t in range(1, n):\n    errors[t] = phi * errors[t - 1] + eta[t]\n  return errors\n\ndef simulate_regression_with_ar1_errors(n, beta0, beta1, phi_x, phi_u, sigma):\n  \"\"\"\n  Simulate a regression model with AR(1) error terms. \n  Parameters:\n    n (int): Number of observations.\n    beta0 (float): Intercept of the regression model.\n    beta1 (float): Slope of the regression model.\n    phi (float): Coefficient of the AR(1) process in the error term.\n    sigma (float): Standard deviation of the innovation term in the AR(1) process.  \n    Returns:\n  tuple: x (independent variable), y (dependent variable), errors (AR(1) process)\n  \"\"\"\n  x = simulate_ar1(n, phi_x, sigma)\n  u = simulate_ar1(n, phi_u, sigma)\n  y = beta0 + beta1 * x + u\n  return x, y, u\n\nT = 500              # Number of observations\nbeta0 = 1.           # Intercept\nbeta1 = 2           # Slope\nphi_x = 0.7             # AR(1) coefficient for x\nphi_u = 0.7             # AR(1) coefficient for the errors\nsigma = 1             # Standard deviation of the white noise\n\n# Simulating the model\n\n## Do monte carlo\nt_stats_hc = []\nt_stats_hac = []\n\nfor i in range(1000):\n  x, y, errors = simulate_regression_with_ar1_errors(T, beta0, beta1, phi_x, phi_u, sigma)\n  X = sm.add_constant(x)\n  model = sm.OLS(y, X).fit(cov_type='HC1')\n  t_stats_hc.append(model.t_test('x1=2').tvalue)\n     ## Use HAC: takes into account serial correlation \n  model2 = sm.OLS(y, X).fit(cov_type='HAC', cov_kwds={'maxlags': np.floor(1.3*T**(1/2)).astype(int)})\n  t_stats_hac.append(model2.t_test('x1=2').tvalue)  \n\n## Check we reject the null hypothesis at alpha=0.05 about 5% of the time\n\nprint(f\"Empirical size test beta_1=2 using White SE: {np.mean(np.abs(np.array(t_stats_hc)) &gt; 1.965)}\")\nprint(f\"Empirical size test beta_1=2 using HAC SE: {np.mean(np.abs(np.array(t_stats_hac)) &gt; 1.965)}\")\n\nEmpirical size test beta_1=2 using White SE: 0.255\nEmpirical size test beta_1=2 using HAC SE: 0.075\n\n\nWe can use the bootstrap to obtain the standard errors of \\(\\hat{\\beta}_1\\). Unfortunately, with time-dependent data, the bootstrap needs to be modified. Instead of sampling with replacement observations from \\(y_t\\) and \\(x_t\\), we will sample blocks of length \\(\\ell\\) with replacement. By resampling blocks, we ensure that the correlation in the data is preserved.\nThe moving block bootstrap (MBB) adapts the traditional bootstrap method to handle data where observations are dependent, such as in time series analysis. This approach involves resampling consecutive observation blocks to preserve the data’s internal structure and dependence.\n\nSteps for Moving Block Bootstrap (MBB):\n\n** Choose Block Length **: Determine the length of the blocks, \\(\\ell\\), that will be resampled. This length should be chosen based on the data’s correlation structure: it should be large enough to capture the dependence within the data.\nGenerate Blocks: From the original dataset of size \\(T\\), generate new datasets by sampling blocks of length \\(\\ell\\) and concatenate them until reaching the size \\(T\\). Blocks should be sampled with replacement.\nResample Data within Blocks: Sample \\(y_t\\) and \\(x_t\\) using the sampled blocks. This is the bootstrapped dataset.\nRefit the Model: Fit the regression model to each bootstrapped dataset and collect the estimated parameters for each dataset.\nCalculate Statistics: Calculate the standard deviation of the bootstrap estimate. These are the standard errors.\n\n\nimport numpy as np\nimport statsmodels.api as sm\n\ndef moving_block_bootstrap(x, y, block_length, num_bootstrap):\n  T = len(y)  # Total number of observations\n  num_blocks = T // block_length + (1 if T % block_length else 0)\n\n  # Fit the original model\n  X = sm.add_constant(x)\n  original_model = sm.OLS(y, X)\n  original_results = original_model.fit()\n\n  bootstrap_estimates = np.zeros((num_bootstrap, 2))  # Storing estimates for beta_0 and beta_1\n\n  # Perform the bootstrap\n  for i in range(num_bootstrap):\n    # Create bootstrap sample\n    bootstrap_indices = np.random.choice(np.arange(num_blocks) * block_length, size=num_blocks, replace=True)\n    bootstrap_sample_indices = np.hstack([np.arange(index, min(index + block_length, T)) for index in bootstrap_indices])\n    bootstrap_sample_indices = bootstrap_sample_indices[:T]  # Ensure the bootstrap sample is the same size as the original data\n\n    x_bootstrap = x[bootstrap_sample_indices]\n    y_bootstrap = y[bootstrap_sample_indices]\n\n    # Refit the model on bootstrap sample\n    X_bootstrap = sm.add_constant(x_bootstrap)\n    bootstrap_model = sm.OLS(y_bootstrap, X_bootstrap)\n    bootstrap_results = bootstrap_model.fit()\n\n    # Store the estimates\n    bootstrap_estimates[i, :] = bootstrap_results.params\n    \n    return bootstrap_estimates\n\n# Run moving block bootstrap\nblock_length = 12\nnum_bootstrap = 1000\nx, y, errors = simulate_regression_with_ar1_errors(200, beta0, beta1, phi_x, phi_u, sigma)\nbootstrap_results = moving_block_bootstrap(x, y, block_length, num_bootstrap)\n\n# Calculate and print standard errors\nbootstrap_standard_errors = bootstrap_results.std(axis=0)\nprint(\"Bootstrap Standard Errors:\")\nprint(\"SE(beta_0):\", bootstrap_standard_errors[0])\nprint(\"SE(beta_1):\", bootstrap_standard_errors[1])\n\nBootstrap Standard Errors:\nSE(beta_0): 0.026787210831234053\nSE(beta_1): 0.05979213797333231"
  },
  {
    "objectID": "comptools_ass3.html#assignment-3",
    "href": "comptools_ass3.html#assignment-3",
    "title": "Computational Tools for Macroeconometrics",
    "section": "Assignment 3",
    "text": "Assignment 3\n\nTask\nApply Monte Carlo simulations combined with bootstrap methods to evaluate the quality of inference on \\(\\beta_1\\)​ using serially correlated data.\n\nSteps\n\nSimulate data according to simulate_regression_with_ar1_errors.\nCalculate bootstrap standard errors.\nConstruct a 95% confidence interval for \\(\\beta_1\\)​ using both the bootstrap and the theoretical standard errors.\nPerform Monte Carlo simulations for \\(T=100\\) and \\(T=500\\), and assess the empirical coverage of the confidence intervals."
  },
  {
    "objectID": "comptools_ass1.html",
    "href": "comptools_ass1.html",
    "title": "Computational Tools for Macroeconometrics",
    "section": "",
    "text": "This assignment introduces to practical and theoretical aspects of macroeconometrics, focusing on forecasting using the FRED-MD dataset. You will learn to handle macroeconomic data, perform necessary transformations, apply univariate models to predict key economic indicators and to evaluate these forecasts.\nThis is the first assignment and it will only be considered toward your final grade if it is satisfactory. It does not mean that you won’t have to put effort into it. It is important that you try to do it so you can start learning the basics of the programming language of your choice. Also, it will help you familiarize with GitHub, Visual Studio code and other tools you will need for the remainder of the course."
  },
  {
    "objectID": "comptools_ass1.html#introduction",
    "href": "comptools_ass1.html#introduction",
    "title": "Computational Tools for Macroeconometrics",
    "section": "",
    "text": "This assignment introduces to practical and theoretical aspects of macroeconometrics, focusing on forecasting using the FRED-MD dataset. You will learn to handle macroeconomic data, perform necessary transformations, apply univariate models to predict key economic indicators and to evaluate these forecasts.\nThis is the first assignment and it will only be considered toward your final grade if it is satisfactory. It does not mean that you won’t have to put effort into it. It is important that you try to do it so you can start learning the basics of the programming language of your choice. Also, it will help you familiarize with GitHub, Visual Studio code and other tools you will need for the remainder of the course."
  },
  {
    "objectID": "comptools_ass1.html#the-fred-md-dataset",
    "href": "comptools_ass1.html#the-fred-md-dataset",
    "title": "Computational Tools for Macroeconometrics",
    "section": "The FRED-MD dataset",
    "text": "The FRED-MD dataset\nThe FRED-MD dataset is a comprehensive monthly database for macroeconomic research compiled by the Federal Reserve Bank of St. Louis. It features a wide array of economic indicators. The list of economic indicators can be obtained from the paper accompanying the data pdf.\nThe data can be downloaded here. The page contains all the different vintages of the data.\nLet us start to download the current.csv file:\n\nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('https://www.stlouisfed.org/-/media/project/frbstl/stlouisfed/research/fred-md/monthly/current.csv?sc_lang=en&hash=80445D12401C59CF716410F3F7863B64')\n\n# Clean the DataFrame by removing the row with transformation codes\ndf_cleaned = df.drop(index=0)\ndf_cleaned.reset_index(drop=True, inplace=True)\ndf_cleaned['sasdate'] = pd.to_datetime(df_cleaned['sasdate'], format='%m/%d/%Y')\ndf_cleaned\n\n\n\n\n\n\n\n\nsasdate\nRPI\nW875RX1\nDPCERA3M086SBEA\nCMRMTSPLx\nRETAILx\nINDPRO\nIPFPNSS\nIPFINAL\nIPCONGD\n...\nDNDGRG3M086SBEA\nDSERRG3M086SBEA\nCES0600000008\nCES2000000008\nCES3000000008\nUMCSENTx\nDTCOLNVHFNM\nDTCTHFNM\nINVEST\nVIXCLSx\n\n\n\n\n0\n1959-01-01\n2583.560\n2426.0\n15.188\n2.766768e+05\n18235.77392\n21.9616\n23.3868\n22.2620\n31.6664\n...\n18.294\n10.152\n2.13\n2.45\n2.04\nNaN\n6476.00\n12298.00\n84.2043\nNaN\n\n\n1\n1959-02-01\n2593.596\n2434.8\n15.346\n2.787140e+05\n18369.56308\n22.3917\n23.7024\n22.4549\n31.8987\n...\n18.302\n10.167\n2.14\n2.46\n2.05\nNaN\n6476.00\n12298.00\n83.5280\nNaN\n\n\n2\n1959-03-01\n2610.396\n2452.7\n15.491\n2.777753e+05\n18523.05762\n22.7142\n23.8459\n22.5651\n31.8987\n...\n18.289\n10.185\n2.15\n2.45\n2.07\nNaN\n6508.00\n12349.00\n81.6405\nNaN\n\n\n3\n1959-04-01\n2627.446\n2470.0\n15.435\n2.833627e+05\n18534.46600\n23.1981\n24.1903\n22.8957\n32.4019\n...\n18.300\n10.221\n2.16\n2.47\n2.08\nNaN\n6620.00\n12484.00\n81.8099\nNaN\n\n\n4\n1959-05-01\n2642.720\n2486.4\n15.622\n2.853072e+05\n18679.66354\n23.5476\n24.3911\n23.1161\n32.5567\n...\n18.280\n10.238\n2.17\n2.48\n2.08\n95.3\n6753.00\n12646.00\n80.7315\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n788\n2024-09-01\n19993.464\n16283.1\n121.690\n1.541305e+06\n716388.00000\n102.5873\n100.4044\n100.1100\n102.0602\n...\n119.220\n128.682\n31.45\n36.18\n28.01\n70.1\n553347.06\n934283.59\n5368.5671\n17.6597\n\n\n789\n2024-10-01\n20067.376\n16340.0\n121.904\n1.538666e+06\n720393.00000\n102.1219\n99.6821\n99.0178\n101.4336\n...\n119.218\n129.176\n31.53\n36.27\n28.07\n70.5\n554951.25\n938525.34\n5407.2449\n19.9478\n\n\n790\n2024-11-01\n20111.246\n16385.1\n122.435\n1.544822e+06\n725079.00000\n101.9736\n99.5645\n99.0025\n101.0038\n...\n119.230\n129.390\n31.59\n36.26\n28.22\n71.8\n556075.09\n941204.79\n5382.5669\n15.9822\n\n\n791\n2024-12-01\n20136.069\n16407.9\n123.103\n1.555153e+06\n730300.00000\n102.9833\n100.2940\n99.6550\n101.3436\n...\n119.746\n129.875\n31.73\n36.46\n28.33\n74.0\n558854.68\n946489.00\n5370.9871\n15.6997\n\n\n792\n2025-01-01\n20248.025\n16464.7\n122.519\nNaN\n723853.00000\n103.5110\n101.1431\n100.7228\n102.1889\n...\n120.453\n130.196\n31.90\n36.54\n28.55\n71.7\nNaN\nNaN\n5372.8381\n16.8122\n\n\n\n\n793 rows × 127 columns\n\n\n\n\n# Extract transformation codes\ntransformation_codes = df.iloc[0, 1:].to_frame().reset_index()\ntransformation_codes.columns = ['Series', 'Transformation_Code']\n\nThe transformation codes map variables to the transformations we must apply to each variable to render them (approximately) stationary. The data frame transformation_codes has the variable’s name (Series) and its transformation (Transformation_Code). There are six possible transformations (\\(x_t\\) denotes the variable to which the transformation is to be applied):\n\ntransformation_code=1: no trasformation\ntransformation_code=2: \\(\\Delta x_t\\)\ntransformation_code=3: \\(\\Delta^2 x_t\\)\ntransformation_code=4: \\(log(x_t)\\)\ntransformation_code=5: \\(\\Delta log(x_t)\\)\ntransformation_code=6: \\(\\Delta^2 log(x_t)\\)\ntransformation_code=7: \\(\\Delta (x_t/x_{t-1} - 1)\\)\n\nWe can apply these transformations using the following code:\n\nimport numpy as np\n\n# Function to apply transformations based on the transformation code\ndef apply_transformation(series, code):\n    if code == 1:\n        # No transformation\n        return series\n    elif code == 2:\n        # First difference\n        return series.diff()\n    elif code == 3:\n        # Second difference\n        return series.diff().diff()\n    elif code == 4:\n        # Log\n        return np.log(series)\n    elif code == 5:\n        # First difference of log\n        return np.log(series).diff()\n    elif code == 6:\n        # Second difference of log\n        return np.log(series).diff().diff()\n    elif code == 7:\n        # Delta (x_t/x_{t-1} - 1)\n        return series.pct_change()\n    else:\n        raise ValueError(\"Invalid transformation code\")\n\n# Applying the transformations to each column in df_cleaned based on transformation_codes\nfor series_name, code in transformation_codes.values:\n    df_cleaned[series_name] = apply_transformation(df_cleaned[series_name].astype(float), float(code))\n\n\n1df_cleaned = df_cleaned[2:]\n2df_cleaned.reset_index(drop=True, inplace=True)\ndf_cleaned.head()\n\n\n1\n\nSince some transformations induce missing values, we drop the first two observations of the dataset\n\n2\n\nWe reset the index so that the first observation of the dataset has index 0\n\n\n\n\n\n\n\n\n\n\n\nsasdate\nRPI\nW875RX1\nDPCERA3M086SBEA\nCMRMTSPLx\nRETAILx\nINDPRO\nIPFPNSS\nIPFINAL\nIPCONGD\n...\nDNDGRG3M086SBEA\nDSERRG3M086SBEA\nCES0600000008\nCES2000000008\nCES3000000008\nUMCSENTx\nDTCOLNVHFNM\nDTCTHFNM\nINVEST\nVIXCLSx\n\n\n\n\n0\n1959-03-01\n0.006457\n0.007325\n0.009404\n-0.003374\n0.008321\n0.014300\n0.006036\n0.004896\n0.000000\n...\n-0.001148\n0.000292\n-0.000022\n-0.008147\n0.004819\nNaN\n0.004929\n0.004138\n-0.014792\nNaN\n\n\n1\n1959-04-01\n0.006510\n0.007029\n-0.003622\n0.019915\n0.000616\n0.021080\n0.014339\n0.014545\n0.015652\n...\n0.001312\n0.001760\n-0.000022\n0.012203\n-0.004890\nNaN\n0.012134\n0.006734\n0.024929\nNaN\n\n\n2\n1959-05-01\n0.005796\n0.006618\n0.012043\n0.006839\n0.007803\n0.014954\n0.008267\n0.009580\n0.004766\n...\n-0.001695\n-0.001867\n-0.000021\n-0.004090\n-0.004819\nNaN\n0.002828\n0.002020\n-0.015342\nNaN\n\n\n3\n1959-06-01\n0.003068\n0.003012\n0.003642\n-0.000097\n0.009064\n0.001137\n0.007035\n0.007125\n-0.004766\n...\n0.003334\n0.001946\n-0.004619\n0.003992\n0.004796\nNaN\n0.009726\n0.009007\n-0.012252\nNaN\n\n\n4\n1959-07-01\n-0.000580\n-0.000762\n-0.003386\n0.012155\n-0.000330\n-0.024237\n0.001168\n0.008251\n0.013056\n...\n-0.001204\n-0.000013\n0.000000\n-0.004040\n-0.004796\nNaN\n-0.004631\n-0.001000\n0.029341\nNaN\n\n\n\n\n5 rows × 127 columns\n\n\n\n\n1import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\n2series_to_plot = ['INDPRO', 'CPIAUCSL', 'TB3MS']\nseries_names = ['Industrial Production',\n                'Inflation (CPI)',\n                '3-month Treasury Bill rate']\n\n\n# Create a figure and a grid of subplots\n3fig, axs = plt.subplots(len(series_to_plot), 1, figsize=(8, 15))\n\n# Iterate over the selected series and plot each one\nfor ax, series_name, plot_title in zip(axs, series_to_plot, series_names):\n4    if series_name in df_cleaned.columns:\n5        dates = pd.to_datetime(df_cleaned['sasdate'], format='%m/%d/%Y')\n6        ax.plot(dates, df_cleaned[series_name], label=plot_title)\n7        ax.xaxis.set_major_locator(mdates.YearLocator(base=5))\n        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n8        ax.set_title(plot_title)\n9        ax.set_xlabel('Year')\n        ax.set_ylabel('Transformed Value')\n10        ax.legend(loc='upper left')\n11        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n    else:\n        ax.set_visible(False)  # Hide plots for which the data is not available\n\n12plt.tight_layout()\n13plt.show()\n\n\n1\n\nWe use library matplotlib to plot\n\n2\n\nWe consider three series (INDPRO, CPIAUCSL, TB3MS) and assign them human-readable names (“Industrial Production”, “Inflation (CPI)”, “3-month Treasury Bill rate.”).\n\n3\n\nWe create a figure with three (len(series_to_plot)) subplots arranged vertically. The figure size is 8x15 inches.\n\n4\n\nWe check if the series exists in each series df_cleaned DataFrame columns.\n\n5\n\nWe convert the sasdate column to datetime format (not necessary, since sasdate was converter earlier)\n\n6\n\nWe plot each series against the sasdate on the corresponding subplot, labeling the plot with its human-readable name.\n\n7\n\nWe format the x-axis to display ticks and label the x-axis with dates taken every five years.\n\n8\n\nEach subplot is titled with the name of the economic indicator.\n\n9\n\nWe label the x-axis “Year,” and the y-axis “Transformed Value,” to indicate that the data was transformed before plotting.\n\n10\n\nA legend is added to the upper left of each subplot for clarity.\n\n11\n\nWe rotate the x-axis labels by 45 degrees to prevent overlap and improve legibility.\n\n12\n\nplt.tight_layout() automatically adjusts subplot parameters to give specified padding and avoid overlap.\n\n13\n\nplt.show() displays the figure with its subplots."
  },
  {
    "objectID": "comptools_ass1.html#forecasting-in-time-series",
    "href": "comptools_ass1.html#forecasting-in-time-series",
    "title": "Computational Tools for Macroeconometrics",
    "section": "Forecasting in Time Series",
    "text": "Forecasting in Time Series\nForecasting in time series analysis involves using historical data to predict future values. The objective is to model the conditional expectation of a time series based on past observations.\n\nDirect Forecasts\nDirect forecasting involves modeling the target variable directly at the desired forecast horizon. Unlike iterative approaches, which forecast one step ahead and then use those forecasts as inputs for subsequent steps, direct forecasting directly models the relationship between past observations and future value.\n\n\nARX Models\nAutoregressive Moving with predictors (ARX) models are a class of univariate time series models that extend ARMA models by incorporating exogenous (independent) variables. These models are formulated as follows:\n\\[\n\\begin{aligned}Y_{t+h} & =\\alpha+\\phi_{0}Y_{t}+\\phi_{1}Y_{t-1}+\\dots+\\phi_{p}Y_{t-p}\\\\\n& \\qquad+\\theta_{0,1}X_{t,1}+\\theta_{1,1}X_{t-1,1}+\\dots+\\theta_{p,1}X_{t-p,1}+\\dots\\\\\n& \\qquad+\\theta_{0,k}X_{t,k}+\\dots+\\theta_{p,k}X_{t-p,k}+u_{t+h}\\\\\n& =\\alpha+\\sum_{i=0}^{p}\\phi_{i}Y_{t-i}+\\sum_{j=1}^{k}\\sum_{s=0}^{p}\\theta_{s,j}X_{t-s,j}+\\epsilon_{t+h}\n\\end{aligned}\n\\tag{1}\\]\n\n\\(Y_{t+h}\\): The target variable at time \\(t+h\\).\n\\(X_{t,j}\\): Predictors (variable \\(j=1,\\ldots,k\\) at time \\(t\\)).\n\\(p\\) number of lags of the target and the predictors.1\n\\(\\phi_i\\), \\(i=0,\\dots,p\\), and \\(\\theta_{j,s}\\), \\(j=1,\\dots,k\\), \\(s=1,\\ldots,r\\): Parameters of the model.\n\\(\\epsilon_{t+h}\\): error term.\n\nFor instance, to predict Industrial Prediction using as predictor inflation and the 3-month t-bill, the target variable is INDPRO, and the predictors are CPIAUSL and TB3MS. Notice that the target and the predictors are the transformed variables. Thus, if we use INDPRO as the target, we are predicting the log-difference of industrial production, which is a good approximation for its month-to-month percentage change.\nBy convention, the data ranges from \\(t=1,\\ldots,T\\), where \\(T\\) is the last period, we have data (for the df_cleaned dataset, \\(T\\) corresponds to January 2024).\n\n\nForecasting with ARX\nSuppose that we know the parameters of the model for the moment. To obtain a forecast for \\(Y_{T+h}\\), the \\(h\\)-step ahead forecast, we calculate \\[\n\\begin{aligned}\n\\hat{Y}_{T+h} &=  \\alpha + \\phi_0 Y_T + \\phi_1 Y_{T-1} + \\dots + \\phi_p Y_{T-p} \\\\\n                  &\\,\\,\\quad \\quad + \\theta_{0,1} X_{T,1} + \\theta_{1,1} X_{T-1,1} + \\dots + \\theta_{p,1} X_{T-p,1} \\\\\n                  &\\,\\,\\quad \\quad + \\dots + \\theta_{0,k} X_{T,k} + \\dots + \\theta_{p,k} X_{T-p,k}\\\\\n        &=  \\alpha + \\sum_{i=0}^p \\phi_i Y_{T-i} + \\sum_{j=1}^k\\sum_{s=0}^p \\theta_{s,j} X_{T-s,j}\n\\end{aligned}\n\\]\nWhile this is conceptually easy, implementing the steps needed to calculate the forecast is insidious, and care must be taken to ensure we are calculating the correct forecast.\nTo start, it is convenient to rewrite the model in Equation 1 as a linear model \\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{u},\n\\] where \\(\\boldsymbol{\\beta}\\) is the vector (of size \\(1+(1+p)(1+k)\\)) \\[\n\\boldsymbol{\\beta}=\\begin{pmatrix}\\alpha\\\\\n\\phi_{0}\\\\\n\\vdots\\\\\n\\phi_{p}\\\\\n\\theta_{0,1}\\\\\n\\vdots\\\\\n\\theta_{p,1}\\\\\n\\vdots\\\\\n\\theta_{1,k}\\\\\n\\vdots\\\\\n\\theta_{p,k}\n\\end{pmatrix},\n\\] \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) are respectively given by \\[\n\\mathbf{y} = \\begin{pmatrix}\ny_{p+h+1}  \\\\\ny_{p+h+2}\\\\\n\\vdots \\\\\ny_{T}\n\\end{pmatrix}\n\\] and \\[\n\\mathbf{X} = \\begin{pmatrix}1 & Y_{p+1} & Y_{p} & \\cdots & Y_{1} & X_{p+1,1} & X_{p,1} & \\cdots & X_{1,1} & X_{p+1,k} & X_{p,k} & \\cdots & X_{1,k}\\\\\n\\vdots & \\vdots & \\vdots &  & \\vdots & \\vdots & \\vdots &  & \\vdots & \\vdots & \\vdots &  & \\vdots\\\\\n1 & Y_{T-h-1} & Y_{T-h-2} & \\cdots & Y_{T-h-p-1} & X_{T-h-1,1} & X_{T-h-2,1} & \\cdots & X_{T-h-p-1,1} & X_{T-h-1,k} & X_{T-h-2,k} & \\cdots & X_{T-h-p-1,k}\\\\\n1 & Y_{T-h} & Y_{T-h-1} & \\cdots & Y_{T-h-p} & X_{T-h,1} & X_{T-h-1,1} & \\cdots & X_{T-h-p,1} & X_{T-h,k} & X_{T-h-1,k} &  & X_{T-h-p,k}\n\\end{pmatrix}.\n\\] The size of \\(\\mathbf{X}\\) is \\((T-p-h)\\times 1+(1+k)(1+p)\\) and that of \\(\\mathbf{y}\\) is \\(T-h-p\\).\nThe matrix \\(\\mathbf{X}\\) can be obtained in the following way:\n\nYraw = df_cleaned['INDPRO']\nXraw = df_cleaned[['CPIAUCSL', 'TB3MS']]\n\nnum_lags  = 4  ## this is p\nnum_leads = 1  ## this is h\nX = pd.DataFrame()\n## Add the lagged values of Y\ncol = 'INDPRO'\nfor lag in range(0,num_lags+1):\n        # Shift each column in the DataFrame and name it with a lag suffix\n        X[f'{col}_lag{lag}'] = Yraw.shift(lag)\n\nfor col in Xraw.columns:\n    for lag in range(0,num_lags+1):\n        # Shift each column in the DataFrame and name it with a lag suffix\n        X[f'{col}_lag{lag}'] = Xraw[col].shift(lag)\n## Add a column on ones (for the intercept)\nX.insert(0, 'Ones', np.ones(len(X)))\n\n\n## X is now a DataFrame\nX.head()\n\n\n\n\n\n\n\n\nOnes\nINDPRO_lag0\nINDPRO_lag1\nINDPRO_lag2\nINDPRO_lag3\nINDPRO_lag4\nCPIAUCSL_lag0\nCPIAUCSL_lag1\nCPIAUCSL_lag2\nCPIAUCSL_lag3\nCPIAUCSL_lag4\nTB3MS_lag0\nTB3MS_lag1\nTB3MS_lag2\nTB3MS_lag3\nTB3MS_lag4\n\n\n\n\n0\n1.0\n0.014300\nNaN\nNaN\nNaN\nNaN\n-0.000690\nNaN\nNaN\nNaN\nNaN\n0.10\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1.0\n0.021080\n0.014300\nNaN\nNaN\nNaN\n0.001380\n-0.000690\nNaN\nNaN\nNaN\n0.15\n0.10\nNaN\nNaN\nNaN\n\n\n2\n1.0\n0.014954\n0.021080\n0.014300\nNaN\nNaN\n0.001723\n0.001380\n-0.000690\nNaN\nNaN\n-0.11\n0.15\n0.10\nNaN\nNaN\n\n\n3\n1.0\n0.001137\n0.014954\n0.021080\n0.01430\nNaN\n0.000339\n0.001723\n0.001380\n-0.00069\nNaN\n0.37\n-0.11\n0.15\n0.10\nNaN\n\n\n4\n1.0\n-0.024237\n0.001137\n0.014954\n0.02108\n0.0143\n-0.001034\n0.000339\n0.001723\n0.00138\n-0.00069\n-0.01\n0.37\n-0.11\n0.15\n0.1\n\n\n\n\n\n\n\nNote that the first \\(p=\\)4 rows of X have missing values.\nThe vector \\(\\mathbf{y}\\) can be similarly created as\n\ny = Yraw.shift(-num_leads)\ny\n\n0      0.021080\n1      0.014954\n2      0.001137\n3     -0.024237\n4     -0.034464\n         ...   \n786   -0.004547\n787   -0.001453\n788    0.009853\n789    0.005111\n790         NaN\nName: INDPRO, Length: 791, dtype: float64\n\n\nThe variable y has missing values in the last h positions (it is not possible to lead the target beyond \\(T\\)).\nNotice also that we must keep the last row of X for constructing the forecast.\nNow we create two numpy arrays with the missing values stripped:\n\n## Save last row of X (converted to numpy)\nX_T = X.iloc[-1:].values\n## Subset getting only rows of X and y from p+1 to h-1\n## and convert to numpy array\ny = y.iloc[num_lags:-num_leads].values\nX = X.iloc[num_lags:-num_leads].values\n\n\nX_T\n\narray([[ 1.00000000e+00,  5.11104809e-03,  9.85288291e-03,\n        -1.45324151e-03, -4.54694564e-03, -4.20511799e-03,\n         1.01839974e-03,  8.39506878e-04,  5.38574156e-04,\n        -2.74117085e-05,  4.89182561e-04, -6.00000000e-02,\n        -1.50000000e-01, -9.00000000e-02, -2.10000000e-01,\n        -3.30000000e-01]])\n\n\nNow, we have to estimate the parameters and obtain the forecast.\n\n\nEstimation\nThe parameters of the model can be estimated by OLS (the OLS estimates the coefficient of the linear projection of \\(Y_{t+h}\\) on its lags and the lags of \\(X_t\\)).\nThe OLS estimator of \\(\\boldsymbol{\\beta}\\) is \\[\n\\hat{\\boldsymbol{\\beta}} = (X'X)^{-1}X'Y.\n\\]\nWhile this is the formula used to describe the OLS estimator, from a computational poijnt of view is much better to define the estimator as the solution of the set of linear equations: \\[\n(X'X)\\boldsymbol{\\beta} = X'Y\n\\]\nThe function solve can be used to solve this linear system of equation.\n\nfrom numpy.linalg import solve\n# Solving for the OLS estimator beta: (X'X)^{-1} X'Y\nbeta_ols = solve(X.T @ X, X.T @ y)\n\n## Produce the One step ahead forecast\n## % change month-to-month INDPRO\nforecast = X_T@beta_ols*100\nforecast\n\narray([0.24917247])\n\n\nThe variable forecast contains now the one-step ahead (\\(h=1\\) forecast) of INDPRO. Since INDPRO has been transformed in logarithmic differences, we are forecasting the percentage change (and multiplying by 100 gives the forecast in percentage points).\nTo obtain the \\(h\\)-step ahead forecast, we must repeat all the above steps using a different h.\n\n\nForecasting Exercise\nHow good is the forecast that the model is producing? One thing we could do to assess the forecast’s quality is to wait for the new data on industrial production and see how big the forecasting error is. However, this evaluation would not be appropriate because we need to evaluate the forecast as if it were repeatedly used to forecast future values of the target variables. To properly assess the model and its ability to forecast INDPRO, we must keep producing forecasts and calculating the errors as new data arrive. This procedure would take time as we must wait for many months to have a series of errors that is large enough.\nA different approach is to do what is called a Real-time evaluation. A Real-time evaluation procedure consists of putting ourselves in the shoes of a forecaster who has been using the forecasting model for a long time.\nIn practice, that is what are the steps to follow to do a Real-time evaluation of the model:\n\nSet \\(T\\) such that the last observation of df coincides with December 1999;\nEstimate the model using the data up to \\(T\\)\nProduce \\(\\hat{Y}_{T+1}, \\hat{Y}_{T+2}, \\dots, \\hat{Y}_{T+H}\\)\nSince we have the actual data for January, February, …, we can calculate the forecasting errors of our model \\[\n\\hat{e}_{T+h} = \\hat{Y}_{T+h} - Y_{T+h}, \\,\\, h = 1,\\ldots, H.\n\\]\nSet \\(T = T+1\\) and do all the steps above.\n\nThe process results are a series of forecasting errors we can evaluate using several metrics. The most commonly used is the MSFE, which is defined as \\[\nMSFE_h = \\frac{1}{J}\\sum_{j=1}^J  \\hat{e}_{T+j+h}^2,\n\\] where \\(J\\) is the number of errors we collected through our real-time evaluation.\nThis assignment asks you to perform a real-time evaluation assessment of our simple forecasting model and calculate the MSFE for steps \\(h=1,4,8\\).\nAs a bonus, we can evaluate different models and see how they perform differently. For instance, you might consider different numbers of lags and/or different variables in the model.\n\nHint\nA sensible way to structure the code for real-time evaluation is to use several functions. For instance, you can define a function that calculates the forecast given the DataFrame.\n\ndef calculate_forecast(df_cleaned, p = 4, H = [1,4,8], end_date = '12/1/1999',target = 'INDPRO', xvars = ['CPIAUCSL', 'TB3MS']):\n\n    ## Subset df_cleaned to use only data up to end_date\n    rt_df = df_cleaned[df_cleaned['sasdate'] &lt;= pd.Timestamp(end_date)]\n    ## Get the actual values of target at different steps ahead\n    Y_actual = []\n    for h in H:\n        os = pd.Timestamp(end_date) + pd.DateOffset(months=h)\n        Y_actual.append(df_cleaned[df_cleaned['sasdate'] == os][target]*100)\n        ## Now Y contains the true values at T+H (multiplying * 100)\n\n    Yraw = rt_df[target]\n    Xraw = rt_df[xvars]\n\n    X = pd.DataFrame()\n    ## Add the lagged values of Y\n    for lag in range(0,p):\n        # Shift each column in the DataFrame and name it with a lag suffix\n        X[f'{target}_lag{lag}'] = Yraw.shift(lag)\n\n    for col in Xraw.columns:\n        for lag in range(0,p):\n            X[f'{col}_lag{lag}'] = Xraw[col].shift(lag)\n    \n    ## Add a column on ones (for the intercept)\n    X.insert(0, 'Ones', np.ones(len(X)))\n    \n    ## Save last row of X (converted to numpy)\n    X_T = X.iloc[-1:].values\n\n    ## While the X will be the same, Y needs to be leaded differently\n    Yhat = []\n    for h in H:\n        y_h = Yraw.shift(-h)\n        ## Subset getting only rows of X and y from p+1 to h-1\n        y = y_h.iloc[p:-h].values\n        X_ = X.iloc[p:-h].values\n        # Solving for the OLS estimator beta: (X'X)^{-1} X'Y\n        beta_ols = solve(X_.T @ X_, X_.T @ y)\n        ## Produce the One step ahead forecast\n        ## % change month-to-month INDPRO\n        Yhat.append(X_T@beta_ols*100)\n\n    ## Now calculate the forecasting error and return\n\n    return np.array(Y_actual) - np.array(Yhat)\n\nWith this function, you can calculate real-time errors by looping over the end_date to ensure you end the loop at the right time.\n\nt0 = pd.Timestamp('12/1/1999')\ne = []\nT = []\nfor j in range(0, 10):\n    t0 = t0 + pd.DateOffset(months=1)\n    print(f'Using data up to {t0}')\n    ehat = calculate_forecast(df_cleaned, p = 4, H = [1,4,8], end_date = t0)\n    e.append(ehat.flatten())\n    T.append(t0)\n\n## Create a pandas DataFrame from the list\nedf = pd.DataFrame(e)\n## Calculate the RMSFE, that is, the square root of the MSFE\nnp.sqrt(edf.apply(np.square).mean())\n\nUsing data up to 2000-01-01 00:00:00\nUsing data up to 2000-02-01 00:00:00\nUsing data up to 2000-03-01 00:00:00\nUsing data up to 2000-04-01 00:00:00\nUsing data up to 2000-05-01 00:00:00\nUsing data up to 2000-06-01 00:00:00\nUsing data up to 2000-07-01 00:00:00\nUsing data up to 2000-08-01 00:00:00\nUsing data up to 2000-09-01 00:00:00\nUsing data up to 2000-10-01 00:00:00\n\n\n0    0.345387\n1    0.516334\n2    0.627048\ndtype: float64\n\n\nYou may change the function calculate_forecast to output also the actual data end the forecast, so you can, for instance, construct a plot."
  },
  {
    "objectID": "comptools_ass1.html#working-with-github",
    "href": "comptools_ass1.html#working-with-github",
    "title": "Computational Tools for Macroeconometrics",
    "section": "Working with github",
    "text": "Working with github\nThe https://github.com/gragusa/assignment1 repository contains four files:\n\ncomptools_ass1.qmd\nassignment1_julia.jl\nassignment1_python.py\nassignment1_r.r\n\nThe comptools_ass1.qmd is this file (in quarto format). The repository also contains the pdf and the html version of this file.\nThe other files, assignment1_julia.jl, assignment1_julia.py, and assignment1_julia.py, are the starter kit of the code you have to write in Julia, R, and Python. You can use them to start your work.\n\nUsing Visual Studio Code\nUnless you are familiar with the command line and you are using Linux or MacOS, the best way to interact with github is through Visual Studio Code. Instructions on how to install Visual Studio Code on Windows are here. For MacOS the instructions are here.\nVisual Studio Code has an extension system. The extensions extend VSCode adding features that simplify writing and interacting with code.\nThe extensions you should install are\n\nJulia Extension Instructions\nPython Extension Instruction\nR Extension Instructions\n\nThere are many other extensions that you might find useful. For those, google is your friend.\n\n\nCloning the repository\nCloning a repository from GitHub into Visual Studio Code (VSCode) allows you to work on projects directly from your local machine. Here’s a detailed step-by-step guide on how to clone the repository https://github.com/uniroma/comptools-assignments into VSCode:\n\nOpen Visual Studio Code\n\n\nStart by opening Visual Studio Code on your computer.\n\n\nAccess the Command Palette\n\n\nWith VSCode open, access the Command Palette by pressing Ctrl+Shift+P on Windows/Linux or Cmd+Shift+P on macOS. This is where you can quickly access various commands in VSCode.\n\n\nClone Repository\n\n\nIn the Command Palette, type “Git: Clone” and select the option Git: Clone from the list that appears. This action will prompt VSCode to clone a repository.\n\n\nEnter the Repository URL\n\n\nA text box asking for the repository URL will appear at the top of the VSCode window. Enter https://github.com/uniroma/comptools-assignments and press Enter. (This is the URL of the assignment 1 repository).\n\n\nChoose a Directory\n\n\nNext, VSCode will ask you to select a directory where you want to clone the repository. Navigate through your file system and choose a directory that will be the local storage place for the repository. The directory should exist. Create it if it doesn’t. Once selected, the cloning process will start.\n\n\nOpen the Cloned Repository\n\n\nAfter the repository has been successfully cloned, a notification will pop up in the bottom right corner of VSCode with the option to Open Repository. Click on it. If you missed the notification, you can navigate to the directory where you cloned the repository and open it manually from within VSCode by going to File &gt; Open Folder.\n\n\nStart Working\n\n\nNow that the repository is cloned and opened in VSCode, you can start working on the project. You can edit files, commit changes, and manage branches directly from VSCode.\n\n\n\n\n\n\n\nTip\n\n\n\n\nEnsure you have Git installed on your computer to use the Git features in VSCode. If you do not have Git installed, you can download it from the official Git website. Instructions to install Git.\nIf you are working with GitHub repositories frequently, consider authenticating with GitHub in VSCode to streamline your workflow. This can be done through the Command Palette by finding the GitHub: Sign in command.\n\n\n\n\n\nMake changes and commit them to the repository\n\nMake Your Changes\n\nOpen the repository you have cloned in VSCode.\nNavigate to the file(s) you wish to change within the VSCode Explorer pane.\nMake the necessary modifications or additions to the file(s). These changes can be anything from fixing a bug to adding new features.\n\nReview Your Changes\n\n\nAfter making changes, you can see which files have been modified by looking at the Source Control panel. You can access this panel by clicking on the Source Control icon (it looks like a branch or a fork) on the sidebar or by pressing Ctrl+Shift+G (Windows/Linux) or Cmd+Shift+G (macOS) and searching for Show control panel.\nModified files are listed within the Source Control panel. Click on a file to view the changes (differences) between your working version and the last commit. Lines added are highlighted in green, and lines removed are highlighted in red.\n\n\nStage Your Changes\n\n\nBefore committing, you need to stage your changes. Staging is like preparing and reviewing exactly what changes you will commit to without making the commit final.\nYou can stage changes by right-clicking on a modified file in the Source Control panel and selecting Stage Changes. Alternatively, you can stage all changes at once by clicking the + icon next to the “Changes” header.\n\n\nCommit Your Changes\n\n\nAfter staging your changes, commit them to the repository. To do this, type a commit message in the message box at the top of the Source Control panel. This message should briefly describe the changes you’ve made.\nPress Ctrl+Enter (Windows/Linux) or Cmd+Enter (macOS) to commit the staged changes (search for Git: Commit). Alternatively, you can click the checkmark icon (Commit) at the top of the Source Control panel.\nBefore committing, you should enter a commit message that briefly describes the changes that you have made. Commit messages are essential for making the project’s history understandable for yourself and the other collaborators.\n\n\nPush Your Changes\n\n\nIf you’re working with a remote repository (like one hosted on GitHub), you must push your commits to update the remote repository with your local changes.\nYou can push changes by clicking on the three dots (...) menu in the Source Control panel, navigating to Push and selecting it. If you’re using Git in VSCode for the first time, you might be prompted to enter your GitHub credentials or authenticate in another way.\n\n\n\n\n\n\n\nTip\n\n\n\nIt’s a good practice to pull changes from the remote repository before starting your work session (to ensure you’re working with the latest version) and before pushing your changes (to ensure no conflicts). You can pull changes by clicking on the three dots (...) menu in the Source Control panel and selecting Pull.\n\n\nThe following video explores in more detail how to use git in VSCode."
  },
  {
    "objectID": "comptools_ass1.html#using-codespace",
    "href": "comptools_ass1.html#using-codespace",
    "title": "Computational Tools for Macroeconometrics",
    "section": "Using Codespace",
    "text": "Using Codespace\nIf you cloned the repository and your GitHub username is student1, then visiting\nhttps://github.com/codespaces/badge.svg)](https://codespaces.new/student1/assignment1?quickstart=1&devcontainer_path=.devcontainer%2Fdevcontainer.json\nwill open a CodeSpace version of Visual Studio code on your repository. The codespace has R, Julia and Python installed and Visual Studio Code is setup with the relevant extensions."
  },
  {
    "objectID": "comptools_ass1.html#footnotes",
    "href": "comptools_ass1.html#footnotes",
    "title": "Computational Tools for Macroeconometrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTheoretically, the number of lags for the target variables and the predictors could be different. Here, we consider the simpler case in which both are equal.↩︎"
  },
  {
    "objectID": "comptools_ass2.html",
    "href": "comptools_ass2.html",
    "title": "Optimization",
    "section": "",
    "text": "Optimization is a field of mathematics that focuses on the problem of finding the solution to a minimization problem. More precisely, given a function \\(f:\\mathbb{R}^{k}\\to \\mathbb{R}\\), we seek \\(x^{*}\\in\\mathcal{C}\\subset \\mathbb{R}^{k}\\) such that \\[\nf(x^{*}) \\leqslant f(x) \\text{ for all }x\\in\\mathcal{C}.\n\\] The set \\(\\mathcal{C}\\) constraints the solution to leave in a subset of \\(\\mathbb{R}^{k}\\)\nWhen \\(\\mathcal{C}\\) coincides with \\(\\mathbb{R}^k\\) the problem is said to be unconstrained.\nIn unconstrained optimization, the objective function \\(f(x)\\) needs to be minimized (or maximized) without any restrictions on the variable \\(x\\). The problem is described as \\[\n\\min_{x} f(x), \\quad x\\in\\mathbb{R}^k.\n\\]\nMany problems involve constraints that the solution must satisfy, that is, \\(\\mathcal{C}\\) does not coincide with \\(\\mathbb{R}^k\\). For instance, we want to minimize the function over a space where \\(x_j &lt; c_j\\), \\(c_j\\in\\mathbb{R}\\), \\(j=1,\\ldots,k\\) or we may be interested in values of \\(x\\) that minimizes \\(f\\) when certain restrictions are satisfied. Generally, the constrained optimization is \\[\n   \\begin{aligned}\n   &\\min_{x} \\ f(x) \\\\\n   &\\text{subject to} \\\\\n   & \\ g_i(x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n                     & \\ h_j(x) = 0, \\quad j = 1, \\ldots, p.\n   \\end{aligned}\n\\] Here, \\(g_i(x)\\) and \\(h_j(x)\\) are functions that represent inequality and equality constraints.\nIn what follows, we will focus on the class of unconstrained problems where \\(f\\) is smooth, that is, a function that everywhere continuously differentiable.\n\n\n\\[\n\\min_{x} f(x), \\quad x\\in\\mathbb{R}^k.\n\\tag{1}\\]\nA point \\(x^*\\) is a global minimizer of \\(f\\) if \\(f(x^*)\\leqslant f(x)\\) for all \\(x\\) ranging over of of \\(\\mathbb{R}^k\\).\nThe global minimizer can be difficult to find. The algorithms to solve Equation 1 exploit local knowledge of \\(f\\), we do not have a clear picture of the overall shape of \\(f\\) and, as such, we cannot ever be sure that the solution we find is indeed a global solution to the minimization problem. Most algorithms are able to find only a local minimizer, which is a point that achieves the samllest value of \\(f\\) in a neighborhood of \\(x\\).\nA point \\(x^*\\) is a local minimizer if there is a neighborhood1 \\(\\mathcal{N}\\) of \\(x^*\\) such that \\(f(x^*)\\leqslant f(x)\\) for all \\(x\\in\\mathcal{N}\\).\nA point \\(x^*\\) is a strict local minimizer if there a neighborhood of \\(\\mathcal{N}\\) such that \\(f(x^*) &lt; f(x)\\) for all \\(x\\in \\mathcal{N}\\).\nFor instance, for the function \\(f(x) = 2024\\), every point is a weak local minimizer, while the function \\(f(x) = (x-a)^2\\) has a strict local minimizer at \\(x=a^{-1}\\).\nNotation: Given a function \\(f:\\mathbb{R}^k\\to\\mathbb{R}\\), the gradient of \\(f\\) evaluated at \\(x^o\\) is: \\[\n\\nabla f(x^o) := \\begin{pmatrix}\\left.\\frac{\\partial f(x)}{\\partial x_{1}}\\right|_{x=x^{o}}\\\\\n\\left.\\frac{\\partial f(x)}{\\partial x_{2}}\\right|_{x=x^{o}}\\\\\n\\vdots\\\\\n\\left.\\frac{\\partial f(x)}{\\partial x_{k}}\\right|_{x=x^{o}}\n\\end{pmatrix}.\n\\] Similarly, the \\(k\\times k\\) hessian matrix of \\(f\\) evaluated at \\(x^o\\) is \\[\n\\nabla^{2}f(x^{o})=\\begin{pmatrix}\\left.\\frac{\\partial^{2}f(x)}{\\partial x_{1}\\partial x_{1}}\\right|_{x=x^{o}} & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{1}\\partial x_{2}}\\right|_{x=x^{o}} & \\cdots & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{1}\\partial x_{k}}\\right|_{x=x^{o}}\\\\\n\\left.\\frac{\\partial^{2}f(x)}{\\partial x_{2}\\partial x_{1}}\\right|_{x=x^{o}} & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{2}\\partial x_{2}}\\right|_{x=x^{o}} & \\cdots & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{2}\\partial x_{k}}\\right|_{x=x^{o}}\\\\\n\\vdots & \\vdots &  & \\vdots\\\\\n\\left.\\frac{\\partial^{2}f(x)}{\\partial x_{k}\\partial x_{1}}\\right|_{x=x^{o}} & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{k}\\partial x_{2}}\\right|_{x=x^{o}} & \\cdots & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{k}\\partial x_{k}}\\right|_{x=x^{o}}\n\\end{pmatrix}.\n\\]\n\n\n\nThe necessary conditions for optimality are derived assuming that \\(x^*\\) is a local minimizer and the proving facts about \\(\\nabla f(x^*)\\) and \\(\\nabla^2 f(x^*)\\).\n\nTheorem 1 If \\(x^*\\) is a local optimizer and \\(f\\) is continuously differentiable in an open neighborhood of \\(x^*\\), then \\(\\nabla f(x^*)=0\\) (first-order necessary condition); if \\(\\nabla^2 f\\) exists and its continuous in an open neighborhood of \\(x^*\\), then \\(\\nabla^2 f(x^*)\\) is positive definite (second-order necessary conditions).2\n\nSufficient conditions for optimality are conditions on the derivatives of \\(f\\) at the point \\(x^*\\) that guarantee that \\(x^*\\) is a local minimizer.\n\nTheorem 2 Suppose that \\(\\nabla^2 f(x^*)\\) is continuous in an open neighborhood of \\(x^*\\) and that \\(\\nabla f(x^*)=0\\) and \\(\\nabla^2 f(x^*)\\) is positive definite. Then \\(x^*\\) is a strict local minimizer of \\(f\\).\n\nThe second-order sufficient conditions guarantee something stronger than the necessary conditions; namely, that the minimizer is a strict local minimizer. Note too that the second-order sufficient conditions are not necessary: a point \\(x^∗\\) may be a strict local minimizer, and yet may fail to satisfy the sufficient conditions. A simple example is given by the function \\(f(x) = x^3\\), for which the point \\(x^∗=0\\) is a strict local minimizer at which the Hessian matrix vanishes (and is therefore not positive definite).\nWhen the objective function is convex, local and global minimizers are simple to characterize.\n\nTheorem 3 When \\(f\\) is convex, any local minimizer of \\(x^*\\) is a global minimizer of \\(f\\). If in addition, \\(f\\) is differentiable, any stationary point \\(x^*\\) is a global minimizer.\n\n\nExample 1 The sum of square residuals \\[\nSSR(\\beta) = \\sum_{i=1}^n (Y_i - X'_i\\beta)^2\n\\] is strictly convex provided that \\(\\sum_{i=1}^n X_iX'_i\\) is invertible (that is, it has full column rank).\n\n\n\n\nA common approach to optimization is to incrementally improve a point \\(x\\) by taking a step that minimizes the objective value based on a local model. The local model may be obtained, for example, from a first- or second-order Taylor approximation. Optimization algorithms that follow this general approach are referred to as descent direction methods. They start with a design point \\(x^{(0)}\\) and then generate a sequence of points, sometimes called iterates, to converge to a local minimum.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function and its derivative\ndef f(x):\n    return x**2 - np.log(x)\n\ndef df(x):\n    return 2*x - 1/x\n\ndef connectpoints(x,y,p1,p2):\n    x1, x2 = x[p1], x[p2]\n    y1, y2 = y[p1], y[p2]\n    plt.plot([x1,x2],[y1,y2],'k-')\n\n\n# Initial point\nx0 = 2\nalpha = 0.3\n\n# Gradient descent update\nx1 = x0 - alpha * df(x0)\nx2 = x1 - alpha * df(x1)\n# Points for the function plot\nx = np.linspace(-2.5, 2.5, 400)\ny = f(x)\n\n# Tangent line at x0 (y = m*x + b)\n\n# Creating the plot\n\nplt.figure(figsize=(8, 5))\nplt.plot(x, y, label='f(x) = x^2')\nplt.scatter([x0, x1], [f(x0), f(x1)], color='red')  # Points\nm = df(x0)\nb = f(x0) - m*x0\ntangent_line = m*x + b\nplt.plot(x, tangent_line, 'b--', label=f'Tangent at x0={x0}')\nplt.arrow(x0, 0.4, x1-x0, 0.0, head_width=0.1, length_includes_head=True, color = 'r')\n\nm = df(x1)\nb = f(x1) - m*x1\ntangent_line = m*x + b\nplt.plot(x, tangent_line, 'b--', label=f'Tangent at x0={x1}', )\nplt.ylim([-0.2,6])\nplt.xlim([-0.4,3])\nplt.plot(x, tangent_line, 'b--', label=f'Tangent at x0={x1}', )\nm = df(x0)\nb = f(x0) - m*x0\ntangent_line = m*x + b\n\nplt.scatter(x0, f(x0), color='green')  # Initial point\nplt.scatter(x1, f(x1), color='green')  # Next point after step\nplt.scatter(x2, f(x2), color='green')  # Next point after step\n\nplt.arrow(x1, 0., x2-x1, 0., head_width=0.1, length_includes_head=True, color = 'r')\n\nplt.title('Gradient Descent on f(x)')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.xticks([])  # Remove x-axis ticks\nplt.xticks([x0, x1, x2], [r\"$x^{(0)}$\", r\"$x^{(1)}$\", r\"$x^{(2)}$\"])\n#plt.legend()\nplt.grid(True)\nplt.plot([x0, x0],[f(x0), 0],'g--')\nplt.plot([x1, x1],[f(x1), 0],'g--')\nplt.plot([x2, x2],[f(x1), 0],'g--')\nplt.show()\n\n/var/folders/72/d3_1t_ps0z1fw0_l58cwv_s80000gn/T/ipykernel_35042/189701708.py:6: RuntimeWarning: invalid value encountered in log\n  return x**2 - np.log(x)\n\n\n\n\n\n\n\n\nFigure 1: Gradient descent\n\n\n\n\n\nThe intuition behind this approach is relatively simple. Consider the function plotted in Figure 1. We start the algorithm at \\(x^{(0)}\\). The derivative at this point (the tangent line in blue) is positive, that is, \\(f'(x^{(0)})&gt;0\\). Thus, to descend toward smaller values of the function have to set a point \\(x^{(1)}\\) smaller. One possibility is to use the following iteration \\[\nx^{(1)} = x^{(0)} - \\alpha \\nabla f(x^{(0)}),\n\\] where \\(\\alpha\\in(0,1)\\) is the step factor. As it can be seen from Figure 1, at this point the value of \\(f\\) is now lower. Applying a new iterate we obtain \\(x^{(2)} = x^{(1)} - \\alpha f'(x^{(1)})\\) which is now very close to the minimum value of the function.\nWe will keep iterating until the termination condition is satisfied. The termination condition will be satisfied when the current iterate is likely to be a local minimum.[^termination]\n3: The most common termination conditions are 1. Maximum iterations. We may want to terminate when the number of iterations \\(k\\) exceeds some threshold \\(k_max\\). Alternatively, we might want to terminate once a maximum amount of elapsed time is exceeded. 2. Absolute improvement. This termination condition looks at the change in the function value over subsequent steps. If the change is smaller than a given threshold, it will terminate: \\[\nf(x^{(k)}) - f(x^{(r+1)}) &lt; \\epsilon_a\n\\] 1. Relative improvement. This termination condition looks at the change in function value but uses the relative change of the function. The iterations will terminate if \\[\nf(x^{(k)}) - f(x^{(r+1)}) &lt; \\epsilon_r|f(x^{(k)})|\n\\] 1. Gradient magnitude. We can terminate if the derivative is smaller than a certain tolerance \\[\n\\Vert \\delta f(x^{(k)}) \\Vert &lt; \\epsilon_g\n\\]\nThe same logic can be applied to the case in which \\(f:\\mathbb{R}^k \\to \\mathbb{R}\\) the only difference is that now \\(\\nabla f(x^{(0)})\\) is a \\(k\\times 1\\) vector instead of being a scalar.\nThe gradient descent idea can be generalized by considering the following iterate \\[\nx^{(r)} = x^{(r-1)} - \\alpha d^{(r)},\n\\] where \\(d^{(r)}\\) is a descent direction. The idea of this generalization is that instead of using the gradient as direction, we can use different directions that might speed up the algorithm.\nWhen \\(d^{(r)} = \\nabla f(x^{(r)})\\), the algorithm is called the gradient descent (and direction is called the direction of deepest descent).\nDirections that can be used belong to two classes: 1. first-order: the direction only uses information about the gradient of \\(f\\). The many variations of the gradient descent (Ada, Momentum, etc.) and the conjugate gradient method belong to this class. 2. second order: the direction incorporate information about the second derivatives of \\(f\\). The key idea here is that \\[\nf(x^{(r+1)}) = f(x^{(r)}) + \\nabla f'(x^{(r)})(x^{(r+1)}-x^{(r)}) + (x^{(r+1)}-x^{(r)}) \\dot{H}_k (x^{(r+1)}-x^{(r)}),\n\\] where \\(\\dot{H}_k = \\nabla^2 f(\\dot{x}^{(r)})\\) is the Hessian evaluated at some point between \\(x^{(r+1)}\\) and \\(x^{(r)}\\). Then, approximating the gradient of \\(f\\) and setting it equal to zero yields \\[\nx^{(r+1)} = x^{(r)}-[\\nabla^2 f(x^{(r)})]^{-1} \\nabla f(x^{(r)}) +\n\\] which suggests using the inverse of the hessian to form the direction. Since evaluating the Hessian at each iterate is too costly computationally, different algorithms approximate the Hessian in different ways.\n\n\n\nThe following code implements a gradient descent with steepest direction in Python.\n\nfrom numpy import linalg as la\n\ndef steepest_descent(f, gradient, initial_guess, learning_rate, num_iterations = 100, epsilon_g = 1e-07):\n    x = initial_guess\n    for i in range(num_iterations):\n        grad = gradient(x)\n        x = x - learning_rate * grad\n        normg = la.norm(grad)\n        print(f\"Iteration {i+1}: x = {x}, f(x) = {f(x)}, ||g(x)||={normg}\")\n        ## Termination condition\n        if  normg &lt; epsilon_g:\n            break\n    return x\n\nThis is the Julia version.\n#| label: julia-descent\nusing LinearAlgebra ## needed for norm\nfunction steepest_descent(f, gradient, initial_guess, learning_rate; num_iterations = 100; epsilon_g = 1e-7)\n    x = initial_guess\n    for i in 1:num_iterations\n        grad = gradient(x)\n        x = x - learning_rate * grad\n        normg = norm(g)\n        println(\"Iteration $i: x = $x, f(x) = $(objective_function(x)), ||g(x)||=$(normg)\")\n        if normg &lt; epsilon_g\n            break\n        end\n    end\n    return x\nend\nSuppose we want to solve the following problem \\[\n\\min_{x} f(x)\n\\] where, for some \\(d&gt;1\\), \\[\nf(x) = \\sum_{i=1}^d \\left((x_i-3)^2 \\right)\n\\] The gradient of this function is \\[\n\\nabla f(x)=\\begin{pmatrix}2(x_{1}-3)\\\\\n2(x_{2}-3)\\\\\n\\vdots\\\\\n2(x_{d}-3)\n\\end{pmatrix}.\n\\]\nThe minimum of this function is \\(x_i = 3\\).\n\ndef f(x):\n    return np.sum((x-3.0)**2)\n\ndef gradient(x):\n    return 2*(x-3.0)\n\nsteepest_descent(f, gradient, np.array([0., 0.]), 0.2)\n\nIteration 1: x = [1.2 1.2], f(x) = 6.479999999999999, ||g(x)||=8.48528137423857\nIteration 2: x = [1.92 1.92], f(x) = 2.3327999999999993, ||g(x)||=5.091168824543142\nIteration 3: x = [2.352 2.352], f(x) = 0.8398079999999992, ||g(x)||=3.0547012947258847\nIteration 4: x = [2.6112 2.6112], f(x) = 0.3023308799999997, ||g(x)||=1.8328207768355302\nIteration 5: x = [2.76672 2.76672], f(x) = 0.10883911679999973, ||g(x)||=1.099692466101318\nIteration 6: x = [2.860032 2.860032], f(x) = 0.039182082047999806, ||g(x)||=0.6598154796607905\nIteration 7: x = [2.9160192 2.9160192], f(x) = 0.014105549537279988, ||g(x)||=0.39588928779647375\nIteration 8: x = [2.94961152 2.94961152], f(x) = 0.005077997833420814, ||g(x)||=0.23753357267788475\nIteration 9: x = [2.96976691 2.96976691], f(x) = 0.0018280792200315037, ||g(x)||=0.1425201436067311\nIteration 10: x = [2.98186015 2.98186015], f(x) = 0.0006581085192113414, ||g(x)||=0.08551208616403891\nIteration 11: x = [2.98911609 2.98911609], f(x) = 0.00023691906691608675, ||g(x)||=0.051307251698423345\nIteration 12: x = [2.99346965 2.99346965], f(x) = 8.529086408979587e-05, ||g(x)||=0.03078435101905426\nIteration 13: x = [2.99608179 2.99608179], f(x) = 3.070471107232651e-05, ||g(x)||=0.01847061061143306\nIteration 14: x = [2.99764908 2.99764908], f(x) = 1.1053695986035874e-05, ||g(x)||=0.011082366366859834\nIteration 15: x = [2.99858945 2.99858945], f(x) = 3.9793305549719125e-06, ||g(x)||=0.0066494198201153985\nIteration 16: x = [2.99915367 2.99915367], f(x) = 1.4325589997895879e-06, ||g(x)||=0.003989651892068737\nIteration 17: x = [2.9994922 2.9994922], f(x) = 5.15721239924432e-07, ||g(x)||=0.002393791135240991\nIteration 18: x = [2.99969532 2.99969532], f(x) = 1.8565964637257903e-07, ||g(x)||=0.0014362746811448456\nIteration 19: x = [2.99981719 2.99981719], f(x) = 6.683747269425835e-08, ||g(x)||=0.000861764808686405\nIteration 20: x = [2.99989032 2.99989032], f(x) = 2.4061490169894037e-08, ||g(x)||=0.0005170588852123454\nIteration 21: x = [2.99993419 2.99993419], f(x) = 8.662136461115092e-09, ||g(x)||=0.00031023533112715604\nIteration 22: x = [2.99996051 2.99996051], f(x) = 3.118369125973376e-09, ||g(x)||=0.0001861411986757912\nIteration 23: x = [2.99997631 2.99997631], f(x) = 1.1226128853672496e-09, ||g(x)||=0.00011168471920497228\nIteration 24: x = [2.99998578 2.99998578], f(x) = 4.0414063872210937e-10, ||g(x)||=6.70108315234858e-05\nIteration 25: x = [2.99999147 2.99999147], f(x) = 1.454906299338991e-10, ||g(x)||=4.020649891358905e-05\nIteration 26: x = [2.99999488 2.99999488], f(x) = 5.237662677983984e-11, ||g(x)||=2.4123899347651e-05\nIteration 27: x = [2.99999693 2.99999693], f(x) = 1.8855585639651492e-11, ||g(x)||=1.447433960909303e-05\nIteration 28: x = [2.99999816 2.99999816], f(x) = 6.788010829620027e-12, ||g(x)||=8.684603765204602e-06\nIteration 29: x = [2.99999889 2.99999889], f(x) = 2.4436838986632097e-12, ||g(x)||=5.210762258871547e-06\nIteration 30: x = [2.99999934 2.99999934], f(x) = 8.797262039900029e-13, ||g(x)||=3.1264573553229284e-06\nIteration 31: x = [2.9999996 2.9999996], f(x) = 3.167014331536526e-13, ||g(x)||=1.8758744136961865e-06\nIteration 32: x = [2.99999976 2.99999976], f(x) = 1.1401251593531493e-13, ||g(x)||=1.1255246477152823e-06\nIteration 33: x = [2.99999986 2.99999986], f(x) = 4.10445057876081e-14, ||g(x)||=6.753147886291694e-07\nIteration 34: x = [2.99999991 2.99999991], f(x) = 1.477602202246525e-14, ||g(x)||=4.0518887342871644e-07\nIteration 35: x = [2.99999995 2.99999995], f(x) = 5.31936792808749e-15, ||g(x)||=2.431133235548003e-07\nIteration 36: x = [2.99999997 2.99999997], f(x) = 1.914972432124978e-15, ||g(x)||=1.4586799413288017e-07\nIteration 37: x = [2.99999998 2.99999998], f(x) = 6.893900623730809e-16, ||g(x)||=8.752079597729852e-08\n\n\narray([2.99999998, 2.99999998])\n\n\nfunction f(x)\n    sum((x.-3.0).^2)\nend\n\nfunction gradient(x)\n    2.*(x.-3.0)\nend\n\nsteepest_descent(f, gradient, [.0, .0], 0.2)\n\n\n\nWhen the gradient is difficult to calculate analytically, we can use algorithmically calculate the derivative of the function \\(f\\).\n\nFinite difference: We use \\[\n\\lim_{h} \\frac{f(x+h) - f(x)}{h}\n\\]\n\nusing FiniteDifferences\n\n# Create a central finite difference method with the default settings\nfdm = central_fdm(5, 1)\n\n# Calculate the gradient at a point\nx0 = [1.0, 2.0]\ngradient = grad(fdm, f, x0)\n\nprintln(\"Numerical Gradient:\", gradient)\n\nimport numpy as np\nfrom scipy.optimize import approx_fprime\n\n\nepsilon = np.sqrt(np.finfo(float).eps)  \n\n# Point at which to calculate the gradient\nx0 = np.array([1.0, 2.0])\n\n# Calculate the gradient at the point x0\ngradient = approx_fprime(x0, f, epsilon)\n\nprint(\"Gradient at x0:\", gradient)\n\nGradient at x0: [-4. -2.]\n\n\n\n\n\nAutomatic Differentiation (AD) is a computational technique used to evaluate the derivative of a function specified by a computer program. AD exploits the fact that any computer program, no matter how complex, executes a sequence of elementary arithmetic operations and functions (like additions, multiplications, and trigonometric functions). By applying the chain rule to these operations, AD efficiently computes derivatives of arbitrary order, which are accurate up to machine precision. This contrasts with numerical differentiation, which can introduce significant rounding errors.\nOne popular Python library that implements automatic differentiation is autograd. It extends the capabilities of NumPy by allowing you to automatically compute derivatives of functions composed of many standard mathematical operations. Here is a simple example of using autograd to compute the derivative of a function:\nNow, let’s compute the derivative of the function defined above.\n\nimport autograd.numpy as np   # Import wrapped NumPy\nfrom autograd import grad    # Import the gradient function\n\n# Create a function that returns the derivative of f\ndf = grad(f)\n\n# Evaluate the derivative at x = pi\nprint(\"The derivative of f(x) at x = [0.2, 0.1] is:\", df(np.array([0.2, 0.1])))\n\nThe derivative of f(x) at x = [0.2, 0.1] is: [-5.6 -5.8]\n\n\nWhen comparing the precision and applicability of finite difference methods and automatic differentiation (AD), especially for functions with large input dimensions, there are several key points to consider.\nFinite difference methods approximate derivatives by evaluating differences in function values at nearby points. The accuracy of these methods is highly dependent on the step size chosen: too large, and the approximation becomes poor; too small, and floating-point errors can dominate the result. This trade-off can be particularly challenging in high-dimensional spaces as errors can accumulate more significantly (each partial derivative may introduce errors that affect the overall gradient computation).\nAD computes derivatives using the exact chain rule and is not based on numerical approximations of difference quotients. Therefore, it can provide derivatives that are accurate to machine precision. AD efficiently handles computations in high dimensions because it systematically applies elementary operations and chain rules, bypassing the curse of dimensionality that often affects finite difference methods. AD is less sensitive to the numerical issues that affect finite differences, such as choosing a step size or dealing with subtractive cancellation.\nThe visualization provided by @fig-error demonstrates the comparative performance of the finite difference method and automatic differentiation (AD) when used to calculate gradients. This graph illustrates a key observation: as the dimensionality of the input increases, the error associated with the finite difference method tends to rise almost linearly. This escalation in error is accompanied by an increase in computation time, underscoring the method’s sensitivity to higher dimensions.\nIn contrast, the automatic differentiation method showcases remarkable stability across dimensions. The error remains negligible, essentially zero, highlighting AD’s inherent precision. Furthermore, AD’s computation time remains consistent regardless of input dimensionality. This performance characteristic of AD is due to its methodological approach, which systematically applies the chain rule to derive exact derivatives, bypassing the numerical instability and scaling issues often encountered with finite differences.\nThese insights are particularly relevant in fields that rely heavily on precise and efficient computation of derivatives, such as in numerical optimization, machine learning model training, and dynamic systems simulation. The stability and scalability of AD make it an invaluable tool in these areas, especially when dealing with high-dimensional data sets or complex models.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\ndef multivariate_function(x):\n    \"\"\" A sample multivariate function, sum of squares plus sin of each component. \"\"\"\n    return np.sum(x**2 + np.sin(x))\n\ndef analytical_derivative(x):\n    \"\"\" Analytical derivative of the multivariate function. \"\"\"\n    return 2*x + np.cos(x)\n\ndef finite_difference_derivative(f, x, h=1e-5):\n    \"\"\" Compute the gradient of `f` at `x` using the central finite difference method. \"\"\"\n    grad = np.zeros_like(x)\n    for i in range(len(x)):\n        x_plus = np.copy(x)\n        x_minus = np.copy(x)\n        x_plus[i] += h\n        x_minus[i] -= h\n        grad[i] = (f(x_plus) - f(x_minus)) / (2*h)\n    return grad\n\n# Range of dimensions\ndimensions = range(1, 101)\nerrors = []\ntimes = []\n\n# Loop over dimensions\nfor dim in dimensions:\n    x = np.random.randn(dim)\n    \n    # Analytical derivative\n    true_grad = analytical_derivative(x)\n    \n    # Start timing\n    start_time = time.time()\n    \n    # Finite difference derivative\n    fd_grad = finite_difference_derivative(multivariate_function, x)\n    \n    # End timing\n    elapsed_time = time.time() - start_time\n    times.append(elapsed_time)\n    \n    # Error\n    error = np.linalg.norm(fd_grad - true_grad)\n    errors.append(error)\n\n# Plotting error\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(dimensions, errors, marker='o')\nplt.xlabel('Number of Dimensions')\nplt.ylabel('Error')\nplt.title('Approximation Error by Dimension')\nplt.grid(True)\n\n# Plotting computational time\nplt.subplot(1, 2, 2)\nplt.plot(dimensions, times, marker='o')\nplt.xlabel('Number of Dimensions')\nplt.ylabel('Time (seconds)')\nplt.title('Computational Time by Dimension')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\nIt is almost always advantageous to utilize established minimization routines and libraries rather than crafting custom code from scratch. This approach not only saves time but also leverages the extensive testing and optimizations embedded within these libraries, which are designed to handle a broad range of mathematical challenges efficiently and accurately.\nIn Python, the scipy.optimize module from the SciPy library is a robust toolkit that provides several algorithms for function minimization, including methods for unconstrained and constrained optimization. Some of the notable algorithms include BFGS, Nelder-Mead, and conjugate gradient, among others. These algorithms are well-suited for numerical optimization in scientific computing.\nIn Julia, Optim.jl offers a similar breadth of optimization routines, with support for a variety of optimization problems, from simple univariate function minimization to complex multivariate cases. Optim.jl includes algorithms like L-BFGS, Gradient Descent, and Newton’s Method, each tailored for specific types of optimization scenarios.\nBoth Python’s SciPy and Julia’s Optim.jl represent just a slice of the available tools. There are many other solvers and libraries dedicated to optimization. The most widely used is Ipopt which is particularly useful for problems that require embedding in lower-level systems for performance.\nThe choice of a solver can depend on several factors:\n\nProblem Type: Some solvers are better suited for large-scale problems, others for problems with complex constraints, or for nondifferentiable or noisy functions.\nAccuracy and Precision: Different algorithms and implementations can provide varying levels of precision and robustness, important in applications like aerospace or finance.\nPerformance and Speed: Depending on the implementation, some solvers might be optimized for speed using advanced techniques like parallel processing or tailored data structures.\nEase of Use and Flexibility: Some libraries offer more user-friendly interfaces and better documentation, which can be crucial for less experienced users or complex projects where customization is key.\n\n\n\n\nOptim.jl supports various optimization algorithms. For our simple example, we can use the BFGS method, which is suitable for smooth functions and is part of a family of quasi-Newton methods.\nYou can now call the optimize function from Optim.jl, specifying the function, an initial guess, and the optimization method. Here is how you do it:\n# Initial guess\ninitial_guess = [0.0, 0.0]\n\n# Perform the optimization\nresult = optimize(f, initial_guess, BFGS())\nThe result object contains all the information about the optimization process, including the optimal values found, the value of the function at the optimum, and the convergence status:\n# Access the results\nprintln(\"Optimal parameters: \", Optim.minimizer(result))\nprintln(\"Minimum value: \", Optim.minimum(result))\nprintln(\"Convergence information: \", result)\nIf your function is more complex or if you want to speed up the convergence for large-scale problems, you can also provide the gradient (and even the Hessian) to the optimizer. Optim.jl can utilize these for more efficient calculations.\nFor constrained optimization, Optim.jl has support for simple box constraints which can be set using the Fminbox method to wrap around other methods like BFGS.\n# Define the bounds\nlower_bounds = [0.5, 1.5]\nupper_bounds = [1.5, 2.5]\n\n# Initial guess within the bounds\ninitial_guess = [1.0, 2.0]\n\n# Set up the optimizer with Fminbox\noptimizer = Fminbox(BFGS())\n# Run the optimization with bounds\nresult = optimize(f, lower_bounds, upper_bounds, initial_guess, optimizer, Optim.Options(g_tol = 1e-6))\n\n\n\nThe scipy.optimize.minimize function in Python is a versatile solver for minimization problems of both unconstrained and constrained functions. It provides a wide range of algorithms for different kinds of optimization problems.\nSciPy’s minimize function supports various methods like ‘BFGS’, ‘Nelder-Mead’, ‘TNC’, etc. The choice of method can depend on the nature of your problem (e.g., whether it has constraints, whether the function is differentiable, etc.). As in the Julia’s discussion, we’ll use ‘BFGS’.\n\nimport numpy as np\nfrom scipy.optimize import minimize\ninitial_guess = np.array([0, 0])\nresult = minimize(f, initial_guess, method='BFGS')\n\nprint(\"Optimal parameters:\", result.x)\nprint(\"Minimum value:\", result.fun)\nprint(\"Success:\", result.success)\nprint(\"Message:\", result.message)\n\nOptimal parameters: [2.99999999 2.99999999]\nMinimum value: 2.4602836913395623e-16\nSuccess: True\nMessage: Optimization terminated successfully.\n\n\nTo add some bounds to the variables we can use the Bounds module.\n\nfrom scipy.optimize import Bounds\n\n# Define bounds (0, +Inf) for all parameters\nbounds = Bounds(np.array([0., 0.]), [np.inf, np.inf])\n\n# Run the optimization with bounds\nresult_with_bounds = minimize(f, initial_guess, method='L-BFGS-B', bounds=bounds)\nprint(\"Optimal parameters:\", result_with_bounds.x)\nprint(\"Minimum value:\", result_with_bounds.fun)\nprint(\"Success:\", result_with_bounds.success)\nprint(\"Message:\", result_with_bounds.message)\n\nOptimal parameters: [3.00000044 3.00000044]\nMinimum value: 3.9332116120454443e-13\nSuccess: True\nMessage: CONVERGENCE: NORM OF PROJECTED GRADIENT &lt;= PGTOL"
  },
  {
    "objectID": "comptools_ass2.html#optimization-problems",
    "href": "comptools_ass2.html#optimization-problems",
    "title": "Optimization",
    "section": "",
    "text": "Optimization is a field of mathematics that focuses on the problem of finding the solution to a minimization problem. More precisely, given a function \\(f:\\mathbb{R}^{k}\\to \\mathbb{R}\\), we seek \\(x^{*}\\in\\mathcal{C}\\subset \\mathbb{R}^{k}\\) such that \\[\nf(x^{*}) \\leqslant f(x) \\text{ for all }x\\in\\mathcal{C}.\n\\] The set \\(\\mathcal{C}\\) constraints the solution to leave in a subset of \\(\\mathbb{R}^{k}\\)\nWhen \\(\\mathcal{C}\\) coincides with \\(\\mathbb{R}^k\\) the problem is said to be unconstrained.\nIn unconstrained optimization, the objective function \\(f(x)\\) needs to be minimized (or maximized) without any restrictions on the variable \\(x\\). The problem is described as \\[\n\\min_{x} f(x), \\quad x\\in\\mathbb{R}^k.\n\\]\nMany problems involve constraints that the solution must satisfy, that is, \\(\\mathcal{C}\\) does not coincide with \\(\\mathbb{R}^k\\). For instance, we want to minimize the function over a space where \\(x_j &lt; c_j\\), \\(c_j\\in\\mathbb{R}\\), \\(j=1,\\ldots,k\\) or we may be interested in values of \\(x\\) that minimizes \\(f\\) when certain restrictions are satisfied. Generally, the constrained optimization is \\[\n   \\begin{aligned}\n   &\\min_{x} \\ f(x) \\\\\n   &\\text{subject to} \\\\\n   & \\ g_i(x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n                     & \\ h_j(x) = 0, \\quad j = 1, \\ldots, p.\n   \\end{aligned}\n\\] Here, \\(g_i(x)\\) and \\(h_j(x)\\) are functions that represent inequality and equality constraints.\nIn what follows, we will focus on the class of unconstrained problems where \\(f\\) is smooth, that is, a function that everywhere continuously differentiable.\n\n\n\\[\n\\min_{x} f(x), \\quad x\\in\\mathbb{R}^k.\n\\tag{1}\\]\nA point \\(x^*\\) is a global minimizer of \\(f\\) if \\(f(x^*)\\leqslant f(x)\\) for all \\(x\\) ranging over of of \\(\\mathbb{R}^k\\).\nThe global minimizer can be difficult to find. The algorithms to solve Equation 1 exploit local knowledge of \\(f\\), we do not have a clear picture of the overall shape of \\(f\\) and, as such, we cannot ever be sure that the solution we find is indeed a global solution to the minimization problem. Most algorithms are able to find only a local minimizer, which is a point that achieves the samllest value of \\(f\\) in a neighborhood of \\(x\\).\nA point \\(x^*\\) is a local minimizer if there is a neighborhood1 \\(\\mathcal{N}\\) of \\(x^*\\) such that \\(f(x^*)\\leqslant f(x)\\) for all \\(x\\in\\mathcal{N}\\).\nA point \\(x^*\\) is a strict local minimizer if there a neighborhood of \\(\\mathcal{N}\\) such that \\(f(x^*) &lt; f(x)\\) for all \\(x\\in \\mathcal{N}\\).\nFor instance, for the function \\(f(x) = 2024\\), every point is a weak local minimizer, while the function \\(f(x) = (x-a)^2\\) has a strict local minimizer at \\(x=a^{-1}\\).\nNotation: Given a function \\(f:\\mathbb{R}^k\\to\\mathbb{R}\\), the gradient of \\(f\\) evaluated at \\(x^o\\) is: \\[\n\\nabla f(x^o) := \\begin{pmatrix}\\left.\\frac{\\partial f(x)}{\\partial x_{1}}\\right|_{x=x^{o}}\\\\\n\\left.\\frac{\\partial f(x)}{\\partial x_{2}}\\right|_{x=x^{o}}\\\\\n\\vdots\\\\\n\\left.\\frac{\\partial f(x)}{\\partial x_{k}}\\right|_{x=x^{o}}\n\\end{pmatrix}.\n\\] Similarly, the \\(k\\times k\\) hessian matrix of \\(f\\) evaluated at \\(x^o\\) is \\[\n\\nabla^{2}f(x^{o})=\\begin{pmatrix}\\left.\\frac{\\partial^{2}f(x)}{\\partial x_{1}\\partial x_{1}}\\right|_{x=x^{o}} & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{1}\\partial x_{2}}\\right|_{x=x^{o}} & \\cdots & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{1}\\partial x_{k}}\\right|_{x=x^{o}}\\\\\n\\left.\\frac{\\partial^{2}f(x)}{\\partial x_{2}\\partial x_{1}}\\right|_{x=x^{o}} & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{2}\\partial x_{2}}\\right|_{x=x^{o}} & \\cdots & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{2}\\partial x_{k}}\\right|_{x=x^{o}}\\\\\n\\vdots & \\vdots &  & \\vdots\\\\\n\\left.\\frac{\\partial^{2}f(x)}{\\partial x_{k}\\partial x_{1}}\\right|_{x=x^{o}} & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{k}\\partial x_{2}}\\right|_{x=x^{o}} & \\cdots & \\left.\\frac{\\partial^{2}f(x)}{\\partial x_{k}\\partial x_{k}}\\right|_{x=x^{o}}\n\\end{pmatrix}.\n\\]\n\n\n\nThe necessary conditions for optimality are derived assuming that \\(x^*\\) is a local minimizer and the proving facts about \\(\\nabla f(x^*)\\) and \\(\\nabla^2 f(x^*)\\).\n\nTheorem 1 If \\(x^*\\) is a local optimizer and \\(f\\) is continuously differentiable in an open neighborhood of \\(x^*\\), then \\(\\nabla f(x^*)=0\\) (first-order necessary condition); if \\(\\nabla^2 f\\) exists and its continuous in an open neighborhood of \\(x^*\\), then \\(\\nabla^2 f(x^*)\\) is positive definite (second-order necessary conditions).2\n\nSufficient conditions for optimality are conditions on the derivatives of \\(f\\) at the point \\(x^*\\) that guarantee that \\(x^*\\) is a local minimizer.\n\nTheorem 2 Suppose that \\(\\nabla^2 f(x^*)\\) is continuous in an open neighborhood of \\(x^*\\) and that \\(\\nabla f(x^*)=0\\) and \\(\\nabla^2 f(x^*)\\) is positive definite. Then \\(x^*\\) is a strict local minimizer of \\(f\\).\n\nThe second-order sufficient conditions guarantee something stronger than the necessary conditions; namely, that the minimizer is a strict local minimizer. Note too that the second-order sufficient conditions are not necessary: a point \\(x^∗\\) may be a strict local minimizer, and yet may fail to satisfy the sufficient conditions. A simple example is given by the function \\(f(x) = x^3\\), for which the point \\(x^∗=0\\) is a strict local minimizer at which the Hessian matrix vanishes (and is therefore not positive definite).\nWhen the objective function is convex, local and global minimizers are simple to characterize.\n\nTheorem 3 When \\(f\\) is convex, any local minimizer of \\(x^*\\) is a global minimizer of \\(f\\). If in addition, \\(f\\) is differentiable, any stationary point \\(x^*\\) is a global minimizer.\n\n\nExample 1 The sum of square residuals \\[\nSSR(\\beta) = \\sum_{i=1}^n (Y_i - X'_i\\beta)^2\n\\] is strictly convex provided that \\(\\sum_{i=1}^n X_iX'_i\\) is invertible (that is, it has full column rank).\n\n\n\n\nA common approach to optimization is to incrementally improve a point \\(x\\) by taking a step that minimizes the objective value based on a local model. The local model may be obtained, for example, from a first- or second-order Taylor approximation. Optimization algorithms that follow this general approach are referred to as descent direction methods. They start with a design point \\(x^{(0)}\\) and then generate a sequence of points, sometimes called iterates, to converge to a local minimum.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the function and its derivative\ndef f(x):\n    return x**2 - np.log(x)\n\ndef df(x):\n    return 2*x - 1/x\n\ndef connectpoints(x,y,p1,p2):\n    x1, x2 = x[p1], x[p2]\n    y1, y2 = y[p1], y[p2]\n    plt.plot([x1,x2],[y1,y2],'k-')\n\n\n# Initial point\nx0 = 2\nalpha = 0.3\n\n# Gradient descent update\nx1 = x0 - alpha * df(x0)\nx2 = x1 - alpha * df(x1)\n# Points for the function plot\nx = np.linspace(-2.5, 2.5, 400)\ny = f(x)\n\n# Tangent line at x0 (y = m*x + b)\n\n# Creating the plot\n\nplt.figure(figsize=(8, 5))\nplt.plot(x, y, label='f(x) = x^2')\nplt.scatter([x0, x1], [f(x0), f(x1)], color='red')  # Points\nm = df(x0)\nb = f(x0) - m*x0\ntangent_line = m*x + b\nplt.plot(x, tangent_line, 'b--', label=f'Tangent at x0={x0}')\nplt.arrow(x0, 0.4, x1-x0, 0.0, head_width=0.1, length_includes_head=True, color = 'r')\n\nm = df(x1)\nb = f(x1) - m*x1\ntangent_line = m*x + b\nplt.plot(x, tangent_line, 'b--', label=f'Tangent at x0={x1}', )\nplt.ylim([-0.2,6])\nplt.xlim([-0.4,3])\nplt.plot(x, tangent_line, 'b--', label=f'Tangent at x0={x1}', )\nm = df(x0)\nb = f(x0) - m*x0\ntangent_line = m*x + b\n\nplt.scatter(x0, f(x0), color='green')  # Initial point\nplt.scatter(x1, f(x1), color='green')  # Next point after step\nplt.scatter(x2, f(x2), color='green')  # Next point after step\n\nplt.arrow(x1, 0., x2-x1, 0., head_width=0.1, length_includes_head=True, color = 'r')\n\nplt.title('Gradient Descent on f(x)')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.xticks([])  # Remove x-axis ticks\nplt.xticks([x0, x1, x2], [r\"$x^{(0)}$\", r\"$x^{(1)}$\", r\"$x^{(2)}$\"])\n#plt.legend()\nplt.grid(True)\nplt.plot([x0, x0],[f(x0), 0],'g--')\nplt.plot([x1, x1],[f(x1), 0],'g--')\nplt.plot([x2, x2],[f(x1), 0],'g--')\nplt.show()\n\n/var/folders/72/d3_1t_ps0z1fw0_l58cwv_s80000gn/T/ipykernel_35042/189701708.py:6: RuntimeWarning: invalid value encountered in log\n  return x**2 - np.log(x)\n\n\n\n\n\n\n\n\nFigure 1: Gradient descent\n\n\n\n\n\nThe intuition behind this approach is relatively simple. Consider the function plotted in Figure 1. We start the algorithm at \\(x^{(0)}\\). The derivative at this point (the tangent line in blue) is positive, that is, \\(f'(x^{(0)})&gt;0\\). Thus, to descend toward smaller values of the function have to set a point \\(x^{(1)}\\) smaller. One possibility is to use the following iteration \\[\nx^{(1)} = x^{(0)} - \\alpha \\nabla f(x^{(0)}),\n\\] where \\(\\alpha\\in(0,1)\\) is the step factor. As it can be seen from Figure 1, at this point the value of \\(f\\) is now lower. Applying a new iterate we obtain \\(x^{(2)} = x^{(1)} - \\alpha f'(x^{(1)})\\) which is now very close to the minimum value of the function.\nWe will keep iterating until the termination condition is satisfied. The termination condition will be satisfied when the current iterate is likely to be a local minimum.[^termination]\n3: The most common termination conditions are 1. Maximum iterations. We may want to terminate when the number of iterations \\(k\\) exceeds some threshold \\(k_max\\). Alternatively, we might want to terminate once a maximum amount of elapsed time is exceeded. 2. Absolute improvement. This termination condition looks at the change in the function value over subsequent steps. If the change is smaller than a given threshold, it will terminate: \\[\nf(x^{(k)}) - f(x^{(r+1)}) &lt; \\epsilon_a\n\\] 1. Relative improvement. This termination condition looks at the change in function value but uses the relative change of the function. The iterations will terminate if \\[\nf(x^{(k)}) - f(x^{(r+1)}) &lt; \\epsilon_r|f(x^{(k)})|\n\\] 1. Gradient magnitude. We can terminate if the derivative is smaller than a certain tolerance \\[\n\\Vert \\delta f(x^{(k)}) \\Vert &lt; \\epsilon_g\n\\]\nThe same logic can be applied to the case in which \\(f:\\mathbb{R}^k \\to \\mathbb{R}\\) the only difference is that now \\(\\nabla f(x^{(0)})\\) is a \\(k\\times 1\\) vector instead of being a scalar.\nThe gradient descent idea can be generalized by considering the following iterate \\[\nx^{(r)} = x^{(r-1)} - \\alpha d^{(r)},\n\\] where \\(d^{(r)}\\) is a descent direction. The idea of this generalization is that instead of using the gradient as direction, we can use different directions that might speed up the algorithm.\nWhen \\(d^{(r)} = \\nabla f(x^{(r)})\\), the algorithm is called the gradient descent (and direction is called the direction of deepest descent).\nDirections that can be used belong to two classes: 1. first-order: the direction only uses information about the gradient of \\(f\\). The many variations of the gradient descent (Ada, Momentum, etc.) and the conjugate gradient method belong to this class. 2. second order: the direction incorporate information about the second derivatives of \\(f\\). The key idea here is that \\[\nf(x^{(r+1)}) = f(x^{(r)}) + \\nabla f'(x^{(r)})(x^{(r+1)}-x^{(r)}) + (x^{(r+1)}-x^{(r)}) \\dot{H}_k (x^{(r+1)}-x^{(r)}),\n\\] where \\(\\dot{H}_k = \\nabla^2 f(\\dot{x}^{(r)})\\) is the Hessian evaluated at some point between \\(x^{(r+1)}\\) and \\(x^{(r)}\\). Then, approximating the gradient of \\(f\\) and setting it equal to zero yields \\[\nx^{(r+1)} = x^{(r)}-[\\nabla^2 f(x^{(r)})]^{-1} \\nabla f(x^{(r)}) +\n\\] which suggests using the inverse of the hessian to form the direction. Since evaluating the Hessian at each iterate is too costly computationally, different algorithms approximate the Hessian in different ways.\n\n\n\nThe following code implements a gradient descent with steepest direction in Python.\n\nfrom numpy import linalg as la\n\ndef steepest_descent(f, gradient, initial_guess, learning_rate, num_iterations = 100, epsilon_g = 1e-07):\n    x = initial_guess\n    for i in range(num_iterations):\n        grad = gradient(x)\n        x = x - learning_rate * grad\n        normg = la.norm(grad)\n        print(f\"Iteration {i+1}: x = {x}, f(x) = {f(x)}, ||g(x)||={normg}\")\n        ## Termination condition\n        if  normg &lt; epsilon_g:\n            break\n    return x\n\nThis is the Julia version.\n#| label: julia-descent\nusing LinearAlgebra ## needed for norm\nfunction steepest_descent(f, gradient, initial_guess, learning_rate; num_iterations = 100; epsilon_g = 1e-7)\n    x = initial_guess\n    for i in 1:num_iterations\n        grad = gradient(x)\n        x = x - learning_rate * grad\n        normg = norm(g)\n        println(\"Iteration $i: x = $x, f(x) = $(objective_function(x)), ||g(x)||=$(normg)\")\n        if normg &lt; epsilon_g\n            break\n        end\n    end\n    return x\nend\nSuppose we want to solve the following problem \\[\n\\min_{x} f(x)\n\\] where, for some \\(d&gt;1\\), \\[\nf(x) = \\sum_{i=1}^d \\left((x_i-3)^2 \\right)\n\\] The gradient of this function is \\[\n\\nabla f(x)=\\begin{pmatrix}2(x_{1}-3)\\\\\n2(x_{2}-3)\\\\\n\\vdots\\\\\n2(x_{d}-3)\n\\end{pmatrix}.\n\\]\nThe minimum of this function is \\(x_i = 3\\).\n\ndef f(x):\n    return np.sum((x-3.0)**2)\n\ndef gradient(x):\n    return 2*(x-3.0)\n\nsteepest_descent(f, gradient, np.array([0., 0.]), 0.2)\n\nIteration 1: x = [1.2 1.2], f(x) = 6.479999999999999, ||g(x)||=8.48528137423857\nIteration 2: x = [1.92 1.92], f(x) = 2.3327999999999993, ||g(x)||=5.091168824543142\nIteration 3: x = [2.352 2.352], f(x) = 0.8398079999999992, ||g(x)||=3.0547012947258847\nIteration 4: x = [2.6112 2.6112], f(x) = 0.3023308799999997, ||g(x)||=1.8328207768355302\nIteration 5: x = [2.76672 2.76672], f(x) = 0.10883911679999973, ||g(x)||=1.099692466101318\nIteration 6: x = [2.860032 2.860032], f(x) = 0.039182082047999806, ||g(x)||=0.6598154796607905\nIteration 7: x = [2.9160192 2.9160192], f(x) = 0.014105549537279988, ||g(x)||=0.39588928779647375\nIteration 8: x = [2.94961152 2.94961152], f(x) = 0.005077997833420814, ||g(x)||=0.23753357267788475\nIteration 9: x = [2.96976691 2.96976691], f(x) = 0.0018280792200315037, ||g(x)||=0.1425201436067311\nIteration 10: x = [2.98186015 2.98186015], f(x) = 0.0006581085192113414, ||g(x)||=0.08551208616403891\nIteration 11: x = [2.98911609 2.98911609], f(x) = 0.00023691906691608675, ||g(x)||=0.051307251698423345\nIteration 12: x = [2.99346965 2.99346965], f(x) = 8.529086408979587e-05, ||g(x)||=0.03078435101905426\nIteration 13: x = [2.99608179 2.99608179], f(x) = 3.070471107232651e-05, ||g(x)||=0.01847061061143306\nIteration 14: x = [2.99764908 2.99764908], f(x) = 1.1053695986035874e-05, ||g(x)||=0.011082366366859834\nIteration 15: x = [2.99858945 2.99858945], f(x) = 3.9793305549719125e-06, ||g(x)||=0.0066494198201153985\nIteration 16: x = [2.99915367 2.99915367], f(x) = 1.4325589997895879e-06, ||g(x)||=0.003989651892068737\nIteration 17: x = [2.9994922 2.9994922], f(x) = 5.15721239924432e-07, ||g(x)||=0.002393791135240991\nIteration 18: x = [2.99969532 2.99969532], f(x) = 1.8565964637257903e-07, ||g(x)||=0.0014362746811448456\nIteration 19: x = [2.99981719 2.99981719], f(x) = 6.683747269425835e-08, ||g(x)||=0.000861764808686405\nIteration 20: x = [2.99989032 2.99989032], f(x) = 2.4061490169894037e-08, ||g(x)||=0.0005170588852123454\nIteration 21: x = [2.99993419 2.99993419], f(x) = 8.662136461115092e-09, ||g(x)||=0.00031023533112715604\nIteration 22: x = [2.99996051 2.99996051], f(x) = 3.118369125973376e-09, ||g(x)||=0.0001861411986757912\nIteration 23: x = [2.99997631 2.99997631], f(x) = 1.1226128853672496e-09, ||g(x)||=0.00011168471920497228\nIteration 24: x = [2.99998578 2.99998578], f(x) = 4.0414063872210937e-10, ||g(x)||=6.70108315234858e-05\nIteration 25: x = [2.99999147 2.99999147], f(x) = 1.454906299338991e-10, ||g(x)||=4.020649891358905e-05\nIteration 26: x = [2.99999488 2.99999488], f(x) = 5.237662677983984e-11, ||g(x)||=2.4123899347651e-05\nIteration 27: x = [2.99999693 2.99999693], f(x) = 1.8855585639651492e-11, ||g(x)||=1.447433960909303e-05\nIteration 28: x = [2.99999816 2.99999816], f(x) = 6.788010829620027e-12, ||g(x)||=8.684603765204602e-06\nIteration 29: x = [2.99999889 2.99999889], f(x) = 2.4436838986632097e-12, ||g(x)||=5.210762258871547e-06\nIteration 30: x = [2.99999934 2.99999934], f(x) = 8.797262039900029e-13, ||g(x)||=3.1264573553229284e-06\nIteration 31: x = [2.9999996 2.9999996], f(x) = 3.167014331536526e-13, ||g(x)||=1.8758744136961865e-06\nIteration 32: x = [2.99999976 2.99999976], f(x) = 1.1401251593531493e-13, ||g(x)||=1.1255246477152823e-06\nIteration 33: x = [2.99999986 2.99999986], f(x) = 4.10445057876081e-14, ||g(x)||=6.753147886291694e-07\nIteration 34: x = [2.99999991 2.99999991], f(x) = 1.477602202246525e-14, ||g(x)||=4.0518887342871644e-07\nIteration 35: x = [2.99999995 2.99999995], f(x) = 5.31936792808749e-15, ||g(x)||=2.431133235548003e-07\nIteration 36: x = [2.99999997 2.99999997], f(x) = 1.914972432124978e-15, ||g(x)||=1.4586799413288017e-07\nIteration 37: x = [2.99999998 2.99999998], f(x) = 6.893900623730809e-16, ||g(x)||=8.752079597729852e-08\n\n\narray([2.99999998, 2.99999998])\n\n\nfunction f(x)\n    sum((x.-3.0).^2)\nend\n\nfunction gradient(x)\n    2.*(x.-3.0)\nend\n\nsteepest_descent(f, gradient, [.0, .0], 0.2)\n\n\n\nWhen the gradient is difficult to calculate analytically, we can use algorithmically calculate the derivative of the function \\(f\\).\n\nFinite difference: We use \\[\n\\lim_{h} \\frac{f(x+h) - f(x)}{h}\n\\]\n\nusing FiniteDifferences\n\n# Create a central finite difference method with the default settings\nfdm = central_fdm(5, 1)\n\n# Calculate the gradient at a point\nx0 = [1.0, 2.0]\ngradient = grad(fdm, f, x0)\n\nprintln(\"Numerical Gradient:\", gradient)\n\nimport numpy as np\nfrom scipy.optimize import approx_fprime\n\n\nepsilon = np.sqrt(np.finfo(float).eps)  \n\n# Point at which to calculate the gradient\nx0 = np.array([1.0, 2.0])\n\n# Calculate the gradient at the point x0\ngradient = approx_fprime(x0, f, epsilon)\n\nprint(\"Gradient at x0:\", gradient)\n\nGradient at x0: [-4. -2.]\n\n\n\n\n\nAutomatic Differentiation (AD) is a computational technique used to evaluate the derivative of a function specified by a computer program. AD exploits the fact that any computer program, no matter how complex, executes a sequence of elementary arithmetic operations and functions (like additions, multiplications, and trigonometric functions). By applying the chain rule to these operations, AD efficiently computes derivatives of arbitrary order, which are accurate up to machine precision. This contrasts with numerical differentiation, which can introduce significant rounding errors.\nOne popular Python library that implements automatic differentiation is autograd. It extends the capabilities of NumPy by allowing you to automatically compute derivatives of functions composed of many standard mathematical operations. Here is a simple example of using autograd to compute the derivative of a function:\nNow, let’s compute the derivative of the function defined above.\n\nimport autograd.numpy as np   # Import wrapped NumPy\nfrom autograd import grad    # Import the gradient function\n\n# Create a function that returns the derivative of f\ndf = grad(f)\n\n# Evaluate the derivative at x = pi\nprint(\"The derivative of f(x) at x = [0.2, 0.1] is:\", df(np.array([0.2, 0.1])))\n\nThe derivative of f(x) at x = [0.2, 0.1] is: [-5.6 -5.8]\n\n\nWhen comparing the precision and applicability of finite difference methods and automatic differentiation (AD), especially for functions with large input dimensions, there are several key points to consider.\nFinite difference methods approximate derivatives by evaluating differences in function values at nearby points. The accuracy of these methods is highly dependent on the step size chosen: too large, and the approximation becomes poor; too small, and floating-point errors can dominate the result. This trade-off can be particularly challenging in high-dimensional spaces as errors can accumulate more significantly (each partial derivative may introduce errors that affect the overall gradient computation).\nAD computes derivatives using the exact chain rule and is not based on numerical approximations of difference quotients. Therefore, it can provide derivatives that are accurate to machine precision. AD efficiently handles computations in high dimensions because it systematically applies elementary operations and chain rules, bypassing the curse of dimensionality that often affects finite difference methods. AD is less sensitive to the numerical issues that affect finite differences, such as choosing a step size or dealing with subtractive cancellation.\nThe visualization provided by @fig-error demonstrates the comparative performance of the finite difference method and automatic differentiation (AD) when used to calculate gradients. This graph illustrates a key observation: as the dimensionality of the input increases, the error associated with the finite difference method tends to rise almost linearly. This escalation in error is accompanied by an increase in computation time, underscoring the method’s sensitivity to higher dimensions.\nIn contrast, the automatic differentiation method showcases remarkable stability across dimensions. The error remains negligible, essentially zero, highlighting AD’s inherent precision. Furthermore, AD’s computation time remains consistent regardless of input dimensionality. This performance characteristic of AD is due to its methodological approach, which systematically applies the chain rule to derive exact derivatives, bypassing the numerical instability and scaling issues often encountered with finite differences.\nThese insights are particularly relevant in fields that rely heavily on precise and efficient computation of derivatives, such as in numerical optimization, machine learning model training, and dynamic systems simulation. The stability and scalability of AD make it an invaluable tool in these areas, especially when dealing with high-dimensional data sets or complex models.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\ndef multivariate_function(x):\n    \"\"\" A sample multivariate function, sum of squares plus sin of each component. \"\"\"\n    return np.sum(x**2 + np.sin(x))\n\ndef analytical_derivative(x):\n    \"\"\" Analytical derivative of the multivariate function. \"\"\"\n    return 2*x + np.cos(x)\n\ndef finite_difference_derivative(f, x, h=1e-5):\n    \"\"\" Compute the gradient of `f` at `x` using the central finite difference method. \"\"\"\n    grad = np.zeros_like(x)\n    for i in range(len(x)):\n        x_plus = np.copy(x)\n        x_minus = np.copy(x)\n        x_plus[i] += h\n        x_minus[i] -= h\n        grad[i] = (f(x_plus) - f(x_minus)) / (2*h)\n    return grad\n\n# Range of dimensions\ndimensions = range(1, 101)\nerrors = []\ntimes = []\n\n# Loop over dimensions\nfor dim in dimensions:\n    x = np.random.randn(dim)\n    \n    # Analytical derivative\n    true_grad = analytical_derivative(x)\n    \n    # Start timing\n    start_time = time.time()\n    \n    # Finite difference derivative\n    fd_grad = finite_difference_derivative(multivariate_function, x)\n    \n    # End timing\n    elapsed_time = time.time() - start_time\n    times.append(elapsed_time)\n    \n    # Error\n    error = np.linalg.norm(fd_grad - true_grad)\n    errors.append(error)\n\n# Plotting error\nplt.figure(figsize=(14, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(dimensions, errors, marker='o')\nplt.xlabel('Number of Dimensions')\nplt.ylabel('Error')\nplt.title('Approximation Error by Dimension')\nplt.grid(True)\n\n# Plotting computational time\nplt.subplot(1, 2, 2)\nplt.plot(dimensions, times, marker='o')\nplt.xlabel('Number of Dimensions')\nplt.ylabel('Time (seconds)')\nplt.title('Computational Time by Dimension')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\nIt is almost always advantageous to utilize established minimization routines and libraries rather than crafting custom code from scratch. This approach not only saves time but also leverages the extensive testing and optimizations embedded within these libraries, which are designed to handle a broad range of mathematical challenges efficiently and accurately.\nIn Python, the scipy.optimize module from the SciPy library is a robust toolkit that provides several algorithms for function minimization, including methods for unconstrained and constrained optimization. Some of the notable algorithms include BFGS, Nelder-Mead, and conjugate gradient, among others. These algorithms are well-suited for numerical optimization in scientific computing.\nIn Julia, Optim.jl offers a similar breadth of optimization routines, with support for a variety of optimization problems, from simple univariate function minimization to complex multivariate cases. Optim.jl includes algorithms like L-BFGS, Gradient Descent, and Newton’s Method, each tailored for specific types of optimization scenarios.\nBoth Python’s SciPy and Julia’s Optim.jl represent just a slice of the available tools. There are many other solvers and libraries dedicated to optimization. The most widely used is Ipopt which is particularly useful for problems that require embedding in lower-level systems for performance.\nThe choice of a solver can depend on several factors:\n\nProblem Type: Some solvers are better suited for large-scale problems, others for problems with complex constraints, or for nondifferentiable or noisy functions.\nAccuracy and Precision: Different algorithms and implementations can provide varying levels of precision and robustness, important in applications like aerospace or finance.\nPerformance and Speed: Depending on the implementation, some solvers might be optimized for speed using advanced techniques like parallel processing or tailored data structures.\nEase of Use and Flexibility: Some libraries offer more user-friendly interfaces and better documentation, which can be crucial for less experienced users or complex projects where customization is key.\n\n\n\n\nOptim.jl supports various optimization algorithms. For our simple example, we can use the BFGS method, which is suitable for smooth functions and is part of a family of quasi-Newton methods.\nYou can now call the optimize function from Optim.jl, specifying the function, an initial guess, and the optimization method. Here is how you do it:\n# Initial guess\ninitial_guess = [0.0, 0.0]\n\n# Perform the optimization\nresult = optimize(f, initial_guess, BFGS())\nThe result object contains all the information about the optimization process, including the optimal values found, the value of the function at the optimum, and the convergence status:\n# Access the results\nprintln(\"Optimal parameters: \", Optim.minimizer(result))\nprintln(\"Minimum value: \", Optim.minimum(result))\nprintln(\"Convergence information: \", result)\nIf your function is more complex or if you want to speed up the convergence for large-scale problems, you can also provide the gradient (and even the Hessian) to the optimizer. Optim.jl can utilize these for more efficient calculations.\nFor constrained optimization, Optim.jl has support for simple box constraints which can be set using the Fminbox method to wrap around other methods like BFGS.\n# Define the bounds\nlower_bounds = [0.5, 1.5]\nupper_bounds = [1.5, 2.5]\n\n# Initial guess within the bounds\ninitial_guess = [1.0, 2.0]\n\n# Set up the optimizer with Fminbox\noptimizer = Fminbox(BFGS())\n# Run the optimization with bounds\nresult = optimize(f, lower_bounds, upper_bounds, initial_guess, optimizer, Optim.Options(g_tol = 1e-6))\n\n\n\nThe scipy.optimize.minimize function in Python is a versatile solver for minimization problems of both unconstrained and constrained functions. It provides a wide range of algorithms for different kinds of optimization problems.\nSciPy’s minimize function supports various methods like ‘BFGS’, ‘Nelder-Mead’, ‘TNC’, etc. The choice of method can depend on the nature of your problem (e.g., whether it has constraints, whether the function is differentiable, etc.). As in the Julia’s discussion, we’ll use ‘BFGS’.\n\nimport numpy as np\nfrom scipy.optimize import minimize\ninitial_guess = np.array([0, 0])\nresult = minimize(f, initial_guess, method='BFGS')\n\nprint(\"Optimal parameters:\", result.x)\nprint(\"Minimum value:\", result.fun)\nprint(\"Success:\", result.success)\nprint(\"Message:\", result.message)\n\nOptimal parameters: [2.99999999 2.99999999]\nMinimum value: 2.4602836913395623e-16\nSuccess: True\nMessage: Optimization terminated successfully.\n\n\nTo add some bounds to the variables we can use the Bounds module.\n\nfrom scipy.optimize import Bounds\n\n# Define bounds (0, +Inf) for all parameters\nbounds = Bounds(np.array([0., 0.]), [np.inf, np.inf])\n\n# Run the optimization with bounds\nresult_with_bounds = minimize(f, initial_guess, method='L-BFGS-B', bounds=bounds)\nprint(\"Optimal parameters:\", result_with_bounds.x)\nprint(\"Minimum value:\", result_with_bounds.fun)\nprint(\"Success:\", result_with_bounds.success)\nprint(\"Message:\", result_with_bounds.message)\n\nOptimal parameters: [3.00000044 3.00000044]\nMinimum value: 3.9332116120454443e-13\nSuccess: True\nMessage: CONVERGENCE: NORM OF PROJECTED GRADIENT &lt;= PGTOL"
  },
  {
    "objectID": "comptools_ass2.html#assignment",
    "href": "comptools_ass2.html#assignment",
    "title": "Optimization",
    "section": "Assignment",
    "text": "Assignment\n\\[\nY_t = c + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\varepsilon_t, \\quad \\varepsilon_t ~ WN(0, \\sigma^2).\n\\]\nParameter Estimation - Estimate the parameters of the AR(2) model using both conditional and unconditional likelihood approaches on the monthly log differences of INDPRO from the FRED-MD (FRED data is here).\nTasks:\n\nCoding the Likelihood Function\n\nImplement the likelihood function for an AR(2) model in Python. You are required to code both the conditional and unconditional likelihood functions.\nConditional Likelihood: This approach uses the first 2 observations as given and starts the likelihood calculation from the 3rd observation.\nUnconditional Likelihood: This approach integrates over \\(p(Y_1, Y_2)\\), the unconditional distributions of the first two observations.\n\nUse the following model specification for the AR(2) process: \\[\nY_t = c + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\epsilon_t\n\\] where \\(\\epsilon_t\\) is i.i.d. normal with mean zero and variance \\(\\sigma^2\\).\nMaximizing the Likelihood\n\nWrite Python or Julia code to maximize the likelihood functions (both conditional and unconditional) with respect to the parameters \\(c, \\phi_1, \\phi_2,\\) and \\(\\sigma^2\\). You may consider using optimization routines available in libraries such as scipy.optimize or Optim.jl.\n\nForecasting\n\nWith the estimated parameters from both approaches, forecast the future values of the log differences of INDPRO for the next 8 months (\\(h=1,2,\\dots,8\\)).\n(Optional) Provide a brief comparison of the forecast accuracy from both sets of parameters based on out-of-sample forecasting. Discuss any notable differences and potential reasons for these differences.\n\n\nDeliverables: - A Python (or Julia) script containing the implementations and results for the tasks outlined above. The script should run (you might give instructions on what is needed to make it run). Ideally, a Jupyter notebook. - A report discussing the methodology and the results.\nAssessment Criteria: - Correctness of the likelihood function implementations. - Quality of the code, including readability and proper documentation.\nResources: - FRED-MD dataset: Access the dataset directly from the FRED website or through any API that provides access to it. - SciPy library documentation for optimization functions. - Optim.jl\nThis assignment will test your ability to implement statistical models, manipulate time-series data, perform parameter estimation, and use statistical methods to forecast future values.\n\nThe AR(2) process\nThe AR(2) process is given by the equation: \\[\nY_t = c + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\epsilon_t\n\\] where \\(\\epsilon_t\\) is white noise with mean zero and variance \\(\\sigma^2\\), and \\(c\\) is a constant.\nThe process is stationary if\n\\[\n  \\phi_2 ​&gt; −1 \\text{ and } \\phi_1+\\phi_2&lt;1 \\text{ and } \\phi_2 - \\phi_1 &lt; 1.\n\\]\n\n\nConditional Likelihood Function\nThe conditional likelihood assumes that the initial values \\(Y_1\\) and \\(Y_2\\) are fixed (conditioning on them). For observations \\(\\{Y_t\\}_{t=3}^T\\):\n\\[\n  \\begin{aligned}\n    L_c(\\theta|Y_1, Y_2, \\ldots, Y_T) &= \\prod_{t=3}^T f(Y_t|Y_{t-1}, Y_{t-2}; \\theta) \\\\\n    &= \\prod_{t=3}^T \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(Y_t - c - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-2})^2}{2\\sigma^2}\\right) \\\\\n    & = (2\\pi\\sigma^2)^{-\\frac{T-2}{2}} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{t=3}^T(Y_t - c - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-2})^2\\right),\n  \\end{aligned}\n\\] where \\(\\theta = (c, \\phi_1, \\phi_2, \\sigma^2)\\) is the parameter vector.\nThe conditional log-likelihood is: \\[\n\\ell_c(\\theta|Y_1, Y_2, \\ldots, Y_T) = -\\frac{T-2}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{t=3}^T(Y_t - c - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-2})^2\n\\]\nThe conditional maximum likelihood estimators are equivalent to the OLS estimators.\nThe following python calculate the conditional likelihood and optimize it.\n\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\ndef ar_likelihood(params, data, p):\n    \"\"\"\n    Calculate the negative (unconditional) log likelihood for an AR(p) model.\n\n    params: list of parameters, where the first p are AR coefficients and the last is the noise variance.\n    data: observed data.\n    p: order of the AR model.\n    \"\"\"\n    # Extract AR coefficients and noise variance\n    c = params[0]\n    phi = params[1:p+1]\n    sigma2 = params[-1]\n        \n    # Calculate residuals\n    T = len(data)\n    residuals = data[p:] - c - np.dot(np.column_stack([data[p-j-1:T-j-1] for j in range(p)]), phi)\n    \n    # Calculate negative log likelihood\n    log_likelihood = (-T/2 * np.log(2 * np.pi * sigma2) - np.sum(residuals**2) / (2 * sigma2))\n    \n    return -log_likelihood\n\ndef estimate_ar_parameters(data, p):\n    \"\"\"\n    Estimate AR model parameters using maximum likelihood estimation.\n\n    data: observed data.\n    p: order of the AR model.\n    \"\"\"\n    # Initial parameter guess (random AR coefficients, variance of 1)\n    params_initial = np.zeros(p+2)\n    params_initial[-1] = 1.0\n\n    ## Bounds\n    bounds = [(None, None)]\n    # Then p AR coefficients, each bounded between -1 and 1\n    bounds += [(-1, 1) for _ in range(p)]\n    # The variance parameter, bounded to be positive\n    bounds += [(1e-6, None)]\n\n    # Minimize the negative log likelihood\n    result = minimize(ar_likelihood, params_initial, args=(data, p), bounds=bounds)\n    \n    if result.success:\n        estimated_params = result.x\n        return estimated_params\n    else:\n        raise Exception(\"Optimization failed:\", result.message)\n\n# Example usage\ndata = np.random.randn(100)  # Simulated data; replace with actual data\np = 2  # AR(2) model\nparams = estimate_ar_parameters(data, p)\nprint(\"Estimated parameters:\", params)\n\nEstimated parameters: [ 0.05955401  0.05062593 -0.06459607  0.78251261]\n\n\nYou can check the \\((c, \\phi_1, \\phi_2)\\) are equivalent to those we obtain by applying the OLS.\n\nimport numpy as np\n\ndef fit_ar_ols_xx(data, p):\n    \"\"\"\n    data: observed data.\n    p: order of the AR model.\n    note: no constant\n    \"\"\"\n    # Prepare the lagged data matrix\n    T = len(data)\n    Y = data[p:]  # Dependent variable (from p to end)\n    X = np.column_stack([data[p-i-1:T-i-1] for i in range(p)])\n    X = np.column_stack((np.ones(X.shape[0]), X))\n    \n    # Calculate OLS estimates using the formula: beta = (X'X)^-1 X'Y\n    XTX = np.dot(X.T, X)  # X'X\n    XTY = np.dot(X.T, Y)  # X'Y\n    beta_hat = np.linalg.solve(XTX, XTY)  # Solve (X'X)beta = X'Y\n    \n    return beta_hat\n\n\nbeta_hat = fit_ar_ols_xx(data, p)\nprint(\"Estimated AR coefficients:\", beta_hat)\n\nEstimated AR coefficients: [ 0.05955245  0.05062542 -0.06459566]\n\n\n\n\nJulia code\nThis is the Julia equivalent of the Python code.\nusing Distributions\nusing Optim\nusing LinearAlgebra\n\nfunction ar_likelihood(params, data, p)\n    \"\"\"\n    Calculate the negative (unconditional) log likelihood for an AR(p) model.\n    \n    params: list of parameters, where the first p+1 are AR coefficients and the last is the noise variance.\n    data: observed data.\n    p: order of the AR model.\n    \"\"\"\n    # Extract AR coefficients and noise variance\n    c = params[1]\n    phi = params[2:p+1]\n    sigma2 = params[p+2]\n        \n    # Calculate residuals\n    T = length(data)\n    Y = data[p+1:end]\n    X = hcat([data[p-j:T-j-1] for j in 0:p-1]...)\n    residuals = Y .- c .- X * phi\n    \n    # Calculate negative log likelihood\n    log_likelihood = (-T/2 * log(2 * π * sigma2) - sum(residuals.^2) / (2 * sigma2))\n    \n    return -log_likelihood\nend\n\nfunction estimate_ar_parameters(data, p)\n    \"\"\"\n    Estimate AR model parameters using maximum likelihood estimation.\n    \n    data: observed data.\n    p: order of the AR model.\n    \"\"\"\n    # Initial parameter guess (zeros AR coefficients, variance of 1)\n    params_initial = zeros(p+2)\n    params_initial[p+2] = 1.0  # Initial variance\n    \n    # Setup lower and upper bounds\n    lower_bounds = vcat(-Inf, fill(-1.0, p), 1e-6)\n    upper_bounds = vcat(+Inf, fill(1.0, p), Inf)\n    \n    # Minimize the negative log likelihood\n    result = optimize(params -&gt; ar_likelihood(params, data, p), \n                     lower_bounds, upper_bounds, params_initial,\n                     Fminbox(LBFGS()))\n    \n    if Optim.converged(result)\n        estimated_params = Optim.minimizer(result)\n        return estimated_params\n    else\n        error(\"Optimization failed\")\n    end\nend\n\n# Example usage\ndata = randn(100)  # Simulated data; replace with actual data\np = 2  # AR(2) model\nparams = estimate_ar_parameters(data, p)\nprintln(\"Estimated parameters: \", params)\n\nfunction fit_ar_ols_xx(data, p)\n    \"\"\"\n    Estimate the parameters of AR(p) by OLS \n    data: observed data.\n    p: order of the AR model.\n    note: no constant\n    \"\"\"\n    # Prepare the lagged data matrix\n    T = length(data)\n    Y = data[p+1:end]  # Dependent variable (from p+1 to end)\n    X = hcat([data[p-j:T-j-1] for j in 0:p-1]...)\n    X = [ones(T-p) X]  # Add a constant to the model\n    # Calculate OLS estimates using the formula: beta = (X'X)^-1 X'Y\n    XTX = X' * X  # X'X\n    XTY = X' * Y  # X'Y\n    beta_hat = XTX \\ XTY  # Solve (X'X)beta = X'Y\n    residuals = Y-X*beta_hat\n    sigma_hat = residuals'residuals/(T-p)\n    return beta_hat, sigma_hat\nend\n\ndata = randn(100)\nbeta_hat = fit_ar_ols_xx(data, 2)\nprintln(\"Estimated AR coefficients c, phi_1, phi_2, ...: \", beta_hat[1])\nprintln(\"Estimated AR coefficients sigma^2: \", beta_hat[2])\n\n\nExact Likelihood Function\nThe exact likelihood incorporates the distribution of the initial values, treating \\(Y_1\\) and \\(Y_2\\) as random variables from the stationary distribution of the process:\n\\[\nL_e(\\theta|Y_1, Y_2, \\ldots, Y_T) = f(Y_1, Y_2; \\theta) \\cdot \\prod_{t=3}^T f(Y_t|Y_{t-1}, Y_{t-2}; \\theta)\n\\]\nFor a stationary AR(2) process, the joint distribution of \\((Y_1, Y_2)\\) is a bivariate normal with:\nMean vector: \\[\n\\mu_0 = \\begin{pmatrix} \\frac{c}{1-\\phi_1-\\phi_2} \\\\ \\frac{c}{1-\\phi_1-\\phi_2} \\end{pmatrix}\n\\]\nVariance-covariance matrix: \\[\n  \\Sigma_0 = \\begin{pmatrix} \\gamma(0) & \\gamma(1) \\\\ \\gamma(1) & \\gamma(0) \\end{pmatrix},\n\\] where: \\[\n  \\gamma(0) = \\frac{\\sigma^2}{1-\\phi_2^2-\\phi_1^2} \\text{ and } \\gamma(1) = \\frac{\\phi_1\\gamma(0)}{1-\\phi_2}.\n\\]\nThus:\n\\[\n  f(Y_1, Y_2; \\theta) = \\frac{1}{2\\pi|\\Sigma_0|^{1/2}} \\exp\\left(-\\frac{1}{2}([Y_1, Y_2] - \\mu_0)'\\Sigma_0^{-1}([Y_1, Y_2] - \\mu_0)\\right),\n\\] where \\(|\\Sigma_0| = \\gamma(0)^2 - \\gamma(1)^2\\).\nThe exact log-likelihood is:\n\\[\n  \\begin{aligned}\n    \\ell_e(\\theta|Y_1, Y_2, \\ldots, Y_T) &= \\ln(f(Y_1, Y_2; \\theta)) + \\ell_c(\\theta|Y_1, Y_2, \\ldots, Y_T)\\\\\n    &= -\\frac{1}{2}\\ln(2\\pi|\\Sigma_0|) - \\frac{1}{2}([Y_1, Y_2] - \\mu_0)'\\Sigma_0^{-1}([Y_1, Y_2] - \\mu_0) \\\\\n    &\\quad-\\frac{T-2}{2}\\ln(2\\pi\\sigma^2)  - \\frac{1}{2\\sigma^2}\\sum_{t=3}^T(Y_t - c - \\phi_1 Y_{t-1} - \\phi_2 Y_{t-2})^2\n  \\end{aligned}\n\\]\n\n\nPython implementation\nThis Python function implements the exact log-likelihood calculation for an AR(2). The function ar2_exact_loglikelihood constructs the variance-covariance matrix for the first two observations, computes their contribution to the likelihood, and then adds the conditional likelihood contribution from remaining observations. It returns the negative log-likelihood value (for minimization in optimization algorithms).\n\nimport numpy as np\nfrom scipy import stats\n\ndef ar2_exact_loglikelihood(params, y):\n    \"\"\"\n    Calculate the exact log-likelihood for an AR(2) model.\n    \n    Parameters:\n    -----------\n    y : array-like\n        data (T x 1)\n    params : tuple or list\n        Model parameters (c, phi1, phi2, sigma2)\n        c: constant term\n        phi1: coefficient of y_{t-1}\n        phi2: coefficient of y_{t-2}\n        sigma2: error variance\n    \n    Returns:\n    --------\n    float\n        Exact log-likelihood value\n    \"\"\"\n    # Extract parameters\n    c, phi1, phi2, sigma2 = params\n    \n    # Check stationarity conditions\n    if not (phi2 &gt; -1 and phi1 + phi2 &lt; 1 and phi2 - phi1 &lt; 1):\n        return -np.inf  # Return negative infinity if not stationary\n    \n    T = len(y)\n    \n    if T &lt; 3:\n        raise ValueError(\"Time series must have at least 3 observations for AR(2)\")\n    \n    # Calculate the unconditional mean of the process\n    mu = c / (1 - phi1 - phi2)\n    \n    # Calculate autocovariances for stationary process\n    gamma0 = sigma2 / (1 - phi2**2 - phi1**2)  # Variance\n    gamma1 = phi1 * gamma0 / (1 - phi2)        # First-order autocovariance\n    \n    # Create initial variance-covariance matrix\n    Sigma0 = np.array([[gamma0, gamma1], \n                        [gamma1, gamma0]])\n    \n    # Calculate determinant of Sigma0\n    det_Sigma0 = gamma0**2 - gamma1**2\n    \n    # Calculate inverse of Sigma0\n    if det_Sigma0 &lt;= 0:  # Check for positive definiteness\n        return -np.inf\n    \n    inv_Sigma0 = np.array([[gamma0, -gamma1], \n                            [-gamma1, gamma0]]) / det_Sigma0\n    \n    # Initial distribution contribution (Y1, Y2)\n    y_init = np.array([y[0], y[1]])\n    mu_init = np.array([mu, mu])\n    \n    diff_init = y_init - mu_init\n    quad_form_init = diff_init.T @ inv_Sigma0 @ diff_init\n    \n    loglik_init = -np.log(2 * np.pi * np.sqrt(det_Sigma0)) - 0.5 * quad_form_init\n    \n    # Conditional log-likelihood contribution (Y3, ..., YT | Y1, Y2)\n    residuals = np.zeros(T-2)\n    for t in range(2, T):\n        y_pred = c + phi1 * y[t-1] + phi2 * y[t-2]\n        residuals[t-2] = y[t] - y_pred\n    \n    loglik_cond = -0.5 * (T-2) * np.log(2 * np.pi * sigma2) - \\\n                   0.5 * np.sum(residuals**2) / sigma2\n    \n    # Total exact log-likelihood\n    exact_loglik = loglik_init + loglik_cond\n    \n    ## Return the negative loglik\n    return -exact_loglik\n\nThis is an example of how this function can be used to maximize the exact likelihood function to obtain an estimate of the parameters\n\nfrom scipy import optimize\ndef fit_ar2_mle(y, initial_params=None):\n    \"\"\"\n    Fit an AR(2) model using maximum likelihood estimation\n    \n    Parameters:\n    -----------\n    y : array-like\n        Time series data\n    initial_params : tuple, optional\n        Initial guess for (c, phi1, phi2, sigma2)\n    \"\"\"\n    # Set default initial parameters if not provided\n    if initial_params is None:\n      # Simple initial estimates\n      c_init = 0.0\n      phi1_init = 0\n      phi2_init = 0\n      sigma2_init = np.var(y)\n        \n      initial_params = (c_init, phi1_init, phi2_init, sigma2_init)\n      # Constraints to ensure positive variance\n    \n    lbnds = (-np.inf, -0.99, -0.99, 1e-6)  # Lower bounds for params\n    ubnds = (np.inf, 0.99, 0.99, np.inf)     # Upper bounds for params\n\n    bnds = optimize.Bounds(lb=lbnds, ub=ubnds)\n    # Optimize\n    result = optimize.minimize(\n        ar2_exact_loglikelihood, \n        initial_params,\n        (y,),\n        bounds = bnds,\n        method='L-BFGS-B', \n        options={'disp': False} # set to true to get more info\n    )\n    \n    if not result.success:\n        print(f\"Warning: Optimization did not converge. {result.message}\")\n    \n    # Return parameters and maximum log-likelihood\n    return result.x, result.fun\n\n\nY = np.random.normal(size=(100,))\nfit_ar2_mle(Y, initial_params=None)\n\n(array([ 0.00664359,  0.17130123, -0.08957015,  0.76026142]),\n np.float64(128.21478098580002))\n\n\n\n\nJulia implmentation\nThis Julia code implements estimation for the AR(2) processes. The ar2_exact_loglikelihood function calculates the exact log-likelihood of an AR(2) model by combining two components: the joint distribution of the initial observations \\((Y_1, Y_2)\\) based on the stationary distribution, and the conditional likelihood of subsequent observations. The fit_ar2_mle function uses this log-likelihood to estimate model parameters through numerical optimization, employing box constraints and the L-BFGS algorithm to find the maximum likelihood estimates of the constant term, autoregressive coefficients, and error variance.\nusing LinearAlgebra\nusing Distributions\nusing Optim\nusing Random\n\n\"\"\"\n    ar2_exact_loglikelihood(y, params)\n\nCalculate the exact log-likelihood for an AR(2) model.\n\n# Arguments\n- `y::Vector{Float64}`: Time series data\n- `params::Vector{Float64}`: Model parameters [c, phi1, phi2, sigma2]\n  - c: constant term\n  - phi1: coefficient of y_{t-1}\n  - phi2: coefficient of y_{t-2}\n  - sigma2: error variance\n\n# Returns\n- `loglik::Float64`: Exact log-likelihood value\n\"\"\"\nfunction ar2_exact_loglikelihood(y::Vector{Float64}, params::Vector{Float64})\n    # Extract parameters\n    c, phi1, phi2, sigma2 = params\n    \n    # Check stationarity conditions\n    if !(phi2 &gt; -1 && phi1 + phi2 &lt; 1 && phi2 - phi1 &lt; 1)\n        return -Inf\n    end\n    \n    T = length(y)\n    \n    if T &lt; 3\n        error(\"Time series must have at least 3 observations for AR(2)\")\n    end\n    \n    # Calculate the unconditional mean of the process\n    mu = c / (1 - phi1 - phi2)\n    \n    # Calculate autocovariances for stationary process\n    gamma0 = sigma2 / (1 - phi2^2 - phi1^2)  # Variance\n    gamma1 = phi1 * gamma0 / (1 - phi2)      # First-order autocovariance\n    \n    # Create initial variance-covariance matrix\n    Sigma0 = [gamma0 gamma1; gamma1 gamma0]\n    \n    # Calculate determinant of Sigma0\n    det_Sigma0 = gamma0^2 - gamma1^2\n    \n    # Check for positive definiteness\n    if det_Sigma0 &lt;= 0\n        return -Inf\n    end\n    \n    # Calculate inverse of Sigma0\n    inv_Sigma0 = [gamma0 -gamma1; -gamma1 gamma0] ./ det_Sigma0\n    \n    # Initial distribution contribution (Y1, Y2)\n    y_init = [y[1], y[2]]\n    mu_init = [mu, mu]\n    \n    diff_init = y_init - mu_init\n    quad_form_init = dot(diff_init, inv_Sigma0 * diff_init)\n    \n    loglik_init = -log(2π * sqrt(det_Sigma0)) - 0.5 * quad_form_init\n    \n    # Conditional log-likelihood contribution (Y3, ..., YT | Y1, Y2)\n    residuals = zeros(T-2)\n    for t in 3:T\n        y_pred = c + phi1 * y[t-1] + phi2 * y[t-2]\n        residuals[t-2] = y[t] - y_pred\n    end\n    \n    loglik_cond = -0.5 * (T-2) * log(2π * sigma2) - 0.5 * sum(residuals.^2) / sigma2\n    \n    # Total exact log-likelihood\n    exact_loglik = loglik_init + loglik_cond\n    \n    ## Return minus loglik\n    return -exact_loglik\nend\n\n\"\"\"\n    fit_ar2_mle(y; initial_params=nothing)\n\nFit an AR(2) model using maximum likelihood estimation\n\n# Arguments\n- `y::Vector{Float64}`: Time series data\n- `initial_params::Union{Vector{Float64}, Nothing}=nothing`: Optional initial guess for [c, phi1, phi2, sigma2]\n\n# Returns\n- `est_params::Vector{Float64}`: Estimated parameters [c, phi1, phi2, sigma2]\n- `max_loglik::Float64`: Maximum log-likelihood value\n\"\"\"\nfunction fit_ar2_mle(y::Vector{Float64}; initial_params=nothing)\n    # Set default initial parameters if not provided\n    if isnothing(initial_params)\n        # Simple initial estimates\n        c_init = mean(y) * 0.1\n        phi1_init = 0.5\n        phi2_init = 0.1\n        \n        # Calculate residuals using initial phi values\n        resid = zeros(length(y)-2)\n        for t in 3:length(y)\n            resid[t-2] = y[t] - c_init - phi1_init * y[t-1] - phi2_init * y[t-2]\n        end\n        sigma2_init = var(resid)\n        \n        initial_params = [c_init, phi1_init, phi2_init, sigma2_init]\n    end\n    \n    # We use box constraints for individual parameters and handle stationarity conditions in the function\n    lower_bounds = [-Inf, -0.99, -0.99, 1e-6]  # Lower bounds for c, phi1, phi2, sigma2\n    upper_bounds = [Inf, 0.99, 0.99, Inf]     # Upper bounds for c, phi1, phi2, sigma2\n    \n    # Optimize using L-BFGS\n    result = optimize(t -&gt; ar2_exact_loglikelihood(y, t), \n      lower_bounds, \n      upper_bounds, \n      initial_params, Fminbox(LBFGS()))\n    \n    # Get optimized parameters\n    est_params = Optim.minimizer(result)\n    max_loglik = -Optim.minimum(result)\n    \n    if !Optim.converged(result)\n        @warn \"Optimization did not converge.\"\n    end\n    \n    return est_params, max_loglik\nend\n\n## You can test wit randomly generated data\nY = randn(100)\nfit_ar2_mle(Y)"
  },
  {
    "objectID": "comptools_ass2.html#footnotes",
    "href": "comptools_ass2.html#footnotes",
    "title": "Optimization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA neighborhood of a point \\(x\\in\\mathbb{R}\\) is any open subset of \\(\\mathbb{R}^k\\) containing \\(x\\).↩︎\nA square matrix \\(H\\) is positive definite if for every \\(z\\in\\mathbb{R}^k\\) with \\(\\norm{z}\\neq 0\\), \\(z'Hz&gt;0\\).↩︎\ntermanation↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Tools for Macroeconometrics (AAF2351)",
    "section": "",
    "text": "Prof. Giuseppe Ragusa\n   Sapienza, University of Rome\n   Department of Economics and Law\n   Viale del Castro Laurenziano, 9\n   giuseppe.ragusa at uniroma1 dot it\n   Office Hour:\n\n\n\n\n\n   Friday\n   17 february, 2025 - 31 may 2025\n   12:00-14:00\n   Ecodir"
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Computational Tools for Macroeconometrics (AAF2351)",
    "section": "Description",
    "text": "Description\nComputational Tools for Macroeconometrics (AAF2351) covers the computational aspects of time series. It begins with an overview of the computational challenges inherent in macroeconomic data analysis and the pivotal role of software tools, with a particular focus on Python, Julia and R. The syllabus covers a broad range of topics starting gong from forecasting to nonlinear optimization and simulations.\nThe course is structured as an interactive lab, emphasizing a hands-on learning approach through practical assignments. In this lab environment, students are expected to learn by doing, applying the theoretical knowledge acquired in lectures to real-world data and scenarios."
  }
]